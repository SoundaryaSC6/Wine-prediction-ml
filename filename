 1/1:
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import matplotlib.pyplot as plt

# Load the dataset
customer_data = pd.read_excel(r"C:\Users\sound\OneDrive\Desktop\Internshala work\customer_churn_large_dataset.xlsx")

# Drop irrelevant columns
columns_to_drop = ['CustomerID', 'Age', 'Monthly_Bill', 'Total_Usage_GB', 'Gender', 'Location', 'Name']
dataset = customer_data.drop(columns_to_drop, axis=1)

# Separate features (X) and target (y)
X = dataset.drop('Churn', axis=1)
y = dataset['Churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize a Random Forest classifier
classifier = RandomForestClassifier(random_state=0)

# Define hyperparameters to search through
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Initialize GridSearchCV
gridsearch = GridSearchCV(classifier, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)

# Perform the grid search on the training data
gridsearch.fit(X_train, y_train)

# Get the best estimator from the grid search
best_classifier = gridsearch.best_estimator_

# Make predictions
y_pred = best_classifier.predict(X_test)
y_score = best_classifier.predict_proba(X_test)[:, 1]

# Evaluate the model
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("AUCROC:", roc_auc_score(y_test, y_score))

# Plot histogram of predicted probabilities
plt.hist(y_score, bins=100) 
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
# Display the plot
plt.show()

# Display the best parameters and their corresponding AUC scores
results = pd.DataFrame(gridsearch.cv_results_)
best_params = results.loc[results['rank_test_score'] == 1, ['params', 'mean_test_score']]
print("Best Parameters and AUC Scores:")
print(best_params)
 3/1:
import matplotlib.pyplot as plt

# Define the categories
categories = ['Nuclear receptor - 13%', 'Cytochrome P450 - 4%', 'Enzyme - 16%', 'Kinase - 20%', 'Hydrolase - 1% ', 'Protease - 14%', 'Phosphatase - 1%', 'Family A G protein-coupled receptor - 11%', 'Primary active transporter - 2%', 'Oxidoreductase - 2%', 'Secreted protein - 1%', 'Unclassified protein - 3%', 'Family C G protein-coupled receptor - 1%', 'Transferase - 3%', 'Other nuclear protein - 1%', 'Membrane receptor - 1%', 'Phosphodiesterase - 1%', 'Toll-like and Il-1 receptors - 1%', 'Voltage-gated ion channel - 1%', 'Other cytosolic protein - 1%', 'Electrochemical transporter - 2%']

# Define the percentages as numerical values
percentages = [13, 4, 16, 20, 1, 14, 1, 11, 2, 2, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 2]

# Adding colors
colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a pie chart
plt.pie(percentages, colors=colors)
plt.legend(categories, loc='center left', bbox_to_anchor=(1, 0.5))
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
 4/1:
# Read the CSV file
file_path <- "C:\\Users\\sound\\OneDrive\\Desktop\\python projrct\\python project 1\\Eplereonone drug target prediction csv file.csv"
dataset <- read.csv("Eplereonone drug target prediction csv file.csv")
head(dataset)
summary(dataset)
 6/1:
# Read the CSV file
file_path <- "C:\\Users\\sound\\OneDrive\\Desktop\\python projrct\\python project 1\\Eplereonone drug target prediction csv file.csv"
dataset <- read.csv("Eplereonone drug target prediction csv file.csv")
head(dataset)
summary(dataset)
 8/1:
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn import model_selection
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn import preprocessing
from sklearn.metrics import classification_report
import warnings
from sklearn.metrics import confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
import tensorflow as tf
from keras.utils import np_utils
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dense, Input, Dropout,BatchNormalization
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
import random
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras import losses,optimizers,Sequential
from tensorflow.keras import backend
random.seed(100)
np.random.seed(100) 
tf.random.set_seed(100)
warnings.filterwarnings("ignore")
 8/2:
# 1: Import and understand the data.

# 1.1 Read the CSV file
file_path = "C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\Eplereonone drug target prediction csv file.csv"
dataset = read.csv(file_path)
df.head(dataset)
summary(dataset)
 8/3:
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn import model_selection
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn import preprocessing
from sklearn.metrics import classification_report
import warnings
from sklearn.metrics import confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
import tensorflow as tf
from keras.utils import np_utils
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dense, Input, Dropout,BatchNormalization
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
import random
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras import losses,optimizers,Sequential
from tensorflow.keras import backend
random.seed(100)
np.random.seed(100) 
tf.random.set_seed(100)
warnings.filterwarnings("ignore")
 8/4:
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn import model_selection
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn import preprocessing
from sklearn.metrics import classification_report
import warnings
from sklearn.metrics import confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
import tensorflow as tf
from keras.utils import np_utils
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dense, Input, Dropout,BatchNormalization
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
import random
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras import losses,optimizers,Sequential
from tensorflow.keras import backend
random.seed(100)
np.random.seed(100) 
tf.random.set_seed(100)
warnings.filterwarnings("ignore")
 8/5:
# 1: Import and understand the data.

# 1.1 Read the CSV file
file_path = "C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\Eplereonone drug target prediction csv file.csv"
dataset = read.csv(file_path)
 8/6:
# 1: Import and understand the data.

# 1.1 Read the CSV file
file_path = "C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\Eplereonone drug target prediction csv file.csv"
dataset = pd.read_csv(file_path)
 8/7:
# 1: Import and understand the data.

# 1.1 Read the CSV file
dataset = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\Eplereonone drug target prediction csv file.csv")
 9/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
 9/2:
# 1.1 Read the CSV file
file_path = "C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\Eplereonone drug target prediction csv file.csv"
dataset = pd.read_csv(file_path)
print(dataset.head())
print(dataset.describe())
 9/3:
# Read the CSV file and print its content
with open('C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\Eplereonone drug target prediction csv file.csv', 'r') as f:
    reader = csv.reader(f)
    for row in reader:
        print(row)
 9/4:
# Read the CSV file
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)

# Print the content of the CSV file
print(dataset)
 9/5:
# Read the CSV file
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)

# Print the content of the CSV file
print(dataset)
 9/6:
# 1.2 Check for Missing Values
na_counts = dataset.isna().sum()
print(na_counts)
 9/7:

# 1.3 Explore Categorical Variables
freq_table = dataset['Target.Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target.Class', data=dataset)
plt.title('Categorical Data')
plt.show()
 9/8:

# 1.3 Explore Categorical Variables
freq_table = dataset['Target Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.title('Categorical Data')
plt.show()
 9/9:
# 1.4 Explore and Visualize Numerical Variables
plt.figure(figsize=(8, 6))
sns.histplot(dataset['Probability.'])
plt.title('Probability Distribution')
plt.show()
9/10:
# 1.4 Explore and Visualize Numerical Variables
plt.figure(figsize=(8, 6))
sns.histplot(dataset['Probability*'])
plt.title('Probability Distribution')
plt.show()
9/11:
# Convert Probability to a categorical variable
dataset['Probability'] = dataset['Probability*'].astype('category')
9/12:
# Convert Probability to a categorical variable
dataset['Probability'] = dataset['Probability*'].astype('category')
9/13:
# Convert Probability to a categorical variable
dataset['Probability'] = dataset['Probability*'].astype('category')
print(dataset['Probability'])
10/1:
# Generate frequency table
freq_table = dataset['Probability'].value_counts()
print(freq_table)
10/2:
# Generate frequency table
freq_table = dataset['Probability*'].value_counts()
print(freq_table)
10/3:
# Generate frequency table
freq_table = dataset['Probability*'].value_counts()
print(freq_table)
10/4:
# Generate frequency table
freq_table = dataset['Probability*'].value_counts()
print(freq_table)
10/5:
# Generate frequency table
freq_table = ["Probability*"].value_counts()
print(freq_table)
10/6:
# Generate frequency table
freq_table = dataset["Probability*"].value_counts()
print(freq_table)
10/7:
# 1.5 Descriptive Statistics
data = dataset[['Target.Class', 'Probability']]
print(data.head())
10/8:
# Read the CSV file
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)

# Print the content of the CSV file
print(dataset)
10/9:
# Read the CSV file
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)

# Print the content of the CSV file
print(dataset)
10/10:
# Read the CSV file
import pandas as pd
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)

# Print the content of the CSV file
print(dataset)
10/11:
# 1.2 Check for Missing Values
na_counts = dataset.isna().sum()
print(na_counts)
10/12:

# 1.3 Explore Categorical Variables
freq_table = dataset['Target Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.title('Categorical Data')
plt.show()
10/13:

# 1.3 Explore Categorical Variables
import matplotlib.pyplot as plt

freq_table = dataset['Target Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.title('Categorical Data')
plt.show()
10/14:

# 1.3 Explore Categorical Variables
import matplotlib.pyplot as plt
import seaborn as sns

freq_table = dataset['Target Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.title('Categorical Data')
plt.show()
10/15:
# 1.4 Explore and Visualize Numerical Variables
plt.figure(figsize=(8, 6))
sns.histplot(dataset['Probability*'])
plt.title('Probability Distribution')
plt.show()
10/16:
# 1.5 Descriptive Statistics
data = dataset[['Target.Class', 'Probability']]
print(data.head())
10/17:
# Generate frequency table
freq_table = dataset['Probability'].value_counts()
print(freq_table)


# 1.5 Descriptive Statistics
data = dataset[['Target.Class', 'Probability']]
print(data.head())
10/18:
# Generate frequency table
import pandas as pd
freq_table = dataset['Probability'].value_counts()
print(freq_table)


# 1.5 Descriptive Statistics
data = dataset[['Target.Class', 'Probability']]
print(data.head())
10/19:
# Generate frequency table
freq_table = dataset['Probability*'].value_counts()
print(freq_table)


# 1.5 Descriptive Statistics
data = dataset[['Target Class', 'Probability*']]
print(data.head())
10/20:
# Generate frequency table

freq_table = dataset['Probability*'].value_counts()
print(freq_table)
10/21:
# 1.5 Descriptive Statistics

data = dataset[['Target Class', 'Probability*']]
print(data.head())
10/22:
# 1.6 An input vector having duplicate values

unique_values = data['Target Class'].unique()
print(unique_values)
print(data['Probability*'].describe())
10/23:
# 1.8 Checking for outliers

plt.figure(figsize=(8, 6))
sns.boxplot(x=dataset['Probability.'])
plt.title('Boxplot of Probability with Outliers')
plt.show()
10/24:
# 1.8 Checking for outliers
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.boxplot(x=dataset['Probability.'])
plt.title('Boxplot of Probability with Outliers')
plt.show()
10/25:
# 1.8 Checking for outliers


plt.figure(figsize=(8, 6))
sns.boxplot(x=dataset['Probability*'])
plt.title('Boxplot of Probability with Outliers')
plt.show()
10/26:
# 1.9 Remove Outliers

Q1 = dataset['Probability*'].quantile(0.25)
Q3 = dataset['Probability*'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
no_outliers = dataset[(dataset['Probability.'] >= lower_bound) & (dataset['Probability.'] <= upper_bound)]
print("Dimensions of no_outliers:", no_outliers.shape)
plt.figure(figsize=(8, 6))
sns.boxplot(x=no_outliers['Probability*'])
plt.title('Boxplot of Probability without Outliers')
plt.show()
10/27:
# 1.9 Remove Outliers

Q1 = dataset['Probability*'].quantile(0.25)
Q3 = dataset['Probability*'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
no_outliers = dataset[(dataset['Probability*'] >= lower_bound) & (dataset['Probability*'] <= upper_bound)]
print("Dimensions of no_outliers:", no_outliers.shape)
plt.figure(figsize=(8, 6))
sns.boxplot(x=no_outliers['Probability*'])
plt.title('Boxplot of Probability without Outliers')
plt.show()
10/28:
# 2.1 Split Data into Training and Testing Sets
from sklearn.model_selection import train_test_split

data['id'] = range(len(data))
training_dataset, testing_dataset = train_test_split(data, test_size=0.3, random_state=1)
10/29:
# 2.1 Split Data into Training and Testing Sets
from sklearn.model_selection import train_test_split

data['id'] = range(len(data))
training_dataset, testing_dataset = train_test_split(data, test_size=0.3, random_state=1)
10/30:
# 2.1 Split Data into Training and Testing Sets
from sklearn.model_selection import train_test_split

data = range(len(data))
training_dataset, testing_dataset = train_test_split(data, test_size=0.3, random_state=1)
10/31:
# 2.1 Split Data into Training and Testing Sets
from sklearn.model_selection import train_test_split

data = range(len(data))
training_dataset, testing_dataset = train_test_split(data, test_size=0.3, random_state=1)
10/32:
# 2.1 Split Data into Training and Testing Sets
from sklearn.model_selection import train_test_split

data = range(len(data))
training_dataset, testing_dataset = train_test_split(data, test_size=0.3, random_state=1)
print(training_dataset, testing_dataset)
10/33:
# 2.2 Verifying if train and test data is in sync
print(len(data) == len(training_dataset) + len(testing_dataset))
10/34:

To convert the R code to Python, you can use the pandas library for data manipulation, matplotlib and seaborn for visualization, and scikit-learn for data preprocessing. Here's the Python equivalent code:

python
Copy code
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

# 1.1 Read the CSV file
file_path = "C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\Eplereonone drug target prediction csv file.csv"
dataset = pd.read_csv(file_path)
print(dataset.head())
print(dataset.describe())

# 1.2 Check for Missing Values
na_counts = dataset.isna().sum()
print(na_counts)

# 1.3 Explore Categorical Variables
freq_table = dataset['Target.Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target.Class', data=dataset)
plt.title('Categorical Data')
plt.show()

# 1.4 Explore and Visualize Numerical Variables
plt.figure(figsize=(8, 6))
sns.histplot(dataset['Probability.'])
plt.title('Probability Distribution')
plt.show()

# Convert Probability to a categorical variable
dataset['Probability'] = dataset['Probability.'].astype('category')

# Generate frequency table
freq_table = dataset['Probability'].value_counts()
print(freq_table)

# 1.5 Descriptive Statistics
data = dataset[['Target.Class', 'Probability']]
print(data.head())

# 1.6 An input vector having duplicate values
unique_values = data['Target.Class'].unique()
print(unique_values)
print(data['Probability'].describe())

# 1.8 Checking for outliers
plt.figure(figsize=(8, 6))
sns.boxplot(x=dataset['Probability.'])
plt.title('Boxplot of Probability with Outliers')
plt.show()

# 1.9 Remove Outliers
Q1 = dataset['Probability.'].quantile(0.25)
Q3 = dataset['Probability.'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
no_outliers = dataset[(dataset['Probability.'] >= lower_bound) & (dataset['Probability.'] <= upper_bound)]
print("Dimensions of no_outliers:", no_outliers.shape)
plt.figure(figsize=(8, 6))
sns.boxplot(x=no_outliers['Probability.'])
plt.title('Boxplot of Probability without Outliers')
plt.show()

# 2.1 Split Data into Training and Testing Sets
data['id'] = range(len(data))
training_dataset, testing_dataset = train_test_split(data, test_size=0.3, random_state=1)

# 2.2 Verifying if train and test data is in sync
print(len(data) == len(training_dataset) + len(testing_dataset))

# 2.3 Visualizing Original dataset with train and test data
plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=data, palette='Reds')
plt.title('Original Data')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=training_dataset, palette='Blues')
plt.title('Train Data')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=testing_dataset, palette='Greens')
plt.title('Test Data')
plt.show()
10/35:
# 2.3 Visualizing Original dataset with train and test data
plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=data, palette='Reds')
plt.title('Original Data')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=training_dataset, palette='Blues')
plt.title('Train Data')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=testing_dataset, palette='Greens')
plt.title('Test Data')
plt.show()
10/36:
# 2.3 Visualizing Original dataset with train and test data
plt.figure(figsize=(10, 6))
sns.countplot(x='Probability*', data=data, palette='Reds')
plt.title('Original Data')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='Probability*', data=training_dataset, palette='Blues')
plt.title('Train Data')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='Probability*', data=testing_dataset, palette='Greens')
plt.title('Test Data')
plt.show()
10/37:
plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=data, palette='Reds')
plt.title('Original Data')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=training_dataset, palette='Blues')
plt.title('Train Data')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=testing_dataset, palette='Greens')
plt.title('Test Data')
plt.show()
11/1:
# Read the CSV file

import pandas as pd
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)

# Print the content of the CSV file
dataset.head()
11/2:
# Read the CSV file

import pandas as pd
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
len(dataset)
dataset.shape
dataset.head()
11/3:
# Read the CSV file

import pandas as pd
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
len(dataset)
dataset.shape
dataset.head()
11/4:
# Read the CSV file

import pandas as pd
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
dataset.head()
print(len(dataset))
print(dataset.shape)
11/5:
# Read the CSV file

import pandas as pd
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
dataset.head()
print(len(dataset))
print(dataset.shape)
11/6:
# Read the CSV file

import pandas as pd
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
dataset.head()
11/7:
# 1.2 Check for Missing Values
na_counts = dataset.isna().sum()
print(na_counts)
print(len(dataset))
print(dataset.shape)
11/8:

# 1.3 Explore Categorical Variables
import matplotlib.pyplot as plt
import seaborn as sns

freq_table = dataset['Target Class', 'Target'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', y= 'Target' data=dataset)
plt.title('Categorical Data')
plt.show()
11/9:

# 1.3 Explore Target Class Variables
import matplotlib.pyplot as plt
import seaborn as sns

#  1.3 Explore Target Variables

freq_table = dataset['Target Class', 'Target'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.title('Categorical Data')
plt.show()

#  1.3.1 Explore Target Class Variables
freq_table = dataset['Target Class', 'Target'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x= 'Target', data=dataset)
plt.title('Categorical Data')
plt.show()
11/10:

# 1.3 Explore Target Class Variables
import matplotlib.pyplot as plt
import seaborn as sns

#  1.3 Explore Target Variables

freq_table = dataset['Target Class', 'Target'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.title('Categorical Data')
plt.show()

#  1.3.1 Explore Target Class Variables
freq_table = dataset['Target Class', 'Target'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x= 'Target', data=dataset)
plt.title('Categorical Data')
plt.show()
11/11:

# 1.3 Explore Target Class Variables
import matplotlib.pyplot as plt
import seaborn as sns

#  1.3 Explore Target Variables

freq_table = dataset['Target Class', 'Target'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.title('Categorical Data')
plt.show()
11/12:

# 1.3 Explore Target Class Variables
import matplotlib.pyplot as plt
import seaborn as sns

#  1.3 Explore Target Variables

freq_table = dataset['Target Class', 'Target'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.title('Target Class')
plt.show()
11/13:
# 1.3 Explore Target Class Variables
import matplotlib.pyplot as plt
import seaborn as sns

#  1.3 Explore Target Variables

freq_table = dataset['Target Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.title('Target Class')
plt.show()
11/14:
# 1.4 Explore and Visualize Numerical Variables
plt.figure(figsize=(8, 6))
sns.histplot(dataset['Probability*'])
plt.title('Probability Distribution')
plt.show()
11/15:
#  1.3.1 Explore Target Class Variables
freq_table = dataset['Target Class', 'Target'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x= 'Target', data=dataset)
plt.title('Categorical Data')
plt.show()
11/16:
#  1.3.1 Explore Target Class Variables
freq_table = dataset['Target'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x= 'Target', data=dataset)
plt.title('Target Data')
plt.show()
11/17:
# 2.1 Split Data into Training and Testing Sets
from sklearn.model_selection import train_test_split

# read the dataset
df = pd.read_csv('C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')
 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
11/18:
# 2.1 Split Data into Training and Testing Sets
from sklearn.model_selection import train_test_split

# read the dataset
df = pd.read_csv('C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')
 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
11/19:
# 2.1 Split Data into Training and Testing Sets
from sklearn.model_selection import train_test_split

# read the dataset
df = pd.read_csv('C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')
 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
11/20:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv('C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')
 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
11/21:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv('C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')
 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
11/22:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
11/23:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
11/24:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
print(X_train)
print(X_test)
print(y_train)
print(y_test)
11/25:
# Read the CSV file

import pandas as pd
file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
dataset.describe()
11/26:
# Read the CSV file

import pandas as pd
from IPython.display import display

file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
display(dataset)
11/27:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
display(X_train)
display(X_test)
display(y_train)
display(y_test)
11/28:
# 2.2 Verifying if train and test data is in sync
print(len(data) == len(X_train, y_train) + len(X_test, y_test))
11/29:
# 2.2 Verifying if train and test data is in sync
print(len(dataset) == len(X_train, y_train) + len(X_test, y_test))
11/30:
# 2.2 Verifying if train and test data is in sync
train = X_train + y_train
test = X_test + y_test
print(len(dataset) == len(train) + len(test))
11/31:
#  verifying if train and test data is in sync:
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'y_pred_prob' contains the predicted probabilities and 'y_true' contains the actual target values

# Create a DataFrame with the predicted probabilities and actual targets
Original_df = pd.DataFrame({Dataset(Target_Class), Dataset(Probability*)})

# Plot the joint plot for Target Class vs. Predicted Probability
sns.jointplot(data=result_df, x='Target Class', y='Predicted Probability', kind='scatter')
plt.title('Target Class vs. Predicted Probability')
plt.show()

# Plot the joint plot for Target vs. Predicted Probability
sns.jointplot(data=result_df, x='Target', y='Predicted Probability', kind='scatter')
plt.title('Target vs. Predicted Probability')
plt.show()



plt.figure(figsize=(15,5))
plt.subplot(1,3,1)
Y.value_counts().plot(kind = 'bar')
plt.title("Original")

plt.subplot(1,3,2)
y_train.value_counts().plot(kind = 'bar')
plt.title("Training set")

plt.subplot(1,3,3)
y_test.value_counts().plot(kind = 'bar')
plt.title("Test set")
plt.show()




plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=data, palette='Reds')
plt.title('Original Data')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=training_dataset, palette='Blues')
plt.title('Train Data')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='Probability', data=testing_dataset, palette='Greens')
plt.title('Test Data')
plt.show()
11/32:
#  verifying if train and test data is in sync:
from scipy import datasets
import seaborn as sns
import matplotlib.pyplot as plt

import pandas as pd

# Assuming Dataset is a DataFrame
Original = pd.DataFrame({
    'Target_Class': datasets['Target_Class'],
    'Probability': datasets['Probability*']
})

# Plot the joint plot for Target Class vs. Predicted Probability
sns.jointplot(data=Original, x='Target Class', y='Probability*', kind='bar')
plt.title('Target Class vs. Predicted Probability')
plt.show()

# Plot the joint plot for Target vs. Predicted Probability
sns.jointplot(data=result_df, x='Target', y='Predicted Probability', kind='bar')
plt.title('Original: Target vs. Predicted Probability')
plt.show()
11/33:
#  verifying if train and test data is in sync:
from scipy import datasets
import seaborn as sns
import matplotlib.pyplot as plt

import pandas as pd

# Assuming Dataset is a DataFrame
Original = pd.DataFrame({
    ['Target_Class']: datasets['Target_Class'],
    ['Probability']: datasets['Probability*']
})

# Plot the joint plot for Target Class vs. Predicted Probability
sns.jointplot(data=Original, x='Target Class', y='Probability*', kind='bar')
plt.title('Target Class vs. Predicted Probability')
plt.show()

# Plot the joint plot for Target vs. Predicted Probability
sns.jointplot(data=result_df, x='Target', y='Predicted Probability', kind='bar')
plt.title('Original: Target vs. Predicted Probability')
plt.show()
11/34:
#  verifying if train and test data is in sync:
from scipy import datasets
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming Dataset is a DataFrame
Original = pd.DataFrame({
    'Target_Class': datasets['Target_Class'],
    'Probability': datasets['Probability*']
})


# Plot the joint plot for Target Class vs. Predicted Probability
sns.jointplot(data=Original, x='Target Class', y='Probability*', kind='bar')
plt.title('Target Class vs. Predicted Probability')
plt.show()

# Plot the joint plot for Target vs. Predicted Probability
sns.jointplot(data=result_df, x='Target', y='Predicted Probability', kind='bar')
plt.title('Original: Target vs. Predicted Probability')
plt.show()
11/35:
#  verifying if train and test data is in sync:
from scipy import datasets
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming Dataset is a DataFrame
Original = pd.DataFrame({ datasets['Target_Class'], 'Probability': datasets['Probability*']})


# Plot the joint plot for Target Class vs. Predicted Probability
sns.jointplot(data=Original, x='Target Class', y='Probability*', kind='bar')
plt.title('Target Class vs. Predicted Probability')
plt.show()

# Plot the joint plot for Target vs. Predicted Probability
sns.jointplot(data=result_df, x='Target', y='Predicted Probability', kind='bar')
plt.title('Original: Target vs. Predicted Probability')
plt.show()
11/36:
#  verifying if train and test data is in sync:
from scipy import datasets
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming Dataset is a DataFrame
Original = pd.DataFrame({ datasets['Target_Class'], 'Probability': datasets['Probability*']})


# Plot the joint plot for Target Class vs. Predicted Probability
sns.jointplot(data=Original, x='Target Class', y='Probability*', kind='bar')
plt.title('Target Class vs. Predicted Probability')
plt.show()

# Plot the joint plot for Target vs. Predicted Probability
sns.jointplot(data=result_df, x='Target', y='Predicted Probability', kind='bar')
plt.title('Original: Target vs. Predicted Probability')
plt.show()
11/37:
#  verifying if train and test data is in sync:
from scipy import datasets
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming Dataset is a DataFrame
Original = pd.DataFrame({ datasets['Target_Class'], datasets['Probability*']})


# Plot the joint plot for Target Class vs. Predicted Probability
sns.jointplot(data=Original, x='Target Class', y='Probability*', kind='bar')
plt.title('Target Class vs. Predicted Probability')
plt.show()

# Plot the joint plot for Target vs. Predicted Probability
sns.jointplot(data=result_df, x='Target', y='Predicted Probability', kind='bar')
plt.title('Original: Target vs. Predicted Probability')
plt.show()
11/38:
#  verifying if train and test data is in sync:
from scipy import datasets
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")


# Plot the joint plot for Target Class vs. Predicted Probability
sns.jointplot(data=df, x='Target Class', y='Probability*', kind='bar')
plt.title('Target Class vs. Predicted Probability')
plt.show()

# Plot the joint plot for Target vs. Predicted Probability
sns.jointplot(data=result_df, x='Target', y='Predicted Probability', kind='bar')
plt.title('Original: Target vs. Predicted Probability')
plt.show()
11/39:
#  verifying if train and test data is in sync:
from scipy import datasets
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")


# Plot the joint plot for Target Class vs. Predicted Probability
sns.jointplot(data=df, x='Target Class', y='Probability*', kind='bar')
plt.title('Target Class vs. Predicted Probability')
plt.show()

# Plot the joint plot for Target vs. Predicted Probability
sns.jointplot(data=df, x='Target', y='Predicted Probability', kind='bar')
plt.title('Original: Target vs. Predicted Probability')
plt.show()
11/40:
#  verifying if train and test data is in sync:
from scipy import datasets
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")


# Plot the joint plot for Target Class vs. Predicted Probability
sns.jointplot(data=df, x='Target Class', y='Probability*', kind='hist')
plt.title('Target Class vs. Predicted Probability')
plt.show()

# Plot the joint plot for Target vs. Predicted Probability
sns.jointplot(data=df, x='Target', y='Predicted Probability', kind='hist')
plt.title('Original: Target vs. Predicted Probability')
plt.show()
11/41:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

# Plot the joint plot for Target Class vs. Predicted Probability
g1 = sns.jointplot(data=df, x='Target Class', y='Probability*', kind='scatter')
g1.set_axis_labels('Target Class', 'Predicted Probability', fontsize=12)
g1.fig.suptitle('Target Class vs. Predicted Probability', fontsize=16)
plt.subplots_adjust(top=0.9)  # Adjust the title position

# Plot the joint plot for Target vs. Predicted Probability
g2 = sns.jointplot(data=df, x='Target', y='Predicted Probability', kind='scatter')
g2.set_axis_labels('Target', 'Predicted Probability', fontsize=12)
g2.fig.suptitle('Original: Target vs. Predicted Probability', fontsize=16)
plt.subplots_adjust(top=0.9)  # Adjust the title position

# Show the plots
plt.show()
11/42:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

# Plot the joint plot for Target Class vs. Predicted Probability
g1 = sns.jointplot(data=df, x='Target Class', y='Probability*', kind='hist')
g1.set_axis_labels('Target Class', 'Predicted Probability', fontsize=12)
g1.fig.suptitle('Target Class vs. Predicted Probability', fontsize=16)
plt.subplots_adjust(top=0.9)  # Adjust the title position

# Plot the joint plot for Target vs. Predicted Probability with legend
plt.figure()  # Create a new figure
g2 = sns.scatterplot(data=df, x='Target', y='Probability*', hue='SomeCategory', palette='Set1')
g2.set_xlabel('Target', fontsize=12)
g2.set_ylabel('Predicted Probability', fontsize=12)
plt.title('Original: Target vs. Predicted Probability', fontsize=16)

# Show the plots
plt.show()
11/43:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

# Plot the joint plot for Target Class vs. Predicted Probability
g1 = sns.jointplot(data=df, x='Target Class', y='Probability*', kind='hist')
g1.set_axis_labels('Target Class', 'Predicted Probability', fontsize=12)
g1.fig.suptitle('Target Class vs. Predicted Probability', fontsize=16)
plt.subplots_adjust(top=0.9)  # Adjust the title position

# Plot the joint plot for Target vs. Predicted Probability with legend
plt.figure()  # Create a new figure
g2 = sns.scatterplot(data=df, x='Target', y='Probability*', hue='Target', palette='Set1')
g2.set_xlabel('Target', fontsize=12)
g2.set_ylabel('Predicted Probability', fontsize=12)
plt.title('Original: Target vs. Predicted Probability', fontsize=16)

# Show the plots
plt.show()
11/44:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")
colors = 'color'

plt.bar('Target Class', 'Probability*', colors = 'color')
plt.legend('Target Class', loc='center left', bbox_to_anchor=(1, 0.5))
plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

plt.bar('Target', 'Probability*', colors = 'color')
plt.legend('Target Class', loc='center left', bbox_to_anchor=(1, 0.5))
plt.title('Original: Target vs. Probability*')
plt.show()

# Show the plots
plt.show()
11/45:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

# Get the number of unique categories for 'Target Class'
num_categories = df['Target Class'].nunique()

# Use a colormap to generate colors
colors = plt.cm.viridis(range(num_categories))

# Plot the bar plot for Target Class vs. Probability*
plt.bar(df['Target Class'], df['Probability*'], color=colors)
plt.xlabel('Target Class')
plt.ylabel('Probability*')
plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

# Plot the bar plot for Target vs. Probability*
plt.bar(df['Target'], df['Probability*'], color=colors)
plt.xlabel('Target')
plt.ylabel('Probability*')
plt.title('Original: Target vs. Predicted Probability')
plt.show()
11/46:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

# Get the number of unique categories for 'Target Class'
num_categories = df['Target Class'].nunique()

# Use a colormap to generate colors
colors = plt.cm.viridis(range(num_categories))

# Plot the bar plot for Target Class vs. Probability*
plt.bar(df['Target Class'], df['Probability*'], color=colors)
plt.xlabel('Target Class')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')

# Remove x-axis labels
plt.xticks([])

plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

# Plot the bar plot for Target vs. Probability*
plt.bar(df['Target'], df['Probability*'], color=colors)
plt.xlabel('Target')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')

# Remove x-axis labels
plt.xticks([])

plt.title('Original: Target vs. Predicted Probability')
plt.show
11/47:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

# Get the number of unique categories for 'Target Class'
num_categories = df['Target Class'].nunique()

# Use a colormap to generate colors
colors = plt.cm.viridis(range(num_categories))

# Plot the bar plot for Target Class vs. Probability*
plt.bar(data['Target Class'], df['Probability*'], color=colors)
plt.xlabel('Target Class')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')

# Remove x-axis labels
plt.xticks([])

plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

# Plot the bar plot for Target vs. Probability*
plt.bar(df['Target'], df['Probability*'], color=colors)
plt.xlabel('Target')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')

# Remove x-axis labels
plt.xticks([])

plt.title('Original: Target vs. Predicted Probability')
plt.show
11/48:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

# Get the number of unique categories for 'Target Class'
num_categories = df['Target Class'].nunique()

# Use a colormap to generate colors
colors = plt.cm.viridis(range(num_categories))

# Plot the bar plot for Target Class vs. Probability*
plt.bar(unique_values, df['Probability*'], color=colors)
plt.xlabel('Target Class')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])
plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

# Plot the bar plot for Target vs. Probability*
plt.bar(df['Target'], df['Probability*'], color=colors)
plt.xlabel('Target')
plt.ylabel('Probability*')
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])

plt.title('Original: Target vs. Predicted Probability')
plt.show
11/49:
# 1.6 An input vector having duplicate values

unique_values = data['Target Class'].unique()
print(unique_values)
print(data['Probability*'].describe())
11/50:
# 1.6 An input vector having duplicate values

unique_values = data['Target Class'].unique()
print(unique_values)
print(data['Probability*'].describe())
11/51:
# 1.6 An input vector having duplicate values

unique_values = dataset['Target Class'].unique()
print(unique_values)
print(data['Probability*'].describe())
11/52:
# 1.6 An input vector having duplicate values

unique_values = dataset['Target Class'].unique()
print(unique_values)
11/53:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

# Get the number of unique categories for 'Target Class'
num_categories = df['Target Class'].nunique()

# Use a colormap to generate colors
colors = plt.cm.viridis(range(num_categories))

# Plot the bar plot for Target Class vs. Probability*
plt.bar(unique_values, df['Probability*'], color=colors)
plt.xlabel('Target Class')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])
plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

# Plot the bar plot for Target vs. Probability*
plt.bar(df['Target'], df['Probability*'], color=colors)
plt.xlabel('Target')
plt.ylabel('Probability*')
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])

plt.title('Original: Target vs. Predicted Probability')
plt.show
11/54:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

# Get the number of unique categories for 'Target Class'
num_categories = df['Target Class'].nunique()

# Use a colormap to generate colors
colors = plt.cm.viridis(range(num_categories))

# Plot the bar plot for Target Class vs. Probability*
plt.bar(unique_values, df['Probability*'], color=colors)
plt.xlabel('Target Class')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])
plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

# Plot the bar plot for Target vs. Probability*
plt.bar(df['Target'], df['Probability*'], color=colors)
plt.xlabel('Target')
plt.ylabel('Probability*')
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])

plt.title('Original: Target vs. Predicted Probability')
plt.show
11/55:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

# Get the number of unique categories for 'Target Class'
num_categories = df['Target Class'].nunique()

# Use a colormap to generate colors
colors = plt.cm.viridis(range(num_categories))

# Plot the bar plot for Target Class vs. Probability*
plt.bar(unique_values, df['Probability*'], color=colors)
plt.xlabel('Target Class')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])
plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

# Plot the bar plot for Target vs. Probability*
plt.bar(df['Target'], df['Probability*'], color=colors)
plt.xlabel('Target')
plt.ylabel('Probability*')
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])

plt.title('Original: Target vs. Predicted Probability')
plt.show
11/56:
# Get the number of unique categories for 'Target Class'
num_categories = df['Target Class'].nunique()
11/57:
# Get the number of unique categories for 'Target Class'
num_categories = df['Target Class'].nunique()
print(num_categories)
11/58:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

# 1.6 An input vector having duplicate values

unique_values = dataset['Target Class'].unique()
print(unique_values)

# Use a colormap to generate colors
colors = plt.cm.viridis(range(num_categories))

# Plot the bar plot for Target Class vs. Probability*
plt.bar(unique_values, df['Probability*'], color=colors)
plt.xlabel('Target Class')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])
plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

# Plot the bar plot for Target vs. Probability*
plt.bar(df['Target'], df['Probability*'], color=colors)
plt.xlabel('Target')
plt.ylabel('Probability*')
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])

plt.title('Original: Target vs. Predicted Probability')
plt.show
11/59:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

unique_values = df['Target Class'].unique()


# Use a colormap to generate colors
colors = plt.cm.viridis(range(num_categories))

# Plot the bar plot for Target Class vs. Probability*
plt.bar(unique_values, df['Probability*'], color=colors)
plt.xlabel('Target Class')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])
plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

# Plot the bar plot for Target vs. Probability*
plt.bar(df['Target'], df['Probability*'], color=colors)
plt.xlabel('Target')
plt.ylabel('Probability*')
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])

plt.title('Original: Target vs. Predicted Probability')
plt.show
11/60:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

unique_values = df['Target Class'].unique()


# Use a colormap to generate colors
colors = plt.cm.viridis(range(num_categories))

# Plot the bar plot for Target Class vs. Probability*
plt.bar(unique_values, df['Probability*'], color=colors)
plt.xlabel('Target Class')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])
plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

# Plot the bar plot for Target vs. Probability*
plt.bar(df['Target'], df['Probability*'], color=colors)
plt.xlabel('Target')
plt.ylabel('Probability*')
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])

plt.title('Original: Target vs. Predicted Probability')
plt.show
11/61:
# Assuming you've imported the necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the DataFrame from your CSV file
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Set style and color palette
sns.set(style="whitegrid")
sns.set_palette("husl")

unique_values = df['Target Class'].unique()


# Use a colormap to generate colors
colors = plt.cm.viridis(range(num_categories))

# Plot the bar plot for Target Class vs. Probability*
plt.bar(unique_values, df['Probability*'], color=colors)
plt.xlabel('Target Class')
plt.ylabel('Probability*')

# Display the actual probability values on the bars
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])
plt.title('Original: Target Class vs. Predicted Probability')
plt.show()

# Plot the bar plot for Target vs. Probability*
plt.bar(df['Target'], df['Probability*'], color=colors)
plt.xlabel('Target')
plt.ylabel('Probability*')
for i, v in enumerate(df['Probability*']):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')
plt.xticks([])

plt.title('Original: Target vs. Predicted Probability')
plt.show
11/62:
import matplotlib.pyplot as plt

# Sample data
categories = ['Target Class', 'Probability*']
fig, ax = plt.subplots()
bar_width = 0.35
x = range(len(categories))
bar1 = ax.bar(x, df['Target Class'], width=bar_width, label='Group 1')
bar2 = ax.bar([i + bar_width for i in x], df['Probability*'], width=bar_width, label='Group 2')
ax.legend()
ax.set_xticks([])
plt.show()
11/63:
import matplotlib.pyplot as plt

# Sample data
categories = ['Target Class', 'Probability*']
value1 = df['Target Class']
value2 = df['Probability*']
fig, ax = plt.subplots()
bar_width = 0.35
x = range(len(categories))
bar1 = ax.bar(x, value1, width=bar_width, label='Group 1')
bar2 = ax.bar([i + bar_width for i in x], value2, width=bar_width, label='Group 2')
ax.legend()
ax.set_xticks([])
plt.show()
11/64:
import matplotlib.pyplot as plt

# Sample data

# 1.3 Explore Target Class Variables
import matplotlib.pyplot as plt
import seaborn as sns

#  1.3 Explore Target Variables

freq_table = dataset['Target Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', y = dataset['Probability*'], data=dataset)
fig, ax = plt.subplots()
ax.legend()
ax.set_xticks([])
plt.title('Target Class')
plt.show()
11/65:
import matplotlib.pyplot as plt

# Sample data

# 1.3 Explore Target Class Variables
import matplotlib.pyplot as plt
import seaborn as sns

#  1.3 Explore Target Variables

freq_table = dataset['Target Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', y = dataset['Probability*'], data=dataset, label = "count")
fig, ax = plt.subplots()
ax.legend()
ax.set_xticks([])
plt.title('Target Class')
plt.show()
11/66:
import matplotlib.pyplot as plt

# Sample data

# 1.3 Explore Target Class Variables
import matplotlib.pyplot as plt
import seaborn as sns

#  1.3 Explore Target Variables

freq_table = dataset['Target Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', y = dataset['Probability*'], data=dataset, label = "count")
fig, ax = plt.subplots()
ax.legend()
ax.set_xticks([])
sns.countplot(x='Target Class', data=dataset)
plt.show()
11/67:
import matplotlib.pyplot as plt
import seaborn as sns

# Count plot for 'Target Class'
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.show()

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.barplot(x='Target Class', y='Probability*', data=dataset)
plt.show()
11/68:
import matplotlib.pyplot as plt
import seaborn as sns

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.barplot(x='Target Class', y='Probability*', data=dataset)
plt.show()
11/69:
import matplotlib.pyplot as plt
import seaborn as sns

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.barplot(x='Target Class', y='Probability*', data=dataset)
plt.xticks([])
plt.legend(title='Legend Title')
plt.show()
11/70:
Target_class = dataset['Target_class']
print[Target_class]
11/71:
Target_class = dataset['Target_Class']
print[Target_class]
11/72:
import matplotlib.pyplot as plt
import seaborn as sns

Target_class = dataset['Target_class']

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.barplot(x='Target Class', y='Probability*', data=dataset)
plt.xticks([])
plt.legend("Target Class", loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
11/73:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"]

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.barplot(x='Target Class', y='Probability*', data=dataset)
plt.xticks([])
plt.legend(Target_Class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
11/74:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.barplot(x='Target Class', y='Probability*', data=dataset)
plt.xticks([])
plt.legend(Target_Class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
11/75:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.barplot(x='Target Class', y='Probability*', data=dataset)
plt.xticks([])
plt.legend(Target_Class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
11/76:
import matplotlib.pyplot as plt
import seaborn as sns

# Generate 21 different colors
colors = plt.cm.viridis_r(range(21))

# Display the colors
plt.figure(figsize=(8, 1))
for i, color in enumerate(colors):
    plt.fill_between([i, i+1], 0, 1, color=color)

Target_Class = dataset["Target Class"].unique()
# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.barplot(x='Target Class', y='Probability*', data=dataset, color= colors)
plt.xticks([])
plt.legend(Target_Class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
11/77:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()
colors = plt.cm.viridis_r(range(21))

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.barplot(x='Target Class', y='Probability*', data=dataset, color= colors)
plt.xticks([])
plt.legend(Target_Class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
11/78:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()
colors = plt.cm.viridis_r(range(21))

# Bar plot for 'Probability*'
plt.figure(figsize=(4, 6))
sns.barplot(x='Target Class', y='Probability*', data=dataset, color= colors)
plt.xticks([])
plt.legend(Target_Class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
11/79:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()
colors = plt.cm.viridis_r(range(21))

# Bar plot for 'Probability*'
plt.figure(figsize=(4, 3))
sns.barplot(x='Target Class', y='Probability*', data=dataset, color= colors)
plt.xticks([])
plt.legend(Target_Class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
11/80:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()
colors = sns.color_palette('husl', n_colors=21)

# Bar plot for 'Probability*'
plt.figure(figsize=(4, 3))
sns.barplot(x='Target Class', y='Probability*', data=dataset, color= colors)
plt.xticks([])
plt.legend(Target_Class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
11/81:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Bar plot for 'Probability*'
plt.figure(figsize=(4, 3))
sns.barplot(x='Target Class', y='Probability*', data=dataset, color= colors)
plt.xticks([])
plt.legend(Target_Class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
11/82:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Bar plot for 'Probability*'
plt.figure(figsize=(4, 3))
sns.barplot(x='Target Class', y='Probability*', data=dataset, palette=colors)
plt.xticks([])
plt.legend(Target_Class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
11/83:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset['Target_Class'], kde=True, color='skyblue', bins=30)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/84:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()


# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset['Target_Class'], kde=True, color='skyblue', bins=30)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/85:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()


# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color='skyblue', bins=30)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/86:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()


# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color='skyblue', bins=30)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/87:
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color=colors, bins=21)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/88:
import matplotlib.pyplot as plt
import seaborn as sns

num_bins = 21
hist, bins = np.histogram(data, bins=num_bins)
colors = plt.cm.viridis(np.linspace(0, 1, len(hist)))

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color=colors, bins=21)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/89:
import matplotlib.pyplot as plt
import seaborn as sns

num_bins = 21
hist, bins = np.hist(data, bins=num_bins)
colors = plt.cm.viridis(np.linspace(0, 1, len(hist)))

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color=colors, bins=21)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/90:
import matplotlib.pyplot as plt
import seaborn as sns

num_bins = 21
hist, bins = np.hist(data, bins=num_bins)
colors = plt.cm.viridis(np.linspace(0, 1, len(hist)))

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color=colors, bins=21)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/91:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

num_bins = 21
hist, bins = np.hist(data, bins=num_bins)
colors = plt.cm.viridis(np.linspace(0, 1, len(hist)))

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color=colors, bins=21)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/92:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

num_bins = 21
hist, bins = np.histogram(data, bins=num_bins)
colors = plt.cm.viridis(np.linspace(0, 1, len(hist)))

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color=colors, bins=21)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/93:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

num_bins = 21
hist, bins = np.histogram(data, bins=num_bins)
colors = plt.cm.viridis(np.linspace(0, 1, len(hist)))

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color=colors, bins=21)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/94:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

num_bins = 21
hist, bins = np.histogram(dataset, bins=num_bins)
colors = plt.cm.viridis(np.linspace(0, 1, len(hist)))

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color=colors, bins=21)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
plt.title('Histogram of Probability*')
plt.show()
11/95:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

num_bins = 21
hist, bins = np.histogram(dataset, bins=num_bins)
colors = plt.cm.viridis(np.linspace(0, 1, len(hist)))

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color=colors, bins=21)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
ax.legend()
ax.set_xticks([])
plt.legend("Target_Class", loc='center left', bbox_to_anchor=(1, 0.5))
plt.title('Histogram of Probability*')
plt.show()
11/96:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

num_bins = 21
hist, bins = np.histogram(dataset, bins=num_bins)
colors = plt.cm.viridis(np.linspace(0, 1, len(hist)))

# Bar plot for 'Probability*'
plt.figure(figsize=(8, 6))
sns.histplot(dataset["Target Class"], kde=True, color=colors, bins=21)
plt.xlabel('Target_Class')
plt.ylabel('Probability*')
ax.legend()
ax.set_xticks([])
plt.legend("Target_Class", loc='center left', bbox_to_anchor=(1, 0.5))
plt.title('Histogram of Probability*')
plt.show()
11/97: Target_Class = dataset["Target Class"].unique()
11/98:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()
Probability = dataset["Probability*"]

num_bins = 21
hist, bins = np.histogram(dataset, bins=num_bins)
colors = plt.cm.viridis(np.linspace(0, 1, len(hist)))

# Bar plot for 'Probability*'
# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(Target_Class, Probability, color=colors)
ax.legend()
ax.set_xticks([])
plt.xlabel('Categories')
plt.ylabel('Percentage')
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
11/99:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

Target_Class = dataset["Target Class"].unique()
Probability = dataset["Probability*"]

num_bins = 21
hist, bins = np.histogram(Probability, bins=num_bins)
colors = plt.cm.viridis(np.linspace(0, 1, len(hist)))

# Bar plot for 'Probability*'
# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(Target_Class, Probability, color=colors)
plt.xticks([])
plt.xlabel('Categories')
plt.ylabel('Percentage')
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
11/100:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()

# Initialize a list to store the histograms
histograms = []

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

for category in Target_Class:
    category_data = dataset[dataset["Target Class"] == category]["Probability*"]
    hist, bins = np.histogram(category_data, bins=num_bins, range=(0, 1))
    histograms.append(hist)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the histograms with different colors
for i, hist in enumerate(histograms):
    ax.bar(bins[:-1], hist, width=(bins[1]-bins[0]), color=colors[i], alpha=0.5, label=Target_Class[i])

# Add labels and title
plt.xlabel('Probability*')
plt.ylabel('Frequency')
plt.title('Histogram of Probability* by Target Class')
plt.legend(loc='upper right')
plt.show()
11/101:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()
Probability = dataset["Probability*"].unique()

# Initialize a list to store the histograms
histograms = []

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

for category in Target_Class:
    category_data = dataset[dataset["Target Class"] == category]["Probability*"]
    hist, bins = np.histogram(category_data, bins=num_bins, range=(0, 1))
    histograms.append(hist)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the histograms with different colors
for i, hist in enumerate(histograms):
    ax.bar(bins[:-1], hist, width=(bins[1]-bins[0]), color=colors[i], alpha=0.5, label=Target_Class[i])

# Add labels and title
plt.xlabel('Probability*')
plt.ylabel('Frequency')
plt.title('Histogram of Probability* by Target Class')
plt.legend(loc='upper right')
plt.show()
11/102:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()
Probability = dataset["Probability*"].unique()

# Initialize a list to store the histograms
histograms = []

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

for category in Target_Class:
    category_data = dataset[dataset["Target Class"] == category]["Probability*"]
    hist, bins = np.histogram(category_data, bins=num_bins, range=(0, 1))
    histograms.append(hist)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the histograms with different colors
for i, hist in enumerate(histograms):
    ax.bar(bins[:-1], hist, width=(bins[1]-bins[0]), color=colors[i], alpha=0.5, label=Target_Class[i])

# Add labels and title
plt.ylabellabel('Probability')
plt.xlabel('Target Class')
plt.title('Histogram of Probability* by Target Class')
plt.legend(loc='upper right')
plt.show()
11/103:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()
Probability = dataset["Probability*"].unique()

# Initialize a list to store the histograms
histograms = []

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

for category in Target_Class:
    category_data = dataset[dataset["Target Class"] == category]["Probability*"]
    hist, bins = np.histogram(category_data, bins=num_bins, range=(0, 1))
    histograms.append(hist)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the histograms with different colors
for i, hist in enumerate(histograms):
    ax.bar(bins[:-1], hist, width=(bins[1]-bins[0]), color=colors[i], alpha=0.5, label=Target_Class[i])

# Add labels and title
plt.ylabel('Probability')
plt.xlabel('Target Class')
plt.title('Histogram of Probability* by Target Class')
plt.legend(loc='upper right')
plt.show()
11/104:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()

# Initialize a list to store the histograms
histograms = []

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

for category in Target_Class:
    category_data = dataset[dataset["Target Class"] == category]["Probability*"]
    hist, bins = np.histogram(category_data, bins=num_bins, range=(0, 1))
    histograms.append(hist)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the histograms with different colors
for i, hist in enumerate(histograms):
    ax.bar(bins[:-1], hist, width=(bins[1]-bins[0]), color=colors[i], alpha=0.5, label=Target_Class[i])

# Add labels and title
plt.xlabel('Probability*')
plt.ylabel('Frequency')
plt.title('Histogram of Probability* by Target Class')
plt.legend(loc='upper right')
plt.show()
11/105:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()

# Initialize a list to store the histograms
histograms = []

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

for category in Target_Class:
    category_data = dataset[dataset["Target Class"] == category]["Probability*"]
    hist, bins = np.histogram(category_data, bins=num_bins, range=(0, 1))
    histograms.append(hist)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the histograms with different colors
for i, hist in enumerate(histograms):
    ax.bar(bins[:-1], hist, color=colors[i], alpha=0.5, label=Target_Class[i])

# Add labels and title
plt.xlabel('Probability*')
plt.ylabel('Frequency')
plt.title('Histogram of Probability* by Target Class')
plt.legend(loc='upper right')
plt.show()
11/106:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()

# Initialize a list to store the histograms
histograms = []

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

for category in Target_Class:
    category_data = dataset[dataset["Target Class"] == category]["Probability*"]
    hist, bins = np.histogram(category_data, bins=num_bins, range=(0, 1))
    histograms.append(hist)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the histograms with different colors
for i, hist in enumerate(histograms):
    ax.bar(bins[:-1], hist, color=colors[i], alpha=0.5, label=Target_Class[i])

# Add labels and title
penguins = sns.load_dataset("Target_Class")
sns.histplot(data=penguins, x="Target_Class")
plt.xlabel('Probability*')
plt.ylabel('Frequency')
plt.title('Histogram of Probability* by Target Class')
plt.legend(loc='upper right')
plt.show()
11/107:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()

# Initialize a list to store the histograms
histograms = []

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

for category in Target_Class:
    category_data = dataset[dataset["Target Class"] == category]["Probability*"]
    hist, bins = np.histogram(category_data, bins=num_bins, range=(0, 1))
    histograms.append(hist)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the histograms with different colors
for i, hist in enumerate(histograms):
    ax.bar(bins[:-1], hist, color=colors[i], alpha=0.5, label=Target_Class[i])

# Add labels and title
penguins = sns.load_dataset("Target_Class")
sns.histplot(data=dataset, x="Target_Class")
plt.xlabel('Probability*')
plt.ylabel('Frequency')
plt.title('Histogram of Probability* by Target Class')
plt.legend(loc='upper right')
plt.show()
11/108:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

# Create subplots
fig, axs = plt.subplots(len(Target_Class), 1, figsize=(8, 6), sharex=True)

for i, category in enumerate(Target_Class):
    category_data = dataset[dataset["Target Class"] == category]["Probability*"]
    hist, bins = np.histogram(category_data, bins=num_bins, range=(0, 1))

    # Plot the histogram for the current category
    axs[i].bar(bins[:-1], hist, color=colors[i], alpha=0.5)
    axs[i].set_ylabel('Frequency')
    axs[i].set_title(f'Histogram of Probability* for {category}')
    axs[i].label_outer()

# Add labels and title
plt.xlabel('Probability*')
plt.tight_layout()
plt.show()
11/109:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

# Create subplots
fig, axs = plt.subplots(len(Target_Class), 1, figsize=(8, 6), sharex=True)

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(Target_Class, color=colors)
plt.xticks()
plt.xlabel('Categories')
plt.ylabel('Percentage')
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
11/110:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

# Create subplots
fig, axs = plt.subplots(len(Target_Class), 1, figsize=(8, 6))

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(Target_Class, color=colors)
plt.xticks()
plt.xlabel('Categories')
plt.ylabel('Percentage')
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
11/111:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

# Create subplots
fig, axs = plt.subplots(len(Target_Class), 1, figsize=(8, 6))

# Create a bar plot
plt.figure(figsize=(15, 5))
plt.bar(Target_Class, color=colors)
plt.subplot(1,3,1)
plt.xticks()
plt.xlabel('Categories')
plt.ylabel('Percentage')
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
11/112:
import numpy as np
import matplotlib.pyplot as plt
from scipy import datasets
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

# Create subplots
fig, axs = plt.subplots(len(Target_Class), 1, figsize=(8, 6))

# Create a bar plot
sns.histplot(data=datasets, x="Target_Class", bins=21)
plt.figure(figsize=(15, 5))
plt.bar(Target_Class, color=colors)
plt.subplot(1,3,1)
plt.xticks()
plt.xlabel('Categories')
plt.ylabel('Percentage')
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
11/113:
import numpy as np
import matplotlib.pyplot as plt
from scipy import datasets
import seaborn as sns

# Assuming 'dataset' is a pandas DataFrame
Target_Class = dataset["Target Class"].unique()

num_bins = 21
colors = plt.cm.viridis(np.linspace(0, 1, num_bins))

# Create a bar plot
sns.histplot(data=datasets, x="Target_Class", bins=21)
plt.figure(figsize=(15, 5))
plt.bar(Target_Class, color=colors)
plt.subplot(1,3,1)
plt.xticks()
plt.xlabel('Categories')
plt.ylabel('Percentage')
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
11/114:
import matplotlib.pyplot as plt

# Sample data (replace with your actual data)
Target_Class = dataset["Target Class"].unique()
probabilities = dataset["Probability*"].unique()

# Create a dictionary to store data for each class
class_data = {}
for cls, prob in zip("Target Class", "Probability*"):
    if cls not in class_data:
        class_data[cls] = []
    class_data[cls].append(prob)

# Plotting
plt.figure(figsize=(10, 6))

for cls, prob_list in class_data.items():
    plt.hist(prob_list, bins=21, alpha=0.5, label=cls)

plt.xlabel('Probability')
plt.ylabel('Frequency')
plt.legend(title='Target Class')
plt.title('Probability Distribution for Each Target Class')
plt.show()
12/1:
import matplotlib.pyplot as plt

# Define the categories
Target_Class = dataset["Target Class"].unique()
Probability = dataset["Probability*"].unique()


# Adding colors
colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 
          'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(Target_Class, Probability, color=colors)
plt.xticks()
plt.xlabel('Target Class')
plt.ylabel('Probability')
plt.title("Probability Distribution for Each Target Class")
plt.show()
12/2:
import matplotlib.pyplot as plt

# Define the categories
Target_Class = dataset["Target Class"].unique()
Probability = dataset["Probability*"].unique()


# Adding colors
colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 
          'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(Target_Class, Probability, color=colors)
plt.xticks()
plt.xlabel('Target Class')
plt.ylabel('Probability')
plt.title("Probability Distribution for Each Target Class")
plt.show()
12/3:
# Get the number of unique categories for 'Target Class'
num_categories = df['Target Class'].nunique()
print(num_categories)
12/4: Target_Class = dataset["Target Class"].unique()
12/5:
# Read the CSV file

import pandas as pd
from IPython.display import display

file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
display(dataset)
12/6:
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']
12/7: Target_Class = dataset["Target Class"].unique()
12/8: Target_Class = dataset["Target Class"].unique()
12/9:
import matplotlib.pyplot as plt

# Define the categories
Target_Class = dataset['Target Class'].unique()
Probability = dataset['Probability*'].unique()


# Adding colors
colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 
          'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(Target_Class, Probability, color=colors)
plt.xticks()
plt.xlabel('Target Class')
plt.ylabel('Probability')
plt.title("Probability Distribution for Each Target Class")
plt.show()
12/10:
import matplotlib.pyplot as plt
import numpy as np

# Assuming Probability is a 2D array with shape (21, 4)
# If Probability is a 1D array, you might need to reshape it
# Probability = Probability.reshape(21, 4)

# Define the categories
Target_Class = dataset['Target Class'].unique()
Probability = dataset['Probability*'].reshape(21, 4)

# Adding colors
colors = ['red', 'green', 'blue', 'yellow']

# Create a bar plot
plt.figure(figsize=(10, 6))

# Define the width of each bar
bar_width = 0.2

# Set the positions for the bars
r1 = np.arange(len(Target_Class))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]
r4 = [x + bar_width for x in r3]

# Create the grouped bar plot
plt.bar(r1, Probability[:,0], color=colors[0], width=bar_width, edgecolor='grey', label='Probability 1')
plt.bar(r2, Probability[:,1], color=colors[1], width=bar_width, edgecolor='grey', label='Probability 2')
plt.bar(r3, Probability[:,2], color=colors[2], width=bar_width, edgecolor='grey', label='Probability 3')
plt.bar(r4, Probability[:,3], color=colors[3], width=bar_width, edgecolor='grey', label='Probability 4')

# Add labels and title
plt.xlabel('Target Class')
plt.ylabel('Probability')
plt.title('Probability Distribution for Each Target Class')
plt.xticks([r + bar_width for r in range(len(Target_Class))], Target_Class)
plt.legend()

# Show the plot
plt.show()
12/11:
import matplotlib.pyplot as plt
import numpy as np

# Assuming Probability is a 1D array-like object
Probability = np.array(dataset['Probability*'])

# Reshape Probability if needed
Probability = Probability.reshape(21, 4)

# Define the categories
Target_Class = dataset['Target Class'].unique()

# Adding colors
colors = ['red', 'green', 'blue', 'yellow']

# Create a bar plot
plt.figure(figsize=(10, 6))

# Define the width of each bar
bar_width = 0.2

# Set the positions for the bars
r1 = np.arange(len(Target_Class))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]
r4 = [x + bar_width for x in r3]

# Create the grouped bar plot
plt.bar(r1, Probability[:,0], color=colors[0], width=bar_width, edgecolor='grey', label='Probability 1')
plt.bar(r2, Probability[:,1], color=colors[1], width=bar_width, edgecolor='grey', label='Probability 2')
plt.bar(r3, Probability[:,2
12/12:
import matplotlib.pyplot as plt
import numpy as np

# Assuming Probability is a 1D array-like object
Probability = np.array(dataset['Probability*'])

# Reshape Probability if needed
Probability = Probability.reshape(21, 4)

# Define the categories
Target_Class = dataset['Target Class'].unique()

# Adding colors
colors = ['red', 'green', 'blue', 'yellow']

# Create a bar plot
plt.figure(figsize=(10, 6))

# Define the width of each bar
bar_width = 0.2

# Set the positions for the bars
r1 = np.arange(len(Target_Class))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]
r4 = [x + bar_width for x in r3]

# Create the grouped bar plot
plt.bar(r1, Probability[:,0], color=colors[0], width=bar_width, edgecolor='grey', label='Probability 1')
plt.bar(r2, Probability[:,1], color=colors[1], width=bar_width, edgecolor='grey', label='Probability 2')
plt.bar(r3, Probability[:,2]
12/13:
import matplotlib.pyplot as plt
import numpy as np

# Assuming Probability is a 1D array-like object
Probability = np.array(dataset['Probability*'])

# Reshape Probability if needed
Probability = Probability.reshape(21, 4)

# Define the categories
Target_Class = dataset['Target Class'].unique()

# Adding colors
colors = ['red', 'green', 'blue', 'yellow']

# Create a bar plot
plt.figure(figsize=(10, 6))

# Define the width of each bar
bar_width = 0.2

# Set the positions for the bars
r1 = np.arange(len(Target_Class))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]
r4 = [x + bar_width for x in r3]

# Create the grouped bar plot
plt.bar(r1, Probability[:,0], color=colors[0], width=bar_width, edgecolor='grey', label='Probability 1')
plt.bar(r2, Probability[:,1], color=colors[1], width=bar_width, edgecolor='grey', label='Probability 2')
plt.bar(r3, Probability[:,2])
12/14:
import matplotlib.pyplot as plt
import numpy as np

# Assuming Probability is a 1D array-like object
Probability = np.array(dataset['Probability*'])

# Reshape Probability into a 2D array with 21 rows and 4 columns
Probability = Probability[:84].reshape(21, 4)

# Define the categories
Target_Class = dataset['Target Class'].unique()

# Adding colors
colors = ['red', 'green', 'blue', 'yellow']

# Create a bar plot
plt.figure(figsize=(10, 6))

# Set the positions for the bars
r = np.arange(len(Target_Class))

# Create the stacked bar plot
for i in range(4):
    plt.bar(r, Probability[:, i], color=colors[i], edgecolor='grey', label=f'Probability {i+1}')

# Add labels and title
plt.xlabel('Target Class')
plt.ylabel('Probability')
plt.title('Probability Distribution for Each Target Class')
plt.xticks(r, Target_Class)
plt.legend()

# Show the plot
plt.show()
12/15:
# Checking for unique values 
Target_Class = dataset['Target Class'].unique()
display(Target_Class)
Probability = dataset['Probability*'].unique
display(Probability)
12/16:
# Checking for unique values 
Target_Class = dataset['Target Class'].unique()
display(Target_Class)
Probability = dataset['Probability*'].unique()
display(Probability)
12/17:
import matplotlib.pyplot as plt

# Assuming you have defined 'percentages' and 'categories'
Probabilities = [0.10616576, 0., 1., 0.18945832]
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
plt.bar(Target_class, Probabilities, color=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.xticks([])
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/18:
import matplotlib.pyplot as plt

# Assuming you have defined 'percentages' and 'categories'
Probabilities = [0.10616576, 0., 1., 0.18945832]
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
plt.bar(Target_class, Probabilities, color=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.xticks([])
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/19:
import matplotlib.pyplot as plt

# Assuming you have defined 'percentages' and 'categories'
Probabilities = [0.10616576, 0., 1., 0.18945832]
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
plt.bar(Target_class, Probabilities, color=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.xticks([])
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/20:
import matplotlib.pyplot as plt

# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
plt.bar(Target_class, color=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.xticks([])
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/21:
import matplotlib.pyplot as plt

# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
sns.countplot(x='Target Class', data=dataset, color=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.xticks([])
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/22:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
sns.countplot(x='Target Class', data=dataset, color=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.xticks([])
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/23:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
counts = dataset['Target Class'].value_counts().values
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(Target_class, counts, color=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.xlabel('Target Class')
plt.ylabel('Count')
plt.xticks([])
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/24: display(Target_class)
12/25:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
counts = dataset['Target Class'].value_counts().values
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(Target_class, counts, color=colors)
for bar, category in zip(Target_class):
    bar.set_label(Target_class)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.xlabel('Target Class')
plt.ylabel('Count')
plt.xticks([])
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/26:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
counts = dataset['Target Class'].value_counts().values
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(Target_class, counts, color=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.xlabel('Target Class')
plt.ylabel('Count')
plt.xticks([])
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/27:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
Probability = dataset['Probability*'].value_counts().values
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(Target_class, Probability, color=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.xlabel('Target Class')
plt.ylabel('Count')
plt.xticks([])
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/28:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
Probability = dataset['Probability*'].value_counts().values
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
# Create a pie chart
plt.pie(Probability, colors=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/29:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme',
       'Family A G protein-coupled receptor',
       'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase',
       'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein',
       'Other nuclear protein', 'Oxidoreductase', 'Phosphatase',
       'Phosphodiesterase', 'Primary active transporter', 'Protease',
       'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase',
       'Unclassified protein', 'Voltage-gated ion channel']
Probability = dataset['Probability*'].value_counts().values
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
# Create a pie chart
plt.pie(Probability, colors=colors)
plt.legend(dataset['Target_class'], loc='center left', bbox_to_anchor=(1, 0.5))
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/30:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme', 'Family A G protein-coupled receptor', 'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase', 'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein', 'Other nuclear protein', 'Oxidoreductase', 'Phosphatase', 'Phosphodiesterase', 'Primary active transporter', 'Protease', 'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase', 'Unclassified protein', 'Voltage-gated ion channel']
Probability = dataset['Probability*'].value_counts().values
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
# Create a pie chart
plt.pie(Probability, colors=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/31:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme', 'Family A G protein-coupled receptor', 'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase', 'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein', 'Other nuclear protein', 'Oxidoreductase', 'Phosphatase', 'Phosphodiesterase', 'Primary active transporter', 'Protease', 'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase', 'Unclassified protein', 'Voltage-gated ion channel']
Probability = dataset['Probability*']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
# Create a pie chart
plt.pie(Probability, colors=colors)
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/32:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme', 'Family A G protein-coupled receptor', 'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase', 'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein', 'Other nuclear protein', 'Oxidoreductase', 'Phosphatase', 'Phosphodiesterase', 'Primary active transporter', 'Protease', 'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase', 'Unclassified protein', 'Voltage-gated ion channel']
Probability = dataset['Probability*']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
# Create a pie chart
plt.pie(Probability, colors=colors)
plt.legend(Target_class  + Probability, loc='center left', bbox_to_anchor=(1, 0.5))
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/33:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = Target_Class_unique
Probability = dataset['Probability*']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
# Create a pie chart
plt.pie(Probability, colors=colors)
plt.legend(Target_class, Probability, loc='center left', bbox_to_anchor=(1, 0.5))
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/34:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = dataset(Target_Class_unique)
Probability = dataset['Probability*']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
# Create a pie chart
plt.pie(Probability, colors=colors)
plt.legend(Target_class, Probability, loc='center left', bbox_to_anchor=(1, 0.5))
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/35:
# Checking for unique values 
Target_Class_unique = dataset['Target Class'].unique()
display(Target_Class_unique)
Probability_unique = dataset['Probability*'].unique()
display(Probability_unique)
12/36:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = dataset(Target_Class_unique)
Probability = dataset['Probability*']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a bar plot
# Create a pie chart
plt.pie(Probability, colors=colors)
plt.legend(Target_class, Probability, loc='center left', bbox_to_anchor=(1, 0.5))
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/37:
import matplotlib.pyplot as plt
import seaborn as sns



# Assuming you have defined 'percentages' and 'categories'
Target_class = ['Cytochrome P450', 'Electrochemical transporter', 'Enzyme', 'Family A G protein-coupled receptor', 'Family C G protein-coupled receptor', 'Hydrolase', 'Kinase', 'Membrane receptor', 'Nuclear receptor', 'Other cytosolic protein', 'Other nuclear protein', 'Oxidoreductase', 'Phosphatase', 'Phosphodiesterase', 'Primary active transporter', 'Protease', 'Secreted protein', 'Toll-like and Il-1 receptors', 'Transferase', 'Unclassified protein', 'Voltage-gated ion channel']
Probability = dataset['Probability*']
colors = ['red', 'green', 'blue', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black', 'white', 
          'brown', 'pink', 'gray', 'lime', 'teal', 'olive', 'navy', 'maroon', 'aqua', 'coral', 'indigo']

# Create a pie chart
plt.pie(Probability, colors=colors)
plt.legend(Target_class, Probability, loc='center left', bbox_to_anchor=(1, 0.5))
plt.title("EPLERENONE DRUG TARGET PREDICTION")
plt.show()
12/38:
import matplotlib.pyplot as plt

# Assuming you have 'Target Class' and 'Probability' columns in your dataset
# Replace 'dataset' with your actual DataFrame name if it's different

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(dataset['Probability'], dataset['Target Class'], color='red', width=0.5)

# Add text labels
for i, count in enumerate(dataset['Target Class']):
    plt.text(i, count + 0.2, str(count), ha='center', va='bottom', fontsize=8)

# Add labels and title
plt.xlabel('Probability')
plt.ylabel('Frequency')
plt.title('Original Data')

# Show the plot
plt.show()
12/39:
import matplotlib.pyplot as plt

# Assuming you have 'Target Class' and 'Probability' columns in your dataset
# Replace 'dataset' with your actual DataFrame name if it's different

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(dataset['Probability*'], dataset['Target Class'], color='red', width=0.5)

# Add text labels
for i, count in enumerate(dataset['Target Class']):
    plt.text(i, count + 0.2, str(count), ha='center', va='bottom', fontsize=8)

# Add labels and title
plt.xlabel('Probability')
plt.ylabel('Frequency')
plt.title('Original Data')

# Show the plot
plt.show()
12/40:
import matplotlib.pyplot as plt

# Assuming you have 'Target Class' and 'Probability' columns in your dataset
# Replace 'dataset' with your actual DataFrame name if it's different

# Get unique classes and set up colors
unique_classes = dataset['Target Class'].unique()
colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar plot with separate colors for each class
plt.figure(figsize=(10, 6))
for i, class_name in enumerate(unique_classes):
    class_data = dataset[dataset['Target Class'] == class_name]
    plt.bar(class_data['Probability*'], class_data['Target Class'], color=colors[i], alpha=0.5, label=class_name, width=0.5)

# Add labels and title
plt.xlabel('Probability')
plt.ylabel('Frequency')
plt.title('Original Data')

# Add legends
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

# Show the plot
plt.show()
12/41:
import matplotlib.pyplot as plt

# Assuming you have 'Target Class' and 'Probability' columns in your dataset
# Replace 'dataset' with your actual DataFrame name if it's different

# Get unique classes and set up colors
unique_classes = dataset['Target Class'].unique()
colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar plot with separate colors for each class
plt.figure(figsize=(10, 6))
for i, class_name in enumerate(unique_classes):
    class_data = dataset[dataset['Probability*'] == class_name]
    plt.bar(class_data['Target Class'], class_data['Probability*'], color=colors[i], alpha=0.5, label=class_name, width=0.5)

# Add labels and title
plt.xlabel('Probability')
plt.ylabel('Frequency')
plt.title('Original Data')

# Add legends
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

# Show the plot
plt.show()
12/42:
# Assuming you have 'Target Class' and 'Probability*' columns in your dataset
# Replace 'dataset' with your actual DataFrame name if it's different

# Get unique probabilities and set up colors
unique_probabilities = dataset['Probability*'].unique()
colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar plot with separate colors for each probability
plt.figure(figsize=(10, 6))
for i, probability in enumerate(unique_probabilities):
    class_data = dataset[dataset['Probability*'] == probability]
    plt.bar(class_data['Target Class'], class_data['Probability*'], color=colors[i], alpha=0.5, label=f'Probability {probability}', width=0.5)

# Add labels and title
plt.xlabel('Target Class')
plt.ylabel('Probability')
plt.title('Original Data')

# Add legends
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

# Show the plot
plt.show()
12/43:
# Assuming you have 'Target Class' and 'Probability*' columns in your dataset
# Replace 'dataset' with your actual DataFrame name if it's different

# Get unique probabilities and set up colors
unique_probabilities = dataset['Probability*'].unique()
colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar plot with separate colors for each probability
plt.figure(figsize=(10, 6))
for i, probability in enumerate(unique_probabilities):
    class_data = dataset[dataset['Probability*'] == probability]
    plt.bar(class_data['Target Class'], class_data['Probability*'], color=colors[i], alpha=0.5, label=f'Probability {probability}', width=0.5)

# Add labels and title
plt.xlabel('Target Class')
plt.ylabel('Probability')
plt.title('Original Data')

# Add legends
plt.legend(Target_class, loc='center left', bbox_to_anchor=(1, 0.5))

# Show the plot
plt.show()
12/44:
# Assuming you have 'Target Class' and 'Probability*' columns in your dataset
# Replace 'dataset' with your actual DataFrame name if it's different

# Get unique classes and set up colors
unique_classes = dataset['Target Class'].unique()
colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar plot with separate colors for each class
plt.figure(figsize=(10, 6))
for i, class_name in enumerate(unique_classes):
    class_data = dataset[dataset['Target Class'] == class_name]
    plt.bar(class_data['Target Class'], class_data['Probability*'], color=colors[i], alpha=0.5, label=f'Target Class: {class_name}', width=0.5)

# Add labels and title
plt.xlabel('Target Class')
plt.ylabel('Probability*')
plt.title('Original Data')

# Add legends
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

# Show the plot
plt.show()
13/1:
import pandas as pd

# Load your dataset (replace 'your_dataset.csv' with the actual file name)
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Calculate the sum of all rows
sum_of_all_rows = df.sum().sum()

# Calculate the sum of each unique value
sum_of_unique_values = df.apply(lambda x: x.nunique()).sum()

# Calculate the final result
result = (sum_of_unique_values / sum_of_all_rows) * 100

print(f"The result is: {result}")
13/2:
import pandas as pd

# Load your dataset (replace 'your_dataset.csv' with the actual file name)
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

# Calculate the sum of all rows
sum_of_probability = df['Probability*'].sum()

# Calculate the sum of each unique value
sum_of_unique_values = df.apply(lambda x: x.nunique()).sum()

# Calculate the final result
result = (sum_of_unique_values / sum_of_all_rows) * 100

print(f"The result is: {result}")
13/3:
import pandas as pd

# Load your dataset (replace 'your_dataset.csv' with the actual file name)
df = pd.read_csv('your_dataset.csv')

# Calculate the sum of the 'probability' column
sum_of_probability = df['probability'].sum()

print(f"The sum of 'probability' column is: {sum_of_probability}")
13/4:
import matplotlib.pyplot as plt



# Group by target class and calculate the mean probability
grouped_data = dataset.groupby('Target Class'), dataset['Probability*'].mean()

# Plotting
plt.bar(grouped_data.index, grouped_data)
plt.xlabel('Target Class')
plt.ylabel('Mean Probability')
plt.xticks(rotation=45)
plt.show()
13/5:
# Read the CSV file

import pandas as pd
from IPython.display import display

file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
display(dataset)
13/6:
import matplotlib.pyplot as plt



# Group by target class and calculate the mean probability
grouped_data = dataset.groupby('Target Class'), dataset['Probability*'].mean()

# Plotting
plt.bar(grouped_data.index, grouped_data)
plt.xlabel('Target Class')
plt.ylabel('Mean Probability')
plt.xticks(rotation=45)
plt.show()
13/7:
import matplotlib.pyplot as plt



# Group by target class and calculate the mean probability
grouped_data = dataset.groupby(Target_class), dataset['Probability*'].mean().reset_index()

# Plotting
plt.bar(grouped_data.index, grouped_data)
plt.xlabel('Target Class')
plt.ylabel('Mean Probability')
plt.xticks(rotation=45)
plt.show()
13/8:
import matplotlib.pyplot as plt



# Group by target class and calculate the mean probability
grouped_data = dataset.groupby('Target_Class'), dataset['Probability*'].mean().reset_index()

# Plotting
plt.bar(grouped_data.index, grouped_data)
plt.xlabel('Target Class')
plt.ylabel('Mean Probability')
plt.xticks(rotation=45)
plt.show()
13/9:
import matplotlib.pyplot as plt



# Group by target class and calculate the mean probability
grouped_data = dataset.groupby('Target Class'), dataset['Probability*'].mean().reset_index()

# Plotting
plt.bar(grouped_data.index, grouped_data)
plt.xlabel('Target Class')
plt.ylabel('Mean Probability')
plt.xticks(rotation=45)
plt.show()
13/10:
import numpy as np
import matplotlib.pyplot as plt



# Group by target class and calculate the mean probability
grouped_data = dataset.groupby('Target Class'), dataset['Probability*'].mean().reset_index()

# Plotting
plt.bar(grouped_data.index, grouped_data)
plt.xlabel('Target Class')
plt.ylabel('Mean Probability')
plt.xticks(rotation=45)
plt.show()
13/11:
import numpy as np
import matplotlib.pyplot as plt



# Group by target class and calculate the mean probability
unique_target_classes, mean_probabilities = np.unique(Target_class, return_means=True)

# Plotting
plt.bar(grouped_data.index, grouped_data)
plt.xlabel('Target Class')
plt.ylabel('Mean Probability')
plt.xticks(rotation=45)
plt.show()
13/12:
import numpy as np
import matplotlib.pyplot as plt



# Group by target class and calculate the mean probability
unique_target_classes, mean_probabilities = np.unique(Target_class, return_means=True)

# Plotting
plt.bar(unique_target_classes, mean_probabilities)
plt.xlabel('Target Class')
plt.ylabel('Mean Probability')
plt.xticks(rotation=45)
plt.show()
13/13:
import numpy as np
import matplotlib.pyplot as plt



# Group by target class and calculate the mean probability
unique_target_classes, mean_probabilities = np.unique('Target_Class', return_means=True)

# Plotting
plt.bar(unique_target_classes, mean_probabilities)
plt.xlabel('Target Class')
plt.ylabel('Mean Probability')
plt.xticks(rotation=45)
plt.show()
13/14:
import matplotlib.pyplot as plt
import numpy as np

# Assuming 'target_classes' is a NumPy array of strings and 'probabilities' is a NumPy array of floats

# Calculate the unique target classes and their corresponding mean probabilities
unique_target_classes, mean_probabilities = [], []

for unique_class in np.unique('Target_Class'):
    mask = ('Target_Class' == unique_class)
    unique_target_classes.append(unique_class)
    mean_probabilities.append(np.mean('Probability*';[mask]))

# Plotting
plt.bar(unique_target_classes, mean_probabilities)
plt.xlabel('Target Class')
plt.ylabel('Mean Probability')
plt.xticks(rotation=45)
plt.show()
13/15:
import matplotlib.pyplot as plt
import numpy as np

# Assuming 'target_classes' is a NumPy array of strings and 'probabilities' is a NumPy array of floats

# Calculate the unique target classes and their corresponding mean probabilities
unique_target_classes, mean_probabilities = [], []

for unique_class in np.unique('Target_Class'):
    mask = ('Target_Class' == unique_class)
    unique_target_classes.append(unique_class)
    mean_probabilities.append(np.mean('Probability*'[mask]))

# Plotting
plt.bar(unique_target_classes, mean_probabilities)
plt.xlabel('Target Class')
plt.ylabel('Mean Probability')
plt.xticks(rotation=45)
plt.show()
13/16:
import matplotlib.pyplot as plt
import numpy as np

# Assuming 'target_classes' is a NumPy array of strings and 'probabilities' is a NumPy array of floats

# Calculate the unique target classes and their corresponding mean probabilities
unique_target_classes, mean_probabilities = np.unique('Target_Class', return_counts=True)

# Plotting
plt.bar(unique_target_classes, mean_probabilities)
plt.xlabel('Target Class')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()
13/17:
# Read the CSV file

import pandas as pd
from IPython.display import display

file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
display(dataset)
13/18:
# 1.2 Check for Missing Values
na_counts = dataset.isna().sum()
print(na_counts)
print(len(dataset))
print(dataset.shape)
13/19:
# 1.3 Explore Target Class Variables
import matplotlib.pyplot as plt
import seaborn as sns

#  1.3 Explore Target Variables

freq_table = dataset['Target Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.title('Target Class')
plt.show()
13/20:
# Convert Probability to a categorical variable
dataset['Probability'] = dataset['Probability*'].astype('category')
print(dataset['Probability'])
13/21:
import matplotlib.pyplot as plt
import numpy as np

x = dataset(Target_class)
y = dataset('Probability*')

plt.bar(x,y)
plt.show()
13/22:
import matplotlib.pyplot as plt
import numpy as np

x = dataset('Target_Class')
y = dataset('Probability*')

plt.bar(x,y)
plt.show()
13/23:
#  verifying if train and test data is in sync:

plt.figure(figsize=(15,5))
plt.subplot(1,3,1)
y.value_counts().plot(kind = 'bar')
plt.title("Original")

plt.subplot(1,3,2)
y_train.value_counts().plot(kind = 'bar')
plt.title("Training set")

plt.subplot(1,3,3)
y_test.value_counts().plot(kind = 'bar')
plt.title("Test set")
plt.show()
13/24:
#  verifying if train and test data is in sync:

plt.figure(figsize=(15,5))
plt.subplot(1,3,1)
y_test.value_counts().plot(kind = 'bar')
plt.title("Original")

plt.subplot(1,3,2)
y_train.value_counts().plot(kind = 'bar')
plt.title("Training set")

plt.subplot(1,3,3)
y_test.value_counts().plot(kind = 'bar')
plt.title("Test set")
plt.show()
13/25:
#  verifying if train and test data is in sync:

import matplotlib.pyplot as plt

# Assuming you have y_test, y_train defined

plt.figure(figsize=(15,5))

# Original Data
plt.subplot(1,3,1)
y_test.value_counts().plot(kind='bar')
plt.title("Original")

# Training Set
plt.subplot(1,3,2)
y_train.value_counts().plot(kind='bar')
plt.title("Training set")

# Test Set
plt.subplot(1,3,3)
y_test.value_counts().plot(kind='bar')
plt.title("Test set")

plt.tight_layout()
plt.show()
13/26:
#  verifying if train and test data is in sync:

import matplotlib.pyplot as plt

# Assuming you have y_test, y_train defined

plt.figure(figsize=(15,5))

# Original Data
plt.subplot(1,3,1)
y_test().plot(kind='bar')
plt.title("Original")

# Training Set
plt.subplot(1,3,2)
y_train().plot(kind='bar')
plt.title("Training set")

# Test Set
plt.subplot(1,3,3)
y_test().plot(kind='bar')
plt.title("Test set")

plt.tight_layout()
plt.show()
13/27:
# 2.2 Verifying if train and test data is in sync
train = X_train + y_train
test = X_test + y_test
print(len(dataset) == len(train) + len(test))
13/28:
# 2.2 Verifying if train and test data is in sync
train = X_train + y_train
test = X_test + y_test
print(len(dataset) == len(train) + len(test))
13/29:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
display(X_train)
display(X_test)
display(y_train)
display(y_test)
13/30:
# 2.2 Verifying if train and test data is in sync
train = X_train + y_train
test = X_test + y_test
print(len(dataset) == len(train) + len(test))
13/31:
# Checking for unique values 
Target_Class_unique = dataset['Target Class'].unique()
display(Target_Class_unique)
Probability_unique = dataset['Probability*'].unique()
display(Probability_unique)
13/32:
# Checking for unique values 
Target_Class_unique = dataset['Target Class'].unique()
display(Target_Class_unique)
Probability_unique = dataset['Probability*'].unique()
display(Probability_unique)
13/33:
#  verifying if train and test data is in sync:

import matplotlib.pyplot as plt

# Assuming you have y_test, y_train defined

plt.figure(figsize=(15,5))

# Original Data
plt.subplot(1,3,1)
dataset('Probability*').plot(kind='bar')
plt.title("Original")

# Training Set
plt.subplot(1,3,2)
y_train('Probability*').plot(kind='bar')
plt.title("Training set")

# Test Set
plt.subplot(1,3,3)
y_test('Probability*').plot(kind='bar')
plt.title("Test set")

plt.tight_layout()
plt.show()
13/34:
# Read the CSV file

import pandas as pd
from IPython.display import display

file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
display(dataset)
13/35:
# 1.2 Check for Missing Values
na_counts = dataset.isna().sum()
print(na_counts)
print(len(dataset))
print(dataset.shape)
13/36:
# 1.3 Explore Target Class Variables
import matplotlib.pyplot as plt
import seaborn as sns

#  1.3 Explore Target Variables

freq_table = dataset['Target Class'].value_counts()
print(freq_table)
plt.figure(figsize=(8, 6))
sns.countplot(x='Target Class', data=dataset)
plt.title('Target Class')
plt.show()
13/37:
# 1.4 Explore and Visualize Numerical Variables
plt.figure(figsize=(8, 6))
sns.histplot(dataset['Probability*'])
plt.title('Probability Distribution')
plt.show()
13/38:
# Convert Probability to a categorical variable
dataset['Probability'] = dataset['Probability*'].astype('category')
print(dataset['Probability'])
13/39:
# Generate frequency table

freq_table = dataset['Probability*'].value_counts()
print(freq_table)
13/40:
# 1.5 Descriptive Statistics

data = dataset[['Target Class', 'Probability*']]
print(data.head())
13/41:
# 1.8 Checking for outliers

plt.figure(figsize=(8, 6))
sns.boxplot(x=dataset['Probability*'])
plt.title('Boxplot of Probability with Outliers')
plt.show()
13/42:
# 1.9 Remove Outliers

Q1 = dataset['Probability*'].quantile(0.25)
Q3 = dataset['Probability*'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
no_outliers = dataset[(dataset['Probability*'] >= lower_bound) & (dataset['Probability*'] <= upper_bound)]
print("Dimensions of no_outliers:", no_outliers.shape)
plt.figure(figsize=(8, 6))
sns.boxplot(x=no_outliers['Probability*'])
plt.title('Boxplot of Probability without Outliers')
plt.show()
13/43:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
display(X_train)
display(X_test)
display(y_train)
display(y_test)
13/44:
# 2.2 Verifying if train and test data is in sync
train = X_train + y_train
test = X_test + y_test
print(len(dataset) == len(train) + len(test))
13/45:
# Checking for unique values 
Target_Class_unique = dataset['Target Class'].unique()
display(Target_Class_unique)
Probability_unique = dataset['Probability*'].unique()
display(Probability_unique)
13/46:
#  verifying if train and test data is in sync:

import matplotlib.pyplot as plt

# Assuming you have y_test, y_train defined

plt.figure(figsize=(15,5))

# Original Data
plt.subplot(1,3,1)
dataset('Probability*').plot(kind='bar')
plt.title("Original")

# Training Set
plt.subplot(1,3,2)
y_train('Probability*').plot(kind='bar')
plt.title("Training set")

# Test Set
plt.subplot(1,3,3)
y_test('Probability*').plot(kind='bar')
plt.title("Test set")

plt.tight_layout()
plt.show()
13/47:
import matplotlib.pyplot as plt

# Assuming you have y_test, y_train defined

plt.figure(figsize=(15,5))

# Original Data
plt.subplot(1,3,1)
dataset['Probability*'].value_counts().plot(kind='bar')
plt.title("Original")

# Training Set
plt.subplot(1,3,2)
y_train['Probability*'].value_counts().plot(kind='bar')
plt.title("Training set")

# Test Set
plt.subplot(1,3,3)
y_test['Probability*'].value_counts().plot(kind='bar')
plt.title("Test set")

plt.tight_layout()
plt.show()
13/48:
import matplotlib.pyplot as plt

# Assuming you have y_test, y_train defined

plt.figure(figsize=(15,5))

# Original Data
plt.subplot(1,3,1)
dataset['Probability*'].value_counts().plot(kind='bar')
plt.title("Original")

# Training Set
plt.subplot(1,3,2)
y_train['Probability*'].value_counts().plot(kind='bar')
plt.title("Training set")

# Test Set
plt.subplot(1,3,3)
y_test['Probability*'].value_counts().plot(kind='bar')
plt.title("Test set")

plt.tight_layout()
plt.show()
13/49:
import matplotlib.pyplot as plt

# Assuming you have y_test, y_train defined

plt.figure(figsize=(15,5))

# Original Data
plt.subplot(1,3,1)
dataset['Probability*'].value_counts().plot(kind='bar')
plt.title("Original")

# Training Set
plt.subplot(1,3,2)
y_train['Probability*'].value_counts().plot(kind='bar')
plt.title("Training set")

# Test Set
plt.subplot(1,3,3)
y_test['Probability*'].value_counts().plot(kind='bar')
plt.title("Test set")

plt.tight_layout()
plt.show()
13/50:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
print(X_train)
print(X_test)
print(y_train)
print(y_test)
13/51:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
print(X_train)
print(X_test)
print(y_train)
print(y_test)
y_train['Probability*']
13/52:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
print(X_train)
print(X_test)
print(y_train)
print(y_test)
y_train["Probability*"]
13/53:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
print(X_train)
print(X_test)
print(y_train)
print(y_test)
y_train[Probability*]
13/54:
import matplotlib.pyplot as plt

# Assuming you have y_test, y_train defined

plt.figure(figsize=(15,5))

# Original Data
plt.subplot(1,3,1)
dataset['Probability*'].value_counts().plot(kind='bar')
plt.title("Original")

# Training Set
plt.subplot(1,3,2)
train['Probability*'].value_counts().plot(kind='bar')
plt.title("Training set")

# Test Set
plt.subplot(1,3,3)
test['Probability*'].value_counts().plot(kind='bar')
plt.title("Test set")

plt.tight_layout()
plt.show()
13/55: train['Probability*']
13/56:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
print(X_train)
print(X_test)
print(y_train)
print(y_test)
13/57:
# 2.2 Verifying if train and test data is in sync
train = X_train + y_train
test = X_test + y_test
print(len(dataset) == len(train) + len(test))
13/58:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
print(X_train)
print(X_test)
print(y_train)
print(y_test)
13/59: print(X_train)
13/60: print(y_train)
13/61:
import matplotlib.pyplot as plt

# Assuming you have y_test, y_train defined

plt.figure(figsize=(15,5))

# Original Data
plt.subplot(1,3,1)
dataset['Probability*'].value_counts().plot(kind='bar')
plt.title("Original")

# Training Set
plt.subplot(1,3,2)
X_train['Probability*'].value_counts().plot(kind='bar')
plt.title("Training set")

# Test Set
plt.subplot(1,3,3)
X_test['Probability*'].value_counts().plot(kind='bar')
plt.title("Test set")

plt.tight_layout()
plt.show()
13/62:
import matplotlib.pyplot as plt

# Assuming you have y_test, y_train defined

plt.figure(figsize=(15,5))

# Original Data
plt.subplot(1,3,1)
dataset['Probability*'].value_counts().plot(kind='bar')
color = 'Green'
plt.title("Original")

# Training Set
plt.subplot(1,3,2)
X_train['Probability*'].value_counts().plot(kind='bar')
Color = 'Blue'
plt.title("Training set")

# Test Set
plt.subplot(1,3,3)
X_test['Probability*'].value_counts().plot(kind='bar')
Color = 'Red'
plt.title("Test set")

plt.tight_layout()
plt.show()
13/63:
import matplotlib.pyplot as plt

plt.figure(figsize=(15,5))

# Original Data
plt.subplot(1,3,1)
dataset['Probability*'].value_counts().plot(kind='bar', color='green')
plt.title("Original")

# Training Set
plt.subplot(1,3,2)
X_train['Probability*'].value_counts().plot(kind='bar', color='blue')
plt.title("Training set")

# Test Set
plt.subplot(1,3,3)
X_test['Probability*'].value_counts().plot(kind='bar', color='red')
plt.title("Test set")

plt.tight_layout()
plt.show()
13/64:
# import modules
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
 

# read the dataset
df = pd.read_csv("C:\\Users\\sound\\OneDrive\\Desktop\\main project\\drug targets\\python project main\\Eplereonone drug target prediction csv file.csv")

 
# get the locations
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
 
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=0)
display(X_train)
display(X_test)
display(y_train)
display(y_test)
13/65:
# Get the shapes
X_train_shape = X_train.shape
X_test_shape = X_test.shape
y_train_shape = len(y_train)
y_test_shape = len(y_test)

# Get the value counts
y_train_counts = y_train.value_counts()
y_test_counts = y_test.value_counts()

# Print the results
print(X_train_shape)
print(X_test_shape)
print(y_train_shape)
print(y_test_shape)
print(y_train_counts)
print(y_test_counts)
13/66:
# Get the shapes
X_train_shape = X_train.shape
X_test_shape = X_test.shape
y_train_shape = len(y_train)
y_test_shape = len(y_test)

# Get the value counts
y_train_counts = y_train.value_counts()
y_test_counts = y_test.value_counts()

# Print the results
print("X train shape:" X_train_shape)
print(X_test_shape)
print(y_train_shape)
print(y_test_shape)
print(y_train_counts)
print(y_test_counts)
13/67:
# Get the shapes
X_train_shape = X_train.shape
X_test_shape = X_test.shape
y_train_shape = len(y_train)
y_test_shape = len(y_test)

# Get the value counts
y_train_counts = y_train.value_counts()
y_test_counts = y_test.value_counts()

# Print the results
print("X train shape:", X_train_shape)
print("X test shape:", X_test_shape)
print("y train shape:", y_train_shape)
print("y test shape:", y_test_shape)
print("y train counts:", y_train_counts)
print("y train counts:", y_test_counts)
13/68:
# 2. # Model Building
from sklearn.tree import DecisionTreeClassifier
from mlxtend.plotting import plot_decision_regions

clf_tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)
clf_tree.fit(X_train, y_train)
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
fig, ax = plt.subplots(figsize=(7, 7))
plot_decision_regions(X_combined, y_combined, clf=clf_tree)
plt.xlabel('Target Class')
plt.ylabel('Probability')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
13/69:
# 2. # Model Building
from sklearn.tree import DecisionTreeClassifier
from mlxtend.plotting import plot_decision_regions

clf_tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)
clf_tree.fit(X_train, y_train)
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
fig, ax = plt.subplots(figsize=(7, 7))
plot_decision_regions(X_combined, y_combined, clf=clf_tree)
plt.xlabel('Target Class')
plt.ylabel('Probability')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
13/70:
# # Encode class values as integers - using Label Encoding
from sklearn.calibration import LabelEncoder


encoder = LabelEncoder()
encoder.fit(y_train)
encoded_y_train = encoder.transform(y_train)

encoder = LabelEncoder()
encoder.fit(y_test)
encoded_y_test = encoder.transform(y_test)
13/71:
# # Encode class values as integers - using Label Encoding
from sklearn.calibration import LabelEncoder


encoder = LabelEncoder()
encoder.fit(X_train)
encoded_y_train = encoder.transform(y_train)

encoder = LabelEncoder()
encoder.fit(X_test)
encoded_y_test = encoder.transform(y_test)
13/72:
# # Encode class values as integers - using Label Encoding
from sklearn.calibration import LabelEncoder


encoder = LabelEncoder()
encoder.fit(X_train)
encoded_X_train = encoder.transform(X_train)

encoder = LabelEncoder()
encoder.fit(X_test)
encoded_X_test = encoder.transform(X_test)
13/73:
from sklearn.preprocessing import LabelEncoder

# Assuming X_train and X_test are your feature arrays with categorical variables

# Initialize the LabelEncoder
encoder = LabelEncoder()

# Fit the encoder on the training data and transform it
encoded_X_train = encoder.fit_transform(X_train)

# Transform the test data using the same encoder
encoded_X_test = encoder.transform(X_test)
13/74: print(type(dataset))
13/75: print(class(dataset))
13/76: print(type(dataset))
13/77:
dataset.info()
dataset.describe()
13/78: dataset['column_name']
13/79:
# 1.8 Checking for outliers

plt.figure(figsize=(8, 6))
sns.boxplot(x=dataset['Target Class'])
plt.title('Boxplot of Probability with Outliers')
plt.show()
13/80:
# 1.8 Checking for outliers

plt.figure(figsize=(8, 6))
sns.boxplot(x=dataset['Probability*'])
plt.title('Boxplot of Probability with Outliers')
plt.show()
13/81: dataset['Target Class']
13/82:
dataset['Target Class']
missing_values = dataset.isnull()
missing_counts = missing_values.sum()
13/83:
dataset['Target Class']
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
13/84:
dataset['Target Class']
missing_values = dataset['Target Class'].isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = df.duplicated()
13/85:
dataset['Target Class']
missing_values = dataset['Target Class'].isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = df.duplicated()
duplicates = df.duplicated()
13/86:
dataset['Target Class']
missing_values = dataset['Target Class'].isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = dataset['Target Class'].duplicated()
13/87:
dataset['Target Class']
missing_values = dataset['Target Class'].isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = dataset['Target Class'].duplicated()
print("Duplicates:", duplicates)
13/88:
dataset['Target Class']
missing_values = dataset['Target Class'].isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = dataset['Target Class'].duplicated()
print("Duplicates:", duplicates)
# Remove duplicate rows
df_cleaned = dataset['Target Class'][~duplicates]
13/89:
dataset['Target Class']
missing_values = dataset['Target Class'].isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = dataset['Target Class'].duplicated()
print("Duplicates:", duplicates)
# Remove duplicate rows
df_cleaned = dataset['Target Class'][~duplicates]
dataset['Target Class']['is_duplicate'] = duplicates
13/90:
dataset['Target Class']
missing_values = dataset['Target Class'].isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = dataset['Target Class'].duplicated()
print("Duplicates:", duplicates)
df_cleaned = dataset['Target Class'][~duplicates]
display(df_cleaned)
13/91:
dataset['Target Class', 'Target', 'Probability']
missing_values = dataset['Target Class'].isnull()
print(missing_values), 
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = dataset['Target Class'].duplicated()
print("Duplicates:", duplicates)
df_cleaned = dataset['Target Class'][~duplicates]
display(df_cleaned)
13/92:
dataset['Target Class', 'Target', 'Probability']
missing_values = datasetdataset['Target Class', 'Target', 'Probability'].isnull()
print(missing_values), 
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = dataset['Target Class', 'Target', 'Probability'].duplicated()
print("Duplicates:", duplicates)
df_cleaned = datasetdataset['Target Class', 'Target', 'Probability'][~duplicates]
display(df_cleaned)
13/93:
dataset['Target Class', 'Target', 'Probability*']
missing_values = datasetdataset['Target Class', 'Target', 'Probability'].isnull()
print(missing_values), 
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = dataset['Target Class', 'Target', 'Probability'].duplicated()
print("Duplicates:", duplicates)
df_cleaned = datasetdataset['Target Class', 'Target', 'Probability'][~duplicates]
display(df_cleaned)
13/94:
dataset['Target Class', 'Target', 'Probability*']
missing_values = dataset['Target Class', 'Target', 'Probability'].isnull()
print(missing_values), 
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = dataset['Target Class', 'Target', 'Probability'].duplicated()
print("Duplicates:", duplicates)
df_cleaned = dataset['Target Class', 'Target', 'Probability'][~duplicates]
display(df_cleaned)
13/95:
dataset['Target Class', 'Target', 'Probability*']
missing_values = dataset['Target Class', 'Target', 'Probability*'].isnull()
print(missing_values), 
missing_counts = missing_values.sum()
print(missing_counts)
duplicates = dataset['Target Class', 'Target', ' Probability*'].duplicated()
print("Duplicates:", duplicates)
df_cleaned = dataset['Target Class', 'Target', 'Probability*'][~duplicates]
display(df_cleaned)
13/96:
dataset['Target Class', 'Target', 'Probability*']
missing_values = dataset['Target Class', 'Target', 'Probability*'].isnull()
14/1:
# Read the CSV file

import pandas as pd
from IPython.display import display

file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
display(dataset)
dataset.head()
14/2:
# Check for missing values
missing_values = dataset.isnull()
missing_counts = missing_values.sum()
14/3:
# Check for missing values
missing_values = dataset.isnull()
missing_counts = missing_values.sum()
14/4:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
14/5:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
14/6:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
display(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
14/7:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
display(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
14/8:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
14/9:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
print(df_cleaned)
14/10:
#Inspect Data Types

data_types = dataset.dtypes
14/11:
#Inspect Data Types

data_types = dataset.dtypes
print(data_types)
14/12:
#Inspect Data Types

data_types = dataset.dtypes
print(data_types)
dataset['Probability*'] = dataset['Probability*'].astype(int)
display(dataset['Probability*'])
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
14/13:
# Scaling and Normalization
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform('Target_CLlass')
15/1:
# Scaling and Normalization
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform('Probability*')
15/2:
# Scaling and Normalization
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform['Probability*']
15/3:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = dataset['Probability*']
np.array(probs).reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs)
15/4:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = ['Probability*']
np.array(probs).reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs)
15/5:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = ['Probability*']
np.array(probs).reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs)
15/6:
#Inspect Data Types

data_types = dataset.dtypes
type(data_types)
dataset['Probability*'] = dataset['Probability*'].astype(int)
display(dataset['Probability*'])
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
15/7:
#Inspect Data Types

data_types = dataset.dtypes
type(data_types)
dataset['Probability*'] = dataset['Probability*'].astype(int)
display(dataset['Probability*'])
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
15/8:
# Read the CSV file

import pandas as pd
from IPython.display import display

file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
display(dataset)
dataset.head()
15/9:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
type(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
print(df_cleaned)
15/10:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
type(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
type(df_cleaned)
15/11:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
type(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
type(df_cleaned)
15/12:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
type(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
type(df_cleaned)
15/13:
# Read the CSV file

import pandas as pd
from IPython.display import display

file_path = r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv'
dataset = pd.read_csv(file_path)
display(dataset)
dataset.head()
15/14:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
type(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
15/15:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
display(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
15/16:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
15/17:
#Inspect Data Types

data_types = dataset.dtypes
type(data_types)
dataset['Probability*'] = dataset['Probability*'].astype(int)
display(dataset['Probability*'])
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
15/18:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Probability*'] = dataset['Probability*'].astype(int)
display(dataset['Probability*'])
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
15/19:
#Inspect Data Types

data_types = dataset.dtypes
data_types.head()
dataset['Probability*'] = dataset['Probability*'].astype(int)
display(dataset['Probability*'])
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
15/20:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = (dataset['Probability*'])
np.array(probs).reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs)
15/21:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = (dataset['Probability*'])
np.array(probs).reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs)
15/22:
#conversion of data sets
dataset['Probability*'] = dataset['Probability*'].astype(int)
15/23:
#conversion of data sets
dataset['Probability*'] = dataset['Probability*'].astype(int)
display(dataset['Probability*'])
15/24:
# conversion of dataset
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
15/25:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Probability*'] = dataset['Probability*'].astype(int)
display(dataset['Probability*'])
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
15/26:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
15/27:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming 'probs' is a 1D array of probabilities
probs = np.array(['Probability*'])  # Example data

# Reshape the data to a 2D array
probs_2d = probs.reshape(-1, 1)
15/28:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming 'probs' is a 1D array of probabilities
probs = np.array(['Probability*'])

# Reshape the data to a 2D array
probs_2d = probs.reshape(-1, 1)
display(probs_2d)
15/29:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming 'probs' is a 1D array of probabilities
probs = np.array(['Probability*'])

# Reshape the data to a 2D array
probs_2d = probs.reshape(-1, 1)
print(probs_2d)
15/30:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming 'probs' is a 1D array of probabilities
probs = np.array(['Probability*'])

# Reshape the data to a 2D array
probs_2d = probs.reshape(-1, 1)
print(probs)
15/31:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming 'probs' is a 1D array of probabilities
probs = np(dataset['Probability*'])

# Reshape the data to a 2D array
probs_2d = probs.reshape(-1, 1)
print(probs)
15/32:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming 'probs' is a 1D array of probabilities
probs = np(dataset['Probability*'])

# Reshape the data to a 2D array
probs_2d = probs.reshape(-1, 1)
print(probs)
15/33:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming 'probs' is a 1D array of probabilities
probs = np(dataset['Probability*'])

# Reshape the data to a 2D array
probs_2d = probs.reshape(-1, 1)
print(probs)
15/34:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = (dataset['Probability*'])
np.array(probs).reshape(-1, 2)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs)
15/35:
from sklearn.preprocessing import LabelEncoder

Target_Class_encoded = dataset['Target Class']
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(Target_Class_encoded)
print(Target_Class_encoded)
15/36:
# One hot Encoding
from sklearn.preprocessing import LabelEncoder

df = pd.DataFrame(dataset['Target Class'])
df_encoded = pd.get_dummies(df, columns=['Target Class'])
print(df_encoded)
15/37:
# One hot Encoding
from sklearn.preprocessing import LabelEncoder

# Create a list of categorical values
df = dataset['Target Class']
label_encoder = LabelEncoder(df)
encoded_labels = label_encoder.fit_transform(df)
print(df)
15/38:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df = dataset['Target Class'] 
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(df)
print(df)
15/39:
# Label encoding
from sklearn.preprocessing import LabelEncoder

# Assuming 'df' is a pandas Series or a 1D array containing categorical values
df = dataset['Target Class']  # Assuming 'dataset' is your DataFrame

# Initialize label encoder
label_encoder = LabelEncoder()

# Fit and transform the data
encoded_labels = label_encoder.fit_transform(df)

# Print the encoded labels
print(encoded_labels)
15/40:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np(dataset['Probability*'])

# Reshape the data to 2D arrays
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)
15/41:
# Scaling and Normalization
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming 'probs' is a 1D array of probabilities
probs = np.array(dataset['Probability*'])  # Assuming 'dataset' is your DataFrame

# Reshape the data to 2D arrays
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)
15/42:
#Visualization of data
## Visualization of target class and probability
import matplotlib.pyplot as plt

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

plt.figure(figsize=(12, 6))

# Histogram for 'Target Class' versus 'Probability*'
plt.subplot(1, 2, 1)
plt.hist([dataset[dataset['Target Class'] == cls]['Probability*'] for cls in dataset['Target Class'].unique()],
         bins=20, alpha=0.5, label=dataset['Target Class'].unique(), color=colors)
plt.xlabel('Probability*')
plt.ylabel('Frequency')
plt.title('Target Class versus Probability*')
plt.legend()

# Histogram for 'Target' versus 'Probability*'
plt.subplot(1, 2, 2)
plt.hist([dataset[dataset['Target'] == cls]['Probability*'] for cls in dataset['Target'].unique()],
         bins=20, alpha=0.5, label=dataset['Target'].unique(), color=colors)
plt.xlabel('Probability*')
plt.ylabel('Frequency')
plt.title('Target versus Probability*')
plt.legend()

plt.tight_layout()
plt.show()
15/43:
#Visualization of data
## Visualization of target class and probability
import matplotlib.pyplot as plt

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

plt.figure(figsize=(12, 6))


import numpy as np

# Assuming you have label encoded classes in 'Encoded Class'
# And a mapping from label to class names in 'class_mapping'

# Histogram for 'Target Class' versus 'Probability*'
plt.subplot(1, 2, 1)

# Extract label encoded values
encoded_classes = [class_mapping[cls] for cls in dataset['Target Class']]

plt.hist([dataset[encoded_classes == cls]['Probability*'] for cls in np.unique(encoded_classes)],
         bins=20, alpha=0.5, label=[class_mapping[cls] for cls in np.unique(encoded_classes)], color=colors)

plt.xlabel('Probability*')
plt.ylabel('Frequency')
plt.title('Target Class versus Probability*')
plt.legend()
plt.show()
15/44:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
contingency_table = pd.crosstab(dataset['Target Class'], dataset['Target'])

plt.figure(figsize=(10, 6))
15/45:
# Label encoding
from sklearn.preprocessing import LabelEncoder
df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
15/46:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
contingency_table = pd.crosstab(encoded_labels1, encoded_labels2)

plt.figure(figsize=(10, 6))
15/47:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
contingency_table = pd.crosstab(encoded_labels1, encoded_labels2)

# Set row and column names
contingency_table.index = ['Target Class ' + str(i) for i in contingency_table.index]
contingency_table.columns = ['Target ' + str(i) for i in contingency_table.columns]

plt.figure(figsize=(10, 6))

sns.heatmap(contingency_table, annot=True, cmap='YlGnBu')

# Labeling
plt.xlabel('Target')
plt.ylabel('Target Class')
plt.title('Contingency Table Heatmap')

plt.show()
16/1:
from matplotlib import colors
import matplotlib.pyplot as plt
import numpy as np

# Assuming you have label encoded classes in 'Encoded Class'
# And a mapping from label to class names in 'class_mapping'

# Create a color map for classes
class_colors = {cls: colors[i] for i, cls in enumerate(np.unique(encoded_labels1))}

# Create a figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

# Histogram for 'Encoded Class' versus 'Probability*'
for cls in np.unique(encoded_labels1):
    class_data = dataset[encoded_labels1 == cls]['Probability*']
    ax1.hist(class_data, bins=20, alpha=0.5, color=class_colors[cls], label=class_mapping[cls])

ax1.set_ylabel('Encoded Class (Target Class)')
ax1.set_xlabel('Probability*')
ax1.set_title('Encoded Class (Target Class) versus Probability*')

# Set legend
ax1.legend()

# Legend with actual class names
handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=class_colors[cls], markersize=10, label=class_mapping[cls]) for cls in np.unique(encoded_classes)]
ax2.legend(handles=handles)

# Hide the axes in the second subplot
ax2.axis('off')

plt.tight_layout()
plt.show()
16/2:
# Label encoding
from sklearn.preprocessing import LabelEncoder
df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
16/3:
# Label encoding
from sklearn.preprocessing import LabelEncoder
df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
16/4:
# Label encoding
from sklearn.preprocessing import LabelEncoder
df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
16/5:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')
display(dataset)
dataset.head()
16/6:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')
display(dataset)
dataset.head()
16/7:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')
display(dataset)
dataset.head()
16/8:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')
display(dataset)
dataset.head()
16/9:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')
display(dataset)
dataset.head()
16/10:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')

print(dataset)
16/11:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')

print(dataset)
16/12:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r'Eplereonone drug target prediction csv file.csv')

print(dataset)
16/13:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r'Eplereonone drug target prediction csv file.csv')

print(dataset)
16/14:
# Read the CSV file

import pandas as pd

dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')

# Display the dataset
print(dataset)
16/15:
# Read the CSV file

import pandas as pd

dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')

# Display the dataset
print(dataset)
16/16:
# Read the CSV file

import pandas as pd

dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')

# Display the dataset
print(dataset)
16/17:
# Read the CSV file

import pandas as pd

dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv')

# Display the dataset
print(dataset)
16/18:
# Read the CSV file

import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# Display the dataset
print(dataset)
16/19:
# Read the CSV file

import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# Display the dataset
print(dataset)
16/20:
# Read the CSV file

import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# Display the dataset
print(dataset)
16/21:
# Read the CSV file

import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# Display the dataset
print(dataset)
16/22:
# Read the CSV file

import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# Display the dataset
print(dataset)
17/1:
# Read the CSV file

import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# Display the dataset
print(dataset)
17/2:
# Read the CSV file

import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

print(dataset)
display(dataset)
17/3:
# Read the CSV file

import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

print(dataset)
pd.options.display.max_columns = None
pd.options.display.max_rows = None
display(dataset.head())
17/4:
# Read the CSV file

import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

print(dataset)
pd.options.display.max_columns = None
pd.options.display.max_rows = None
display(dataset.head())
17/5:
# Read the CSV file

import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

print(dataset)
display(dataset.head())
17/6:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

print(dataset)
display(dataset.head())
17/7:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

print(dataset)
display(dataset.head())
17/8:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

print(dataset)
display(dataset.head())
17/9:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset.head())
17/10:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
17/11:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
17/12:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
17/13:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
17/14:
#Inspect Data Types
from IPython.display import display

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
17/15:
#Inspect Data Types
from IPython.display import display

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
17/16:
#Inspect Data Types
from IPython.display import display
import pandas as pd

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
17/17:
#Inspect Data Types
from IPython.display import display
import pandas as pd

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
17/18:
# Label encoding
from sklearn.preprocessing import LabelEncoder
df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
17/19:
from matplotlib import colors
import matplotlib.pyplot as plt
import numpy as np

# Assuming you have label encoded classes in 'Encoded Class'
# And a mapping from label to class names in 'class_mapping'

# Create a color map for classes
class_colors = {cls: colors[i] for i, cls in enumerate(np.unique(encoded_labels1))}

# Create a figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

# Histogram for 'Encoded Class' versus 'Probability*'
for cls in np.unique(encoded_labels1):
    class_data = dataset[encoded_labels1 == cls]['Probability*']
    ax1.hist(class_data, bins=20, alpha=0.5, color=class_colors[cls], label=class_mapping[cls])

ax1.set_ylabel('Encoded Class (Target Class)')
ax1.set_xlabel('Probability*')
ax1.set_title('Encoded Class (Target Class) versus Probability*')

# Set legend
ax1.legend()

# Legend with actual class names
handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=class_colors[cls], markersize=10, label=class_mapping[cls]) for cls in np.unique(encoded_classes)]
ax2.legend(handles=handles)

# Hide the axes in the second subplot
ax2.axis('off')

plt.tight_layout()
plt.show()
17/20:
from matplotlib import colors
import matplotlib.pyplot as plt
import numpy as np

# Create a color map for classes
class_colors = {cls: colors[i] for i, cls in enumerate(np.unique(encoded_labels1))}

# Create a figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

# Histogram for 'Probability*' versus 'Encoded Class'
for cls in np.unique(encoded_labels1):
    class_data = dataset[encoded_labels1 == cls]['Probability*']
    ax1.hist(class_data, bins=20, alpha=0.5, color=class_colors[cls])

ax1.set_ylabel('Probability*')
ax1.set_xlabel('Encoded Class (Target Class)')
ax1.set_title('Probability* versus Encoded Class (Target Class)')

# Set legend
ax1.legend()

# Legend with actual class names
handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=class_colors[cls], markersize=10)]
ax2.legend(handles=handles)

# Hide the axes in the second subplot
ax2.axis('off')

plt.tight_layout()
plt.show()
17/21:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
histplot = pd.hist(encoded_labels1, 'Probability*')

# Set row and column names

plt.figure(figsize=(10, 6))

# Labeling
plt.xlabel('Target')
plt.ylabel('Target Class')
plt.title('Contingency Table Heatmap')

plt.show()
17/22:
import matplotlib.pyplot as plt

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_classes = [encoded_labels1]
colors = colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']


# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(Probability, encoded_classes, color= colors, align='center', alpha=0.7)
plt.xlabel('Probability')
plt.ylabel('Encoded Class')

# Add legend for actual classes
for i, txt in enumerate(Target_class):
    plt.text(Probability[i], encoded_classes[i], txt, ha='center', va='bottom', fontsize=10, color='red')

# Show the plot
plt.show()
17/23:
import matplotlib.pyplot as plt

# Assuming 'encoded_labels1' is a list and 'Probability' and 'Target_class' are lists or arrays as well
Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0  0  0  0  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  3  3
  3  3  3  3  3  3  3  3  3  3  3  3  4  5  6  6  6  6  6  6  6  6  6  6
  6  6  6  6  6  6  6  6  6  6  6  6  6  7  8  8  8  8  9 10 11 11 12 12
 13 14 14 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 16 17 18 18 18 19
 19 19 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(Probability, encoded_labels1, color=colors, align='center', alpha=0.7)
plt.xlabel('Probability')
plt.ylabel('Encoded Class')

# Add legend for actual classes
for i, txt in enumerate(Target_class):
    plt.text(Probability[i], encoded_labels1[i], txt, ha='center', va='bottom', fontsize=10, color='red')

# Show the plot
plt.show()
17/24:
import matplotlib.pyplot as plt
import numpy as np

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(Probability, encoded_labels1, color=colors, align='center', alpha=0.7)
plt.xlabel('Probability')
plt.ylabel('Encoded Class')

# Add legend for actual classes
for i, txt in enumerate(Target_class):
    plt.text(Probability[i], encoded_labels1[i], txt, ha='center', va='bottom', fontsize=10, color='red')

# Show the plot
plt.show()
17/25:
import matplotlib.pyplot as plt
import numpy as np

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(encoded_labels1, Probability,  color=colors, align='center', alpha=0.7)
plt.xlabel('Target class')
plt.ylabel('Probability')

# Add legend for actual classes
for i, txt in enumerate(Target_class):
    plt.text(Probability[i], encoded_labels1[i], txt, ha='center', va='bottom', fontsize=10, color='red')

# Show the plot
plt.show()
17/26:
import matplotlib.pyplot as plt
import numpy as np

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(encoded_labels1, Probability, color=colors, align='center', alpha=0.7)
plt.xlabel('Encoded Class')
plt.ylabel('Probability')

# Add legend for actual classes
for i, txt in enumerate(Target_class):
    plt.text(encoded_labels1[i], Probability[i], txt, ha='center', va='bottom', fontsize=10, color='red')

# Show the plot
plt.show()
17/27:
import matplotlib.pyplot as plt
import numpy as np

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(encoded_labels1, Probability, color=colors, align='center', alpha=0.7)
plt.xlabel('Encoded Class')
plt.ylabel('Probability')

# Create handles and labels for legend
handles = [plt.Line2D([0], [0], color=color, lw=4) for color in colors]
labels = Target_class

# Add legend inside the plot
plt.legend(handles, labels, title='Actual Classes', loc='upper right')

# Show the plot
plt.show()
17/28:
import matplotlib.pyplot as plt
import numpy as np

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(encoded_labels1, Probability, color=colors, align='center', alpha=0.7)
plt.xlabel('Encoded Class')
plt.ylabel('Probability')

# Create handles and labels for legend
handles = [plt.Line2D([0], [0], color=color, lw=4) for color in colors]
labels = Target_class.unique

# Add legend inside the plot
plt.legend(handles, labels, title='Actual Classes', loc='upper right')

# Show the plot
plt.show()
17/29:
import matplotlib.pyplot as plt

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(encoded_labels1, Probability, color=colors, align='center', alpha=0.7)
plt.xlabel('Encoded Class')
plt.ylabel('Probability')

# Create handles and labels for legend
handles = [plt.Line2D([0], [0], color=color, lw=4) for color in colors]
labels = Target_class.unique()

# Add legend inside the plot
plt.legend(handles, labels, title='Actual Classes', loc='upper right')

# Show the plot
plt.show()
17/30:
import matplotlib.pyplot as plt

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(encoded_labels1.unique(), Probability, color=colors, align='center', alpha=0.7)
plt.xlabel('Encoded Class')
plt.ylabel('Probability')

# Create handles and labels for legend
handles = [plt.Line2D([0], [0], color=color, lw=4) for color in colors]
labels = Target_class.unique()

# Add legend inside the plot
plt.legend(handles, labels, title='Actual Classes', loc='upper right')

# Show the plot
plt.show()
17/31:
import matplotlib.pyplot as plt

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(encoded_labels1, Probability, color=colors, align='center', alpha=0.7)
plt.xlabel('Encoded Class')
plt.ylabel('Probability')

# Create handles and labels for legend
handles = [plt.Line2D([0], [0], color=color, lw=4) for color in colors]
labels = Target_class.unique()

# Add legend inside the plot
plt.legend(handles, labels, title='Actual Classes', loc='upper right')

# Show the plot
plt.show()
17/32:
import matplotlib.pyplot as plt

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a histogram plot
plt.figure(figsize=(10, 6))
plt.hist(Probability, bins=20, color='blue', alpha=0.7)
plt.xlabel('Probability')
plt.ylabel('Frequency')

# Add legend inside the plot
handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in colors]
labels = Target_class.unique()
plt.legend(handles, labels, title='Actual Classes', loc='upper right')

# Show the plot
plt.show()
17/33:
import matplotlib.pyplot as plt

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a histogram plot
plt.figure(figsize=(10, 6))
plt.hist(Probability, bins=21, color=colors, alpha=0.7)
plt.xlabel('Probability')
plt.ylabel('Frequency')

# Add legend inside the plot
handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in colors]
labels = Target_class.unique()
plt.legend(handles, labels, title='Actual Classes', loc='upper right')

# Show the plot
plt.show()
17/34:
import matplotlib.pyplot as plt

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a histogram plot
plt.figure(figsize=(10, 6))
plt.hist(Probability, bins=21, color=colors, alpha=0.7)
plt.xlabel('Probability')
plt.ylabel('Frequency')

# Add legend inside the plot
handles = [plt.Rectangle((0, 0), 1, 1, color=colors) for color in colors]
labels = Target_class.unique()
plt.legend(handles, labels, title='Actual Classes', loc='upper right')

# Show the plot
plt.show()
17/35:
import matplotlib.pyplot as plt

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a histogram plot
plt.figure(figsize=(10, 6))
plt.hist(labels = Target_class.unique()
, bins=21, color=colors, alpha=0.7)
plt.xlabel('Probability')
plt.ylabel('Frequency')

# Add legend inside the plot
handles = [plt.Rectangle((0, 0), 1, 1, color=colors) for color in colors]
labels = Target_class.unique()
plt.legend(handles, labels, title='Actual Classes', loc='upper right')

# Show the plot
plt.show()
17/36:
import matplotlib.pyplot as plt

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a histogram plot
plt.figure(figsize=(10, 6))
plt.hist([Probability[i] for i, label in enumerate(Target_class)], bins=21, color=[colors[i] for i, label in enumerate(Target_class)], alpha=0.7)
plt.xlabel('Probability')
plt.ylabel('Frequency')

# Add legend inside the plot
handles = [plt.Rectangle((0, 0), 1, 1, color=colors[i]) for i, label in enumerate(Target_class)]
labels = Target_class.unique()
plt.legend(handles, labels, title='Actual Classes', loc='upper right')

# Show the plot
plt.show()
17/37:
import matplotlib.pyplot as plt

Probability = dataset['Probability*']
Target_class = dataset['Target Class']
encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class names and their frequencies
class_names = [Target_class.unique()[label] for label in class_counts.keys()]
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
plt.pie(class_frequencies, labels=class_names, autopct='%1.1f%%', startangle=140, colors=colors)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes
plt.legend(handles=[plt.Line2D([0], [0], color=color, lw=4) for color in colors], labels=class_names, title='Actual Classes', loc='upper right')

# Show the plot
plt.show()
17/38:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='%1.1f%%', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes
plt.legend(wedges, class_counts.keys(), title='Actual Classes', loc='upper right')

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
17/39:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(labels=None, autopct='%1.1f%%', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes
plt.legend(wedges, class_counts.keys(), dataset['Target Class'].unique(), class_frequencies, title='Target class', loc='upper right')

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
17/40:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes with frequencies
legend_labels = [f'Class {label} ({count} occurrences)' for label, count in class_counts.items()]
plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
17/41:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes with percentages
legend_labels = [f'Class {label} ({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]
plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
17/42:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes with percentages
legend_labels = [f'(dataset['Target Class']) ({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]
plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
17/43:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes
legend_labels = [f'Class {label}' for label in class_counts.keys()]
plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
17/44:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes with percentages
legend_labels = [f'dataset ({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]
plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
17/45:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes with percentages
legend_labels = [f'Target_Class ({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]
plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
17/46:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes with percentages
legend_labels = [f'{Target_Class[label]} ({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
17/47:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes with percentages
legend_labels = [f'{Target_Class[label]} ({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Show the plot
plt.show()
17/48:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add a legend for actual classes with percentages
legend_labels = [f'Target_Class ({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]


# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Show the plot
plt.show()
17/49:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'Target_Class ({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/50:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [Target_Class, f'({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/51:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [(Target_Class), f'({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/52:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [(dataset['Target Class']), f'({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/53:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [[Target_Class], f'({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/54:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [[Target_Class], f'({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/55:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [[Target_Class], f'({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/56:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [[Target_Class], f'({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/57:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [(Target_Class), f'({(count/len(encoded_labels1))*100:.1f}%)' for label, count in class_counts.items()]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/58:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [[label, f'({(count/len(encoded_labels1))*100:.1f}%)'] for label, count in class_counts.items()]


plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/59:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = 'Cytochrome P450', 'Electrochemical transporter', 'Enzyme', 'Family A G protein-coupled receptor', ..., 'Toll-like and Il-1 receptors', 'Transferase', 'Unclassified protein', 'Voltage-gated ion channel'

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [[Target_Class, f'({(count/len(encoded_labels1))*100:.1f}%)'] for label, count in class_counts.items()]


plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/60: display(dataset['Target Class'])
17/61: display(dataset['Target Class']).unique()
17/62:
un = (dataset['Target Class']).unique()
display(un)
17/63:
un = (dataset['Target Class']).unique()
print(un)
17/64:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class']

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [[Target_Class, f'({(count/len(encoded_labels1))*100:.1f}%)'] for label, count in class_counts.items()]


plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/65:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]
Target_Class = dataset['Target Class'].unique()

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [[Target_Class, f'({(count/len(encoded_labels1))*100:.1f}%)'] for label, count in class_counts.items()]


plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/66:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/67:
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/68:
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/69:
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/70:
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size= 12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/71:
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size= 12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/72:
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
from Untitled-1.ipynb:pylance-notebook-cell:X45sZmlsZQ== import Target_Class
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size= 12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/73:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/74:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(6, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/75:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/76:
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
Target_Class = dataset['Target Class'].unique()
Probability = dataset['Probability*']

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(Probability, labels=Target_Class, autopct='%1.1f%%', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.title('Target Class versus Probability')
plt.show()
17/77:
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
Target_Class = dataset['Target Class']
Probability = dataset['Probability*']

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(Probability, labels=Target_Class, autopct='%1.1f%%', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.title('Target Class versus Probability')
plt.show()
17/78:
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
Target_Class = dataset['Target Class']
Probability = dataset['Probability*']

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(Probability, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.title('Target Class versus Probability')
plt.show()
17/79:
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
Target_Class = dataset['Target Class'].unique()
Probability = dataset['Probability*']

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(Probability, labels=None, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{target} ({prob:.1f}%)' for target, prob in zip(Target_Class, Probability)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/80:
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
Target_Class = dataset['Target Class'].unique()
Probability = dataset.groupby('Target Class')['Probability*'].sum()

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(Probability, labels=Target_Class, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{target} ({prob:.1f}%)' for target, prob in zip(Target_Class, Probability)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/81:
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
Target_Class = dataset['Target Class'].unique()

# Define the probabilities based on your formula
probabilities = [
    0.1061658 * 4,
    0.1061658 * 2,
    0.1061658 * 16,
    0.1061658 * 12,
    0.1061658,
    0.1061658,
    0.1061658 * 22,
    0.1061658,
    1.0000000 + 0.1894583 + 0.1894583 + 0.1061658,
    0.1061658,
    0.1061658,
    0.1061658 * 2,
    0.1061658 * 2,
    0.1061658 * 2,
    0.1061658,
    0.1061658 * 2,
    0.1061658 * 15,
    0.1061658,
    0.1061658,
    0.1061658,
    0.1061658 * 3,
    0.1061658 * 3,
    0.1061658
]

# Calculate the total sum of probabilities
total_probability = sum(probabilities)

# Calculate the percentages
percentage = [(p / total_probability) * 100 for p in probabilities]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentage, labels=Target_Class, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{target} ({prob:.1f}%)' for target, prob in zip(Target_Class, percentage)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/82:
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
Target_Class = dataset['Target Class']

# Define the probabilities based on your formula
probabilities = [
    0.1061658 * 4,
    0.1061658 * 2,
    0.1061658 * 16,
    0.1061658 * 12,
    0.1061658,
    0.1061658,
    0.1061658 * 22,
    0.1061658,
    1.0000000 + 0.1894583 + 0.1894583 + 0.1061658,
    0.1061658,
    0.1061658,
    0.1061658 * 2,
    0.1061658 * 2,
    0.1061658 * 2,
    0.1061658,
    0.1061658 * 2,
    0.1061658 * 15,
    0.1061658,
    0.1061658,
    0.1061658,
    0.1061658 * 3,
    0.1061658 * 3,
    0.1061658
]

# Calculate the total sum of probabilities
total_probability = sum(probabilities)

# Calculate the percentages
percentage = [(p / total_probability) * 100 for p in probabilities]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentage, labels=Target_Class, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{target} ({prob:.1f}%)' for target, prob in zip(Target_Class, percentage)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/83:
import matplotlib.pyplot as plt

# Assuming 'dataset' is your DataFrame
Target_Class = dataset['Target Class'].unique()

# Define the probabilities based on your formula
probabilities = [
    0.1061658 * 4,
    0.1061658 * 2,
    0.1061658 * 16,
    0.1061658 * 12,
    0.1061658,
    0.1061658,
    0.1061658 * 22,
    0.1061658,
    1.0000000 + 0.1894583 + 0.1894583 + 0.1061658,
    0.1061658,
    0.1061658,
    0.1061658 * 2,
    0.1061658 * 2,
    0.1061658 * 2,
    0.1061658,
    0.1061658 * 2,
    0.1061658 * 15,
    0.1061658,
    0.1061658,
    0.1061658,
    0.1061658 * 3,
    0.1061658 * 3,
    0.1061658
]

# Calculate the total sum of probabilities
total_probability = sum(probabilities)

# Calculate the percentages
percentage = [(p / total_probability) * 100 for p in probabilities]

# Check if lengths match
if len(Target_Class) != len(percentage):
    raise ValueError("Lengths of 'Target_Class' and 'percentage' do not match.")

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentage, labels=Target_Class, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{target} ({prob:.1f}%)' for target, prob in zip(Target_Class, percentage)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/84:
# Assuming 'dataset' is your DataFrame
import matplotlib.pyplot as plt

Target_Class = dataset['Target Class'].unique()

# Define the probabilities based on your formula
probabilities = [
    0.1061658 * 4,
    0.1061658 * 2,
    0.1061658 * 16,
    0.1061658 * 12,
    0.1061658,
    0.1061658,
    0.1061658 * 22,
    0.1061658,
    1.0000000 + 0.1894583 + 0.1894583 + 0.1061658,
    0.1061658,
    0.1061658,
    0.1061658 * 2,
    0.1061658 * 2,
    0.1061658 * 2,
    0.1061658,
    0.1061658 * 2,
    0.1061658 * 15,
    0.1061658,
    0.1061658,
    0.1061658,
    0.1061658 * 3,
    0.1061658 * 3,
    0.1061658
]

# Calculate the total sum of probabilities
total_probability = sum(probabilities)

# Calculate the percentages
percentage = [(p / total_probability) * 100 for p in probabilities]

# Display lengths for debugging
print(f"Length of Target_Class: {len(Target_Class)}")
print(f"Length of percentage: {len(percentage)}")
17/85:
# data manipulation against target class and probability
# Given values
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Calculating percentages
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Printing the percentages
for i, percentage in enumerate(percentages):
    print(f"Percentage{i + 1}: {percentage:.2f}%")
17/86:
import matplotlib.pyplot as plt

# Given data
target_classes = ["Class 1", "Class 2", "Class 3", "Class 4", "Class 5", "Class 6", "Class 7", "Class 8", "Class 9", "Class 10", "Class 11", "Class 12", "Class 13", "Class 14", "Class 15", "Class 16", "Class 17", "Class 18", "Class 19", "Class 20", "Class 21"]
percentages = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=target_classes, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
17/87:
import matplotlib.pyplot as plt

# Given data
percentages = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [{Target_Class[i]}, percentages]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
17/88:
import matplotlib.pyplot as plt

# Given data
target_classes = ["Class 1", "Class 2", "Class 3", "Class 4", "Class 5", "Class 6", "Class 7", "Class 8", "Class 9", "Class 10", "Class 11", "Class 12", "Class 13", "Class 14", "Class 15", "Class 16", "Class 17", "Class 18", "Class 19", "Class 20", "Class 21"]
percentages = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=target_classes, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{target} ({prob:.1f}%)' for target, prob in zip(target_classes, percentages)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/89:
import matplotlib.pyplot as plt

# Given data
percentages = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{{Target_Class[i]}} ({percentages})' for target, prob in zip({Target_Class[i]}, percentages)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
17/90:
import matplotlib.pyplot as plt

# Given data
percentages = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = {{Target_Class[i]}} ({percentages})
# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/1:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
18/2:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
18/3:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
18/4:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
18/5:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
18/6:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)
18/7:
# Assuming 'dataset' is your DataFrame
import matplotlib.pyplot as plt

Target_Class = dataset['Target Class'].unique()

# Define the probabilities based on your formula
probabilities = [
    0.1061658 * 4,
    0.1061658 * 2,
    0.1061658 * 16,
    0.1061658 * 12,
    0.1061658,
    0.1061658,
    0.1061658 * 22,
    0.1061658,
    1.0000000 + 0.1894583 + 0.1894583 + 0.1061658,
    0.1061658,
    0.1061658,
    0.1061658 * 2,
    0.1061658 * 2,
    0.1061658 * 2,
    0.1061658,
    0.1061658 * 2,
    0.1061658 * 15,
    0.1061658,
    0.1061658,
    0.1061658,
    0.1061658 * 3,
    0.1061658 * 3,
    0.1061658
]

# Calculate the total sum of probabilities
total_probability = sum(probabilities)

# Calculate the percentages
percentage = [(p / total_probability) * 100 for p in probabilities]

# Display lengths for debugging
print(f"Length of Target_Class: {len(Target_Class)}")
print(f"Length of percentage: {len(percentage)}")
18/8:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
18/9:
import matplotlib.pyplot as plt

# Given data
percentages = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
target_classes = ["Class 1", "Class 2", "Class 3", "Class 4", "Class 5", "Class 6", "Class 7", "Class 8", "Class 9", "Class 10", "Class 11", "Class 12", "Class 13", "Class 14", "Class 15", "Class 16", "Class 17", "Class 18", "Class 19", "Class 20", "Class 21"]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=target_classes, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{target} ({prob:.1f}%)' for target, prob in zip(target_classes, percentages)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/10:
import matplotlib.pyplot as plt

# Given data
percentages = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
target_classes = ["Class 1", "Class 2", "Class 3", "Class 4", "Class 5", "Class 6", "Class 7", "Class 8", "Class 9", "Class 10", "Class 11", "Class 12", "Class 13", "Class 14", "Class 15", "Class 16", "Class 17", "Class 18", "Class 19", "Class 20", "Class 21"]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=target_classes, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({prob:.1f}%)' for target, prob in zip(target_classes, percentages)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/11:
import matplotlib.pyplot as plt

# Given data
percentages = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
target_classes = ["Class 1", "Class 2", "Class 3", "Class 4", "Class 5", "Class 6", "Class 7", "Class 8", "Class 9", "Class 10", "Class 11", "Class 12", "Class 13", "Class 14", "Class 15", "Class 16", "Class 17", "Class 18", "Class 19", "Class 20", "Class 21"]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({prob:.1f}%)' for target, prob in zip(target_classes, percentages)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/12:
import matplotlib.pyplot as plt

# Given data
percentages = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
target_classes = ["Class 1", "Class 2", "Class 3", "Class 4", "Class 5", "Class 6", "Class 7", "Class 8", "Class 9", "Class 10", "Class 11", "Class 12", "Class 13", "Class 14", "Class 15", "Class 16", "Class 17", "Class 18", "Class 19", "Class 20", "Class 21"]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=target_classes, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({prob:.1f}%)' for target, prob in zip(target_classes, percentages)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/13:
import matplotlib.pyplot as plt

# Given data
percentages = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({prob:.1f}%)' for target, prob in zip(target_classes, percentages)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/14:
from Untitled-1.ipynb:pylance-notebook-cell:X23sZmlsZQ== import Target_Class
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/15:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/16:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
18/17:
import matplotlib.pyplot as plt

# Given data
percentages = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{target} ({prob:.1f}%)' for target, prob in zip(target_classes, percentages)]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/18:
import matplotlib.pyplot as plt

# Given data
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{target_classes[i]} ({percentages[i]:.1f}%)' for i in range(len(target_classes))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/19:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(10, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{target_class[i]} ({percentages[i]:.1f}%)' for i in range(len(target_class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/20:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{target_class[i]} ({percentages[i]:.1f}%)' for i in range(len(target_class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/21:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{dataset(Target_Class[i])} ({percentages[i]:.1f}%)' for i in range(len(dataset(Target_Class)))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/22:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/23:
# data manipulation against target class and probability
# Given values
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Calculating percentages
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Printing the percentages
for i, percentage in enumerate(percentages):
    print(f"Percentage{i + 1}: {percentage:.2f}%")
    sum(percentages)
18/24:
# data manipulation against target class and probability
# Given values
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Calculating percentages
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Printing the percentages
for i, percentage in enumerate(percentages):
    print(f"Percentage{i + 1}: {percentage:.2f}%")
total_percentage = sum(percentages)
print(f'The total percentage is: {total_percentage:.2f}%')
18/25:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]
plt.title("TARGET CLASS AND ITS PROBABILITY")
plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
18/26:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/27:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
18/28:
import matplotlib.pyplot as plt

# Given data
Target = dataset['Target'].unique()

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target[i]} ({percentages[i]:.1f}%)' for i in range(len(Target))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/29:
import matplotlib.pyplot as plt

# Given data
Target = dataset['Target'].unique()
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
plt.title("TARGET AND ITS PROBABILITY")

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target[i]} ({percentages[i]:.1f}%)' for i in range(len(Target))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/30:
import matplotlib.pyplot as plt

# Given data
target = dataset['Target']
encoded_labels2 = label_encoder.fit_transform(df2)
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
plt.title("TARGET AND ITS PROBABILITY")

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target[i]} ({percentages[i]:.1f}%)' for i in range(len(Target))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/31:
import matplotlib.pyplot as plt

# Given data
target = dataset['Target']

sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(encoded_labels2, percentages, labels=None, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
plt.title("TARGET AND ITS PROBABILITY")

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target[i]} ({percentages[i]:.1f}%)' for i in range(len(Target))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/32:
import matplotlib.pyplot as plt

# Given data
target = dataset['Target']

sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=target, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
plt.title("TARGET AND ITS PROBABILITY")

# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")

# Show the plot
plt.show()
18/33:
print(len(target))
print(len(percentages))
18/34:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
18/35:
# Feature Engineering
## Interaction Features
import pandas as pd

dataset['Target class and Probability interaction'] = dataset['Target Class'] * dataset['Probability*']
dataset['Target and Probability interaction'] = dataset['Target'] * dataset['Probability*']
18/36:
# Feature Engineering
## Interaction Features
import pandas as pd
import numpy as np

dataset['Target class and Probability interaction'] = dataset['Target Class'] * dataset['Probability*']
dataset['Target and Probability interaction'] = dataset['Target'] * dataset['Probability*']
18/37:
import pandas as pd
import numpy as np

dataset = pd.get_dummies(dataset, columns=['Target Class', 'Probability*'])

# Perform the multiplication operation
dataset['Target class and Probability interaction'] = dataset['Target Class_1'] * dataset['Probability*_1']
dataset['Target and Probability interaction'] = dataset['Target'] * dataset['Probability*']
18/38:
import pandas as pd
import numpy as np

dataset = pd.get_dummies(dataset, columns=['Target Class', 'Probability*'])

# Perform the multiplication operation
dataset['Target class and Probability interaction'] = dataset['Target Class'] * dataset['Probability*']
dataset['Target and Probability interaction'] = dataset['Target'] * dataset['Probability*']
18/39:
import pandas as pd
import numpy as np

dataset = pd.get_dummies(dataset, columns=[encoded_labels1, 'Probability*'])

# Perform the multiplication operation
dataset['Target class and Probability interaction'] = [encoded_labels1] * dataset['Probability*']
dataset['Target and Probability interaction'] = [encoded_labels2] * dataset['Probability*']
18/40:
import pandas as pd
import numpy as np

dataset = pd.get_dummies(dataset, columns=[encoded_labels1, 'Probability*'])

# Perform the multiplication operation
dataset['Target class and Probability interaction'] = dataset[encoded_labels1] * dataset['Probability*']
dataset['Target and Probability interaction'] = dataset[encoded_labels2] * dataset['Probability*']
18/41:
from sklearn.ensemble import RandomForestRegressor

# Assuming 'X' is your feature matrix and 'y' is your target variable
model = RandomForestRegressor()
model.fit(dataset['Target Class']), (dataset['Probability*'])

feature_importance = model.feature_importances_
18/42:
from sklearn.ensemble import RandomForestRegressor

# Assuming 'X' is your feature matrix and 'y' is your target variable
X = dataset['Target_Class']
y = dataset['Probability*']

model = RandomForestRegressor()
model.fit(X, y)

feature_importance = model.feature_importances_
18/43:
from sklearn.ensemble import RandomForestRegressor

# Assuming 'X' is your feature matrix and 'y' is your target variable
X = dataset['Target Class']
y = dataset['Probability*']

model = RandomForestRegressor()
model.fit(X, y)

feature_importance = model.feature_importances_
18/44:
from sklearn.ensemble import RandomForestRegressor

X = dataset['Target_Class']
y = dataset['Probability*']

model = RandomForestRegressor()
model.fit(X, y)

feature_importance = model.feature_importances_
18/45: Target_Class.unique()
18/46: Target_Class.unique()\
18/47:
Target_Class.unique()
for category in categories:
    print(category)
18/48:
Target = Target_Class.unique()
print(Target)
display(Target)
18/49:
from tabulate import tabulate

categories = dataset['Target Class']

# Convert the categories to a list of lists for tabulate
categories_table = [[category] for category in categories]

# Display the table
print(tabulate(categories_table, headers=["Categories"], tablefmt="grid"))
18/50:
from tabulate import tabulate

categories = Target_Class.unique()

# Convert the categories to a list of lists for tabulate
categories_table = [[category] for category in categories]

# Display the table
print(tabulate(categories_table, headers=["Categories"], tablefmt="grid"))
18/51:
from tabulate import tabulate

categories = Target_Class.unique()

# Convert the categories to a list of lists for tabulate
categories_table = [[category] for category in categories]

# Display the table
print(tabulate(categories_table, headers=["Categories"], tablefmt="grid"))
18/52:
from tabulate import tabulate

categories = Target_Class.unique()

# Convert the categories to a list of lists for tabulate
categories_table = [[category] for category in categories]

# Display the table
print(tabulate(categories_table, headers=["Categories"], tablefmt="grid"))
18/53: Porbability*.unique()
18/54: Probability*.unique()
18/55: Probability*.unique()
18/56: 'Probability*'.unique()
18/57: dataset['Probability*'].unique()
18/58:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
dataset['Probability'] = dataset['Probability'].unique()
dataset['Probability']
18/59:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
18/60:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
18/61:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
18/62:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
18/63:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
18/64:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
dataset['Probability'] = dataset['Probability'].unique()
dataset['Probability']
18/65:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
dataset['Probability'] = dataset['Probability*'].unique()
dataset['Probability']
18/66:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
18/67:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
18/68:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
18/69:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
18/70:
# data manipulation against target class and probability
# Given values
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Calculating percentages
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Printing the percentages
for i, percentage in enumerate(percentages):
    print(f"Percentage{i + 1}: {percentage:.2f}%")
18/71:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
18/72:
from sklearn.ensemble import RandomForestClassifier

# Assuming 'X_class' is your feature matrix and 'y_class' is your target class
X_class = dataset['Nuclear receptor', 'Cytochrome P450', 'Enzyme', 'Kinase', 'Hydrolase', 'Protease', 'Phosphatase', 'Family A G protein-coupled receptor', 'Primary active transporter', 'Oxidoreductase', 'Secreted protein', 'Unclassified protein', 'Family C G protein-coupled receptor', 'Transferase', 'Other nuclear protein', 'Membrane receptor', 'Phosphodiesterase', 'Toll-like and Il-1 receptors', 'Voltage-gated ion channel', 'Other cytosolic protein', 'Electrochemical transporter']
y_class = dataset['Target Class']

# Train the Random Forest Classifier for target class
class_model = RandomForestClassifier()
class_model.fit(X_class, y_class)

# Assuming 'X_prob' is your feature matrix and 'y_prob' is your target probability
X_prob = dataset[[]]  # Replace with your actual feature names
y_prob = dataset['Probability*']  # Assuming this is your target probability

# Train the Random Forest Regressor for target probability
reg_model = RandomForestRegressor()
reg_model.fit(X_prob, y_prob)
18/73:
from tabulate import tabulate

categories = dataset['Probability*'].unique()

# Convert the categories to a list of lists for tabulate
categories_table = [[category] for category in categories]

# Display the table
print(tabulate(categories_table, headers=["Categories"], tablefmt="grid"))
18/74:
from sklearn.ensemble import RandomForestClassifier

# Assuming 'X_class' is your feature matrix and 'y_class' is your target class
X_class = dataset['Nuclear receptor', 'Cytochrome P450', 'Enzyme', 'Kinase', 'Hydrolase', 'Protease', 'Phosphatase', 'Family A G protein-coupled receptor', 'Primary active transporter', 'Oxidoreductase', 'Secreted protein', 'Unclassified protein', 'Family C G protein-coupled receptor', 'Transferase', 'Other nuclear protein', 'Membrane receptor', 'Phosphodiesterase', 'Toll-like and Il-1 receptors', 'Voltage-gated ion channel', 'Other cytosolic protein', 'Electrochemical transporter']
y_class = dataset['Target Class']

# Train the Random Forest Classifier for target class
class_model = RandomForestClassifier()
class_model.fit(X_class, y_class)

# Assuming 'X_prob' is your feature matrix and 'y_prob' is your target probability
X_prob = dataset[[0.106166, 0, 1, 0.189458]]  
y_prob = dataset['Probability*']

# Train the Random Forest Regressor for target probability
reg_model = RandomForestRegressor()
reg_model.fit(X_prob, y_prob)
18/75:
from sklearn.ensemble import RandomForestClassifier

# Assuming 'X_class' is your feature matrix and 'y_class' is your target class
X_class = dataset[['Nuclear receptor', 'Cytochrome P450', 'Enzyme', 'Kinase', 'Hydrolase', 'Protease', 'Phosphatase', 'Family A G protein-coupled receptor', 'Primary active transporter', 'Oxidoreductase', 'Secreted protein', 'Unclassified protein', 'Family C G protein-coupled receptor', 'Transferase', 'Other nuclear protein', 'Membrane receptor', 'Phosphodiesterase', 'Toll-like and Il-1 receptors', 'Voltage-gated ion channel', 'Other cytosolic protein', 'Electrochemical transporter']]
y_class = dataset['Target Class']

# Train the Random Forest Classifier for target class
class_model = RandomForestClassifier()
class_model.fit(X_class, y_class)

# Assuming 'X_prob' is your feature matrix and 'y_prob' is your target probability
X_prob = dataset[['0.106166', '0', '1', '0.189458']]  # Replace with your actual feature names
y_prob = dataset['Probability*']

# Train the Random Forest Regressor for target probability
reg_model = RandomForestRegressor()
reg_model.fit(X_prob, y_prob)
19/1:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

X = dataset
y = dataset

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
19/2:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
19/3:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
19/4:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
19/5:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
19/6:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
19/7:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
19/8:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
19/9:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
19/10:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
19/11:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
19/12:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
19/13:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
19/14:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

X = dataset
y = dataset

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
19/15:
from sklearn.model_selection import train_test_split

# Assuming df is your DataFrame containing the preprocessed data
# X should contain features (target class, target, common name, UniProt ID, ChEMBL ID)
# y should contain the target variable (probability)

X = dataset['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
19/16:
from sklearn.model_selection import train_test_split

# Assuming df is your DataFrame containing the preprocessed data
# X should contain features (target class, target, common name, UniProt ID, ChEMBL ID)
# y should contain the target variable (probability)

X = dataset['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
19/17:
from sklearn.model_selection import train_test_split

# Assuming df is your DataFrame containing the preprocessed data
# X should contain features (target class, target, common name, UniProt ID, ChEMBL ID)
# y should contain the target variable (probability)

X = dataset('Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID')
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
19/18:
from sklearn.model_selection import train_test_split

# Assuming df is your DataFrame containing the preprocessed data
# X should contain features (target class, target, common name, UniProt ID, ChEMBL ID)
# y should contain the target variable (probability)

X = dataset('Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID')
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
19/19:
from sklearn.model_selection import train_test_split

# Assuming df is your DataFrame containing the preprocessed data
# X should contain features (target class, target, common name, UniProt ID, ChEMBL ID)
# y should contain the target variable (probability)

X = dataset('Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID')
y = dataset('Probability*')

# Split the data into training and testing sets (e.g., 80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
19/20:
# Assuming df is your DataFrame containing the preprocessed data
# X should contain features (target class, target, common name, UniProt ID, ChEMBL ID)
# y should contain the target variable (probability)

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
19/21:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
19/22:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
19/23:
# Model Selection

from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
19/24:
# Model Selection

from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming 'data' is your DataFrame and 'target' is the column you're trying to predict
# X should be the features and y should be the target variable

# Separate categorical and numerical columns
categorical_cols = ['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID']
numerical_cols = ['Target', ...]

# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(data[categorical_cols])

# Combine with numerical columns
X_numerical = data[numerical_cols]

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Now you can proceed with splitting and training the model as before

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
19/25:
# Model Selection

from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming 'data' is your DataFrame and 'target' is the column you're trying to predict
# X should be the features and y should be the target variable

# Separate categorical and numerical columns
categorical_cols = ['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID']
numerical_cols = ['Target', ...]

# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(data[categorical_cols])

# Combine with numerical columns
X_numerical = dataset[numerical_cols]

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Now you can proceed with splitting and training the model as before

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
19/26:
# Model Selection

from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming 'data' is your DataFrame and 'target' is the column you're trying to predict
# X should be the features and y should be the target variable

# Separate categorical and numerical columns
categorical_cols = ['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID']
numerical_cols = ['Target', ...]

# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(dataset[categorical_cols])

# Combine with numerical columns
X_numerical = dataset[numerical_cols]

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Now you can proceed with splitting and training the model as before

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/1:
# Model Selection

from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming 'data' is your DataFrame and 'target' is the column you're trying to predict
# X should be the features and y should be the target variable

# Separate categorical and numerical columns
categorical_cols = ['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID']
numerical_cols = ['Probability*']
# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(dataset[categorical_cols])

# Combine with numerical columns
X_numerical = dataset[numerical_cols]

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Now you can proceed with splitting and training the model as before

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/2:
# Model Selection

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Separate categorical and numerical columns
categorical_cols = ['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID']
numerical_cols = ['Probability*']
# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(dataset[categorical_cols])

# Combine with numerical columns
X_numerical = dataset[numerical_cols]

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Now you can proceed with splitting and training the model as before

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/3:
# Model Selection

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Separate categorical and numerical columns
categorical_cols = ['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID']
numerical_cols = ['Probability*']
# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(dataset[categorical_cols])

# Combine with numerical columns
X_numerical = dataset[numerical_cols]

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Now you can proceed with splitting and training the model as before

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/4:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
20/5:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
20/6:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
20/7:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
20/8:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
20/9:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
20/10:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
20/11:
# data manipulation against target class and probability
# Given values
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]

# Calculating percentages
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Printing the percentages
for i, percentage in enumerate(percentages):
    print(f"Percentage{i + 1}: {percentage:.2f}%")
20/12:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
20/13:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
20/14:
# Model Selection

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Separate categorical and numerical columns
categorical_cols = ['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID']
numerical_cols = ['Probability*']
# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(dataset['Target'])

# Combine with numerical columns
X_numerical = dataset[numerical_cols]

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Now you can proceed with splitting and training the model as before

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/15:
# Model Selection

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Separate categorical and numerical columns
categorical_cols = ['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID']
numerical_cols = ['Probability*']
# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(dataset[categorical_cols])

# Combine with numerical columns
X_numerical = dataset[numerical_cols]

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Now you can proceed with splitting and training the model as before

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/16:
# Label encoding
from sklearn.preprocessing import LabelEncoder

LE = dataset['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID'] 
df = LabelEncoder()
LE = label_encoder.fit_transform(df)
print(LE)
20/17:
# Label encoding
from sklearn.preprocessing import LabelEncoder

categorical_cols = ['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID']

label_encoder = LabelEncoder()

for col in categorical_cols:
    df = label_encoder.fit_transform(dataset[col])

print(df)
20/18:
from sklearn.preprocessing import LabelEncoder

categorical_cols = ['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID']

label_encodings = {}

for col in categorical_cols:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(dataset[col])
    label_encodings[col] = df_encoded

# Print label encodings for each column
for col, encoding in label_encodings.items():
    print(f"Label Encoding for {col}:")
    print(encoding)
20/19:
from sklearn.preprocessing import LabelEncoder

categorical_cols = ['Target Class', 'Common name', 'Uniprot ID', 'ChEMBL ID']

label_encodings = {}

for col in categorical_cols:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(dataset[col])
    label_encodings[col] = df_encoded

# Print label encodings for each column
for col, df in label_encodings.items():
    print(f"Label Encoding for {col}:")
    print(df)
20/20:
# Model Selection

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Separate categorical and numerical columns
categorical_cols = df
numerical_cols = ['Probability*']
# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(dataset[categorical_cols])

# Combine with numerical columns
X_numerical = dataset[numerical_cols]

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Now you can proceed with splitting and training the model as before

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/21:
# Model Selection

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Separate categorical and numerical columns
categorical_cols = df
numerical_cols = dataset['Probability*']
# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(dataset[categorical_cols])

# Combine with numerical columns
X_numerical = dataset[numerical_cols]

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Now you can proceed with splitting and training the model as before

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/22:
# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(dataset[categorical_cols])

# Assuming 'numerical_cols' contains the correct column names for numerical features
X_numerical = dataset['Probability*']

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Split the data into training and testing sets
# (Remember to import train_test_split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/23:
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Perform one-hot encoding on categorical columns
X_categorical = pd.get_dummies(dataset[categorical_cols])

# Assuming 'numerical_cols' contains the correct column names for numerical features
X_numerical = dataset['Probability*']

X = pd.concat([X_categorical, X_numerical], axis=1)
y = dataset['Probability*']

# Split the data into training and testing sets
# (Remember to import train_test_split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/24:
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_numerical = dataset['Probability*']
X = pd.concat([df, X_numerical], axis=1)
y = dataset['Probability*']

# Split the data into training and testing sets
# (Remember to import train_test_split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/25:
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_numerical = dataset['Probability*']
X = pd.concat([df, X_numerical], axis=1)
y = dataset['Probability*']

# Split the data into training and testing sets
# (Remember to import train_test_split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/26:
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming 'df' is your DataFrame and 'dataset' contains 'Probability*'
X_numerical = dataset['Probability*']
X_numerical = pd.DataFrame(X_numerical, columns=['Probability*'])  # Convert to DataFrame

# Assuming 'df' is your DataFrame
X = pd.concat([df, X_numerical], axis=1)
y = dataset['Probability*']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/27:
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/28:
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(df, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
20/29:
from sklearn.preprocessing import LabelEncoder

X_train_LE = X_train

label_encodings = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(dataset[col])
    label_encodings[col] = df_encoded

# Print label encodings for each column
for col, train_set in label_encodings.items():
    print(f"Label Encoding for {col}:")
    print(train_set)
20/30:
from sklearn.preprocessing import LabelEncoder

X_train_LE = X_train

label_encodings = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(dataset[col])
    label_encodings[col] = df_encoded

# Print label encodings for each column
for col, train_set in label_encodings.items():
    print(f"Label Encoding for {col}:")
    print(train_set)

X_test_LE = X_test

label_encodings = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(dataset[col])
    label_encodings[col] = df_encoded

# Print label encodings for each column
for col, test_set in label_encodings.items():
    print(f"Label Encoding for {col}:")
    print(test_set)
23/1:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
23/2: _ih[-15:]
23/3:
from sysconfig import get_python_version


def rescue_code(function):
    import inspect
    get_python_version().set_next_input("".join(inspect.getsourcelines(function)[0]))
    rescue_code(f)
23/4:
from sysconfig import get_python_version


def rescue_code(function):
    import inspect
    get_python_version().set_next_input("".join(inspect.getsourcelines(function)[0]))
    print(rescue_code(f))
23/5:
from sysconfig import get_python_version


def rescue_code(function):
    import inspect
    get_python_version().set_next_input("".join(inspect.getsourcelines(function)[0]))
    print(rescue_code(f))
23/6:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
23/7: %history
23/8: %history -g -f filename
23/9: %history -o
23/10:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
23/11:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
23/12:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
23/13:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
23/14:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
23/15:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
23/16:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")
23/17:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
23/18:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
23/19:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
23/20:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

X = dataset
y = dataset

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
23/21:
from sklearn.preprocessing import LabelEncoder

X_train_LE = X_train

label_encodings = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(dataset[col])
    label_encodings[col] = df_encoded

# Print label encodings for each column
for col, train_set in label_encodings.items():
    print(f"Label Encoding for {col}:")
    print(train_set)

X_test_LE = X_test

label_encodings = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(dataset[col])
    label_encodings[col] = df_encoded

# Print label encodings for each column
for col, test_set in label_encodings.items():
    print(f"Label Encoding for {col}:")
    print(test_set)
24/1:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
24/2:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
24/3:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
24/4:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
24/5:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
24/6:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
24/7:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
24/8:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
24/9:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
24/10:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

X = dataset[tar]
y = dataset

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
24/11:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

X = dataset['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
24/12:
# Print the column names
print(df.columns)

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']
24/13:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
24/14:
from sklearn.preprocessing import LabelEncoder

X_train_LE = X_train

label_encodings = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(dataset[col])
    label_encodings[col] = df_encoded

# Print label encodings for each column
for col, train_set in label_encodings.items():
    print(f"Label Encoding for {col}:")
    print(train_set)

X_test_LE = X_test

label_encodings = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(dataset[col])
    label_encodings[col] = df_encoded

# Print label encodings for each column
for col, test_set in label_encodings.items():
    print(f"Label Encoding for {col}:")
    print(test_set)
24/15:
from sklearn.preprocessing import LabelEncoder

X_train_LE = X_train

label_encodings = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(dataset[col])
    label_encodings[col] = df_encoded

# Print label encodings for each column
for col, train_set in label_encodings.items():
    print(f"Label Encoding for {col}:")
    print(train_set)

X_test_LE = X_test

label_encodings = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(dataset[col])
    label_encodings[col] = df_encoded

# Print label encodings for each column
for col, test_set in label_encodings.items():
    print(f"Label Encoding for {col}:")
    print(test_set)
print(len(X_train['Target Class']))
print(len(X_test['Target Class']))
24/16:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(test_set)

print(len(X_train_LE['Target Class']))
print(len(X_test_LE['Target Class']))
24/17: jupyter nbconvert --to html my-notebook.ipynb
24/18:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
24/19:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
24/20:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
24/21:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
24/22:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
24/23:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
24/24:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
24/25:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
24/26:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
24/27:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
24/28:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(test_set)

print(len(X_train_LE['Target Class']))
print(len(X_test_LE['Target Class']))
24/29:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(X_train,y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
24/30:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(X_train_LE,y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
24/31:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(X_train_LE,y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
24/32:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(X_train_LE,y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
24/33:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(X_train_LE,y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
24/34:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(test_set)

print(len(X_train_LE['Target Class']))
print(len(X_test_LE['Target Class']))
24/35:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(test_set)

print(len(X_train_LE['Target Class']))
print(len(X_test_LE['Target Class']))
24/36:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(X_train_LE,y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
24/37:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(train_set)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
24/38:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(train_set, y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
24/39:
# Train and test set reshaping
import numpy as np
from sklearn.preprocessing import StandardScaler

# Create some sample data
data1 = np.array(df[train_set])
data2 = np.array(df[test_set])

# Reshape data into 1D array with unknown number of columns
df1 = data1.reshape(1, -1)
df2 = data2.reshape(1, -1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the data and transform the data
scaled_data1 = scaler.fit_transform(df1)
scaled_data2 = scaler.fit_transform(df1)

print("Original data:")
print(data1, data2)
print("\nScaled data:")
print(df1, df2)
24/40:
# Train and test set reshaping
import numpy as np
from sklearn.preprocessing import StandardScaler

# Create some sample data
data1 = (df[train_set])
data2 = (df[test_set])

# Reshape data into 1D array with unknown number of columns
df1 = data1.reshape(1, -1)
df2 = data2.reshape(1, -1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the data and transform the data
scaled_data1 = scaler.fit_transform(df1)
scaled_data2 = scaler.fit_transform(df1)

print("Original data:")
print(data1, data2)
print("\nScaled data:")
print(df1, df2)
24/41:
# Train and test set reshaping
import numpy as np
from sklearn.preprocessing import StandardScaler

# Create some sample data
data1 = (df(train_set))
data2 = (df(test_set))

# Reshape data into 1D array with unknown number of columns
df1 = data1.reshape(1, -1)
df2 = data2.reshape(1, -1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the data and transform the data
scaled_data1 = scaler.fit_transform(df1)
scaled_data2 = scaler.fit_transform(df1)

print("Original data:")
print(data1, data2)
print("\nScaled data:")
print(df1, df2)
24/42:
# Train and test set reshaping
import numpy as np
from sklearn.preprocessing import StandardScaler

# Create some sample data
data1 = df(train_set)
data2 = df(test_set)

# Reshape data into 1D array with unknown number of columns
df1 = data1.reshape(1, -1)
df2 = data2.reshape(1, -1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the data and transform the data
scaled_data1 = scaler.fit_transform(df1)
scaled_data2 = scaler.fit_transform(df1)

print("Original data:")
print(data1, data2)
print("\nScaled data:")
print(df1, df2)
24/43:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler


# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
scaled_train_data = scaler.fit_transform(train_set.values.reshape(1, -1))

# Transform the test set using the scaler fitted on the train set
scaled_test_data = scaler.transform(test_set.values.reshape(1, -1))

print("Original data:")
print(train_set, "\n", test_set)
print("\nScaled data:")
print(scaled_train_data, "\n", scaled_test_data)
24/44:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Initialize the StandardScaler
scaler = StandardScaler()

# Reshape data into 1D array with unknown number of columns
df1 = train_set.reshape(1, -1)
df2 = test_set.reshape(1, -1)

# Fit the scaler to the train set and transform the data
scaled_train_data = scaler.fit_transform(df1)

# Transform the test set using the scaler fitted on the train set
scaled_test_data = scaler.transform(df2)

print("Original data:")
print(train_set, "\n", test_set)
print("\nScaled data:")
print(scaled_train_data, "\n", scaled_test_data)
24/45:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 2D arrays
# train_set.shape = (n_samples_train, n_features)
# test_set.shape = (n_samples_test, n_features)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
scaled_train_data = scaler.fit_transform(train_set)

# Transform the test set using the scaler fitted on the train set
scaled_test_data = scaler.transform(test_set)

print("Original data:")
print(train_set, "\n", test_set)
print("\nScaled data:")
print(scaled_train_data, "\n", scaled_test_data)
24/46:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train = train_set
test = test_set

# Reshape the data
train = train.reshape(-1, 1)
test = test.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
scaled_train_data = scaler.fit_transform(train)

# Transform the test set using the scaler fitted on the train set
scaled_test_data = scaler.transform(test)

print("Original data:")
print(train, "\n", test)
print("\nScaled data:")
print(scaled_train_data, "\n", scaled_test_data)
24/47:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train = train_set
test = test_set

# Reshape the data
train = train.reshape(-1, 1)
test = test.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data = scaler.fit_transform(train)

# Transform the test set using the scaler fitted on the train set
test_data = scaler.transform(test)

print("Original data:")
print(train, "\n", test)
print("\nScaled data:")
print(train_data, "\n", test_data)
24/48:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(train_data, y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
24/49:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(train_data, y_train)
#Predict the response for test dataset
y_pred = clf.predict(test_data)
24/50:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set


# Reshape the data
train = train_X.reshape(-1, 1)
test = test_X.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train)

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test)

print("Original data:")
print(train, "\n", test)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
24/51:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
24/52:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set


# Reshape the data
train = train_X.reshape(-1, 1)
test = test_X.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train)

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test)

print("Original data:")
print(train, "\n", test)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
26/1:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scalar.

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test1)

print("Original data:")
print(train, "\n", test)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
26/2:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scalar.

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test1)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
26/3:
from matplotlib import Scalar
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = Scalar.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test1)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
26/4:
from matplotlib import Scalar
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)  # Corrected this line

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test1)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
26/5:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test1)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
26/6:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
26/7:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
26/8:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
26/9:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
26/10:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
26/11:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
26/12:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
26/13:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
26/14:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
26/15:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
26/16:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
26/17:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
26/18:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test1)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
26/19:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
26/20:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
26/21:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
26/22:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
26/23:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
26/24:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
26/25:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
26/26:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
26/27:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
26/28:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
26/29:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
26/30:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test1)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
26/31:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Convert Series to NumPy array and then reshape
train1 = train_X.values.reshape(-1, 1)
test1 = test_X.values.reshape(-1, 1)
train2 = train_Y.values.reshape(-1, 1)
test2 = test_Y.values.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test1)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
26/32: %history --o
26/33: %history -g -f filename
27/1:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
27/2:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
27/3:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
27/4:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
27/5:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
27/6:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
27/7:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
27/8:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
27/9:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
27/10:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
27/11:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
27/12:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test1)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
27/13:
import numpy as np

# Assuming you have a 1D array
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train1 = train_X.reshape(1, -1)
test1 = test_X.reshape(1, -1)
train2 = train_Y.reshape(1, -1)
test2 = test_Y.reshape(1, -1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test1)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
27/14:
import numpy as np

# Assuming you have a 1D array
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train1 = train_X.reshape(1, -1)
test1 = test_X.reshape(1, -1)
train2 = train_Y.reshape(1, -1)
test2 = test_Y.reshape(1, -1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data1 = scaler.transform(test1)
test_data2 = scaler.transform(test1)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
27/15:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_X and test_X are pandas Series objects
train_X = X_train_set.values
test_X = X_test_set.values

train_Y = y_train.values
test_Y = y_test.values

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data1 = scaler.transform(test1)
test_data2 = scaler.transform(test2)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
27/16:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_X and test_X are pandas Series objects
train_X = X_train_set.values
test_X = X_test_set.values

train_Y = y_train.values
test_Y = y_test.values

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data1 = scaler.transform(test1)
test_data2 = scaler.transform(test2)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
27/17:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_X and test_X are pandas Series objects
train_X = X_train_set.values
test_X = X_test_set.values

train_Y = y_train.values
test_Y = y_test.values

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data1 = scaler.transform(test1)
test_data2 = scaler.transform(test2)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
27/18:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_X and test_X are pandas Series objects
train_X = X_train_set.values
test_X = X_test_set.values

train_Y = y_train.values
test_Y = y_test.values

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data1 = scaler.transform(test1)
test_data2 = scaler.transform(test2)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
27/19: %history
27/20: %history --o
27/21: %history -g -f filename
28/1:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming train_X and test_X are numpy arrays
train_X = np.array(X_train_set)
test_X = np.array(X_test_set)
train_Y = np.array(y_train)
test_Y = np.array(y_test)

# Reshape the data
train_X = train_X.reshape(-1, 1)
test_X = test_X.reshape(-1, 1)
train_Y = train_Y.reshape(-1, 1)
test_Y = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data and transform it
train_X = scaler.fit_transform(train_X)
test_X = scaler.transform(test_X)

# Fit the scaler to the training labels and transform it
train_Y = scaler.fit_transform(train_Y)
test_Y = scaler.transform(test_Y)

print("Original data:")
print(train_X, "\n", test_X)
print("\nScaled data:")
print(train_Y, "\n", test_Y)
28/2:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
28/3:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
28/4:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
28/5:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
28/6:
import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming train_set and test_set are 1D arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train1 = train_X.reshape(-1, 1)
test1 = test_X.reshape(-1, 1)
train2 = train_Y.reshape(-1, 1)
test2 = test_Y.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_data1 = scaler.fit_transform(train1)
train_data2 = scaler.fit_transform(train2)

# Transform the test set using the scaler fitted on the train set
test_data2 = scaler.transform(test1)

print("Original data:")
print(train1, "\n", test1)
print("\nScaled data:")
print(train_data1, "\n", test_data2)
28/7:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming train_X and test_X are pandas Series objects
train_X = X_train_set.values
test_X = X_test_set.values
train_Y = y_train.values
test_Y = y_test.values

# Initialize the StandardScaler
scaler = StandardScaler()

# Reshape the data (Not needed for pandas Series)
# train_X = train_X.reshape(-1, 1)
# test_X = test_X.reshape(-1, 1)
# train_Y = train_Y.reshape(-1, 1)
# test_Y = test_Y.reshape(-1, 1)

# Fit the scaler to the train set and transform the data
train_X_scaled = scaler.fit_transform(train_X.reshape(-1, 1))
train_Y_scaled = scaler.fit_transform(train_Y.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_X_scaled = scaler.transform(test_X.reshape(-1, 1))
test_Y_scaled = scaler.transform(test_Y.reshape(-1, 1))

print("Original data:")
print(train_X, "\n", test_X)
print("\nScaled data:")
print(train_X_scaled, "\n", test_X_scaled)
28/8:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming train_X and test_X are pandas Series objects
train_X = X_train_set.values
test_X = X_test_set.values
train_Y = y_train.values
test_Y = y_test.values

# Initialize the StandardScaler
scaler = StandardScaler()

# Reshape the data (Not needed for pandas Series)
train_X = train_X.reshape(-1, 1)
test_X = test_X.reshape(-1, 1)
train_Y = train_Y.reshape(-1, 1)
test_Y = test_Y.reshape(-1, 1)

# Fit the scaler to the train set and transform the data
train_X_scaled = scaler.fit_transform(train_X.reshape(-1, 1))
train_Y_scaled = scaler.fit_transform(train_Y.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_X_scaled = scaler.transform(test_X.reshape(-1, 1))
test_Y_scaled = scaler.transform(test_Y.reshape(-1, 1))

print("Original data:")
print(train_X, "\n", test_X)
print("\nScaled data:")
print(train_X_scaled, "\n", test_X_scaled)
28/9:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming train_X and test_X are NumPy arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Reshape the data
train_X = train_X.reshape(1, -1)
test_X = test_X.reshape(1, -1)
train_Y = train_Y.reshape(1, -1)
test_Y = test_Y.reshape(1, -1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_X_scaled = scaler.fit_transform(train_X)
train_Y_scaled = scaler.fit_transform(train_Y)

# Transform the test set using the scaler fitted on the train set
test_X_scaled = scaler.transform(test_X)
test_Y_scaled = scaler.transform(test_Y)

print("Original data:")
print(train_X, "\n", test_X)
print("\nScaled data:")
print(train_X_scaled, "\n", test_X_scaled)
28/10:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming train_X and test_X are pandas Series objects
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_X_scaled = scaler.fit_transform(train_X.values.reshape(-1, 1))
train_Y_scaled = scaler.fit_transform(train_Y.values.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_X_scaled = scaler.transform(test_X.values.reshape(-1, 1))
test_Y_scaled = scaler.transform(test_Y.values.reshape(-1, 1))

print("Original data:")
print(train_X, "\n", test_X)
print("\nScaled data:")
print(train_X_scaled, "\n", test_X_scaled)
28/11:
from sklearn.preprocessing import StandardScaler

# Assuming train_X and test_X are NumPy arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Initialize the StandardScaler
scaler = StandardScaler()

# Reshape the data
train_X = train_X.reshape(1, -1)
test_X = test_X.reshape(1, -1)
train_Y = train_Y.reshape(1, -1)
test_Y = test_Y.reshape(1, -1)

# Fit the scaler to the train set and transform the data
train_X_scaled = scaler.fit_transform(train_X)
train_Y_scaled = scaler.fit_transform(train_Y)

# Transform the test set using the scaler fitted on the train set
test_X_scaled = scaler.transform(test_X)
test_Y_scaled = scaler.transform(test_Y)

print("Original data:")
print(train_X, "\n", test_X)
print("\nScaled data:")
print(train_X_scaled, "\n", test_X_scaled)
28/12:
from sklearn.preprocessing import StandardScaler

# Assuming train_X and test_X are NumPy arrays
train_X = X_train_set
test_X = X_test_set
train_Y = y_train
test_Y = y_test

# Initialize the StandardScaler
scaler = StandardScaler()

# Reshape the data
train_X = train_X.reshape(1, -1)
test_X = test_X.reshape(1, -1)
train_Y = train_Y.reshape(1, -1)
test_Y = test_Y.reshape(1, -1)

# Fit the scaler to the train set and transform the data
train_X_scaled = scaler.fit_transform(train_X)
train_Y_scaled = scaler.fit_transform(train_Y)

# Transform the test set using the scaler fitted on the train set
test_X_scaled = scaler.transform(test_X)
test_Y_scaled = scaler.transform(test_Y)

print("Original data:")
print(train_X, "\n", test_X)
print("\nScaled data:")
print(train_X_scaled, "\n", test_X_scaled)
28/13:
from sklearn.preprocessing import StandardScaler

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_Y_scaled = scaler.fit_transform(train_Y.values.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_Y_scaled = scaler.transform(test_Y.values.reshape(-1, 1))

print("Original data:")
print(train_Y, "\n", test_Y)
print("\nScaled data:")
print(train_Y_scaled, "\n", test_Y_scaled)
28/14:
from sklearn.preprocessing import StandardScaler

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_Y_scaled = scaler.fit_transform(train_Y.values.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_Y_scaled = scaler.transform(test_Y.values.reshape(-1, 1))

print("Original data:")
print(train_Y, "\n", test_Y)
print("\nScaled data:")
print(train_Y_scaled, "\n", test_Y_scaled)
28/15:
from sklearn.preprocessing import StandardScaler

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_Y_scaled = scaler.fit_transform(train_Y.values.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_Y_scaled = scaler.transform(test_Y.values.reshape(-1, 1))

print("Original data:")
print(train_Y, "\n", test_Y)
print("\nScaled data:")
print(train_Y_scaled, "\n", test_Y_scaled)
28/16:
from sklearn.preprocessing import StandardScaler

train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_Y_scaled = scaler.fit_transform(train_Y.values.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_Y_scaled = scaler.transform(test_Y.values.reshape(-1, 1))

print("Original data:")
print(train_Y, "\n", test_Y)
print("\nScaled data:")
print(train_Y_scaled, "\n", test_Y_scaled)
28/17:
from sklearn.preprocessing import StandardScaler

train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_Y_scaled = scaler.fit_transform(train_Y.values.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_Y_scaled = scaler.transform(test_Y.values.reshape(-1, 1))

print("Original data:")
print(train_Y, "\n", test_Y)
print("\nScaled data:")
print(train_Y_scaled, "\n", test_Y_scaled)
28/18:
from sklearn.preprocessing import StandardScaler

train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_Y_scaled = scaler.fit_transform(train_Y.values.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_Y_scaled = scaler.transform(test_Y.values.reshape(-1, 1))

print("Original data:")
print(train_Y, "\n", test_Y)
print("\nScaled data:")
print(train_Y_scaled, "\n", test_Y_scaled)
28/19:
from sklearn.preprocessing import StandardScaler

train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_Y_scaled = scaler.fit_transform(train_Y.values.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_Y_scaled = scaler.transform(test_Y.values.reshape(-1, 1))

print("Original data:")
print(train_Y, "\n", test_Y)
print("\nScaled data:")
print(train_Y_scaled, "\n", test_Y_scaled)
28/20:
from sklearn.preprocessing import StandardScaler

train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_Y_scaled = scaler.fit_transform(train_Y.values.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_Y_scaled = scaler.transform(test_Y.values.reshape(-1, 1))

print("Original data:")
print(train_Y, "\n", test_Y)
print("\nScaled data:")
print(train_Y_scaled, "\n", test_Y_scaled)
28/21:
from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

print(train_X, "\n", test_X)
print(X_train_data, "\n", X_test_data)
print(train_Y, "\n", test_Y)
print(Y_train_data, "\n", Y_test_data)
28/22:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(X_train_data, y_train)
#Predict the response for test dataset
y_pred = clf.predict(Y_test_data)
28/23:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()

# Train Decision Tree Classifier
clf = clf.fit(X_train_data, y_train)

# Predict the response for test dataset
y_pred = clf.predict(X_test_data)
28/24:
from sklearn.tree import DecisionTreeClassifier 

# Create Decision Tree classifier object
clf = DecisionTreeClassifier()

# Train Decision Tree Classifier
clf = clf.fit(X_train_data, y_train)

# Predict the response for test dataset
y_pred = clf.predict(X_test_data)
28/25:
from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(1, -1))
X_test_data = scaler.transform(test_Y.values.reshape(1, -1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(1, -1))
Y_test_data = scaler.transform(test_Y.values.reshape(1, -1))

print(train_X, "\n", test_X)
print(X_train_data, "\n", X_test_data)
print(train_Y, "\n", test_Y)
print(Y_train_data, "\n", Y_test_data)
28/26:
from sklearn.preprocessing import StandardScaler

# Assuming train_X and test_X are NumPy arrays
scaler = StandardScaler()

# Fit the scaler on the training data
X_train_scaled = scaler.fit_transform(train_X)

# Use the same scaler to transform the test data
X_test_scaled = scaler.transform(test_X)

# Assuming train_Y and test_Y are pandas Series objects
# Reshape the data for StandardScaler
train_Y_reshaped = train_Y.values.reshape(-1, 1)
test_Y_reshaped = test_Y.values.reshape(-1, 1)

# Fit the scaler on the training labels
Y_train_scaled = scaler.fit_transform(train_Y_reshaped)

# Use the same scaler to transform the test labels
Y_test_scaled = scaler.transform(test_Y_reshaped)
28/27:
from sklearn.preprocessing import StandardScaler

train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
train_Y_scaled = scaler.fit_transform(train_Y.values.reshape(-1, 1))

# Transform the test set using the scaler fitted on the train set
test_Y_scaled = scaler.transform(test_Y.values.reshape(-1, 1))

print("Original data:")
print(train_Y, "\n", test_Y)
print("\nScaled data:")
print(train_Y_scaled, "\n", test_Y_scaled)
28/28:
from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

print(train_Y, "\n", test_Y)
print(train_Y_scaled, "\n", test_Y_scaled)

print(train_Y, "\n", test_Y)
print(train_Y_scaled, "\n", test_Y_scaled)
28/29:
from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

print(train_X, "\n", test_X)
print(X_train_data, "\n", X_test_data)
print(train_Y, "\n", test_Y)
print(Y_train_data, "\n", Y_test_data)
28/30:
from sklearn.tree import DecisionTreeRegressor
regressor = DecisionTreeRegressor()
regressor.fit(X_train_data, y_train)
28/31:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(train_X, y_train)
y_pred = dt_classifier.predict(test_X)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
report = classification_report(y_test, y_pred)
print(report)
28/32:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, y_train)
y_pred = dt_classifier.predict(X_test_data)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
report = classification_report(y_test, y_pred)
print(report)
28/33:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming y_train is a series of categorical labels
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, y_train)
y_pred = dt_classifier.predict(X_test_data)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
28/34:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Assuming y_train is a series of continuous values
dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
28/35:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming y_train is a series of categorical labels
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)
y_pred = dt_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
28/36:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming y_train is a series of categorical labels
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_set, y_train)
y_pred = dt_classifier.predict(X_test_set)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
28/37:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming y_train is a series of categorical labels
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, y_train)
y_pred = dt_classifier.predict(X_test_data)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
28/38:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming y_train is a series of categorical labels
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, Y_train_data)
y_pred = dt_classifier.predict(X_test_data)
accuracy = accuracy_score(Y_train_data, y_pred)
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
28/39:
from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

print("Train X and test X:", train_X, "\n", test_X)
print(X_train_data, "\n", X_test_data)
print("Train Y and test Y:", train_Y, "\n", test_Y)
print(Y_train_data, "\n", Y_test_data)
28/40:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming y_train is a series of categorical labels
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, Y_train_data)
y_pred = dt_classifier.predict(X_test_data)
accuracy = accuracy_score(Y_train_data, y_pred)
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
28/41:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming y_train is a series of categorical labels
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, Y_train_data)  # Corrected y_train here
y_pred = dt_classifier.predict(X_test_data)
accuracy = accuracy_score(Y_test_data, y_pred)  # Corrected y_test here
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
28/42:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming y_train is a series of categorical labels
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, y_train)  # Make sure y_train is used here
y_pred = dt_classifier.predict(X_test_data)
accuracy = accuracy_score(y_test, y_pred)  # Make sure y_test is used here
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
28/43:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming y_train is a series of categorical labels
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, y_train)  # Make sure y_train is used here
y_pred = dt_classifier.predict(X_test_data)
accuracy = accuracy_score(y_test, y_pred)  # Make sure y_test is used here
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
print(type(y_train))
print(type(y_test))
print(type(X_train))
print(type(X_test))
28/44:
print(type(y_train))
print(type(y_test))
print(type(X_train))
print(type(X_test))
28/45:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

y_train = y_train['Probability*']
y_test = y_test['Probability*']
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, y_train)  # Make sure y_train is used here
y_pred = dt_classifier.predict(X_test_data)
accuracy = accuracy_score(y_test, y_pred)  # Make sure y_test is used here
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
28/46:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
28/47:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
28/48:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
28/49:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
28/50:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
28/51:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
28/52:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
28/53:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
28/54:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
28/55:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
28/56:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
28/57:
from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

print("Train X and test X:", train_X, "\n", test_X)
print(X_train_data, "\n", X_test_data)
print("Train Y and test Y:", train_Y, "\n", test_Y)
print(Y_train_data, "\n", Y_test_data)
28/58:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

y_train = y_train['Probability*']
y_test = y_test['Probability*']
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, y_train)  # Make sure y_train is used here
y_pred = dt_classifier.predict(X_test_data)
accuracy = accuracy_score(y_test, y_pred)  # Make sure y_test is used here
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
28/59:
from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display("Train X and test X:", train_X, "\n", test_X)
display(X_train_data, "\n", X_test_data)
display("Train Y and test Y:", train_Y, "\n", test_Y)
display(Y_train_data, "\n", Y_test_data)
28/60:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

y_train = y_train['Probability*']
y_test = y_test['Probability*']
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, y_train)  # Make sure y_train is used here
y_pred = dt_classifier.predict(X_test_data)
accuracy = accuracy_score(y_test, y_pred)  # Make sure y_test is used here
print(f'Accuracy: {accuracy}')

report = classification_report(y_test, y_pred)
print(report)
28/61: print(X_train_data.columns)
28/62: print(type(X_train_data))
28/63:
print(type(X_train_data))
X_train_data[:, 0]
28/64:
print(type(X_train_data))
X_train_data[:, 0]
print(type(y_train))
28/65:
from sklearn.tree import DecisionTreeClassifier

dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_data, y_train)
28/66:
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(dt_regressor, X_train_data, y_train, cv=5, scoring='neg_mean_squared_error')
mse_cv = -cv_scores.mean()
28/67:
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(dt_regressor, X_train_data, y_train, cv=5, scoring='neg_mean_squared_error')
mse_cv = -cv_scores.mean()
print(mse_cv)
28/68:
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(dt_regressor, X_train_data, y_train, cv=5, scoring='neg_mean_squared_error')
mse_cv = -cv_scores.mean()
print(cv_scores)
28/69:
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plot_tree(dt_regressor, filled=True, rounded=True, feature_names=X_train_data.columns)
plt.show()
28/70:
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plot_tree(dt_regressor, filled=True, rounded=True, feature_names=X_train.columns)
plt.show()
28/71:
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plot_tree(dt_regressor, filled=True, rounded=True, feature_names=X_train.columns.tolist())
plt.show()
28/72:
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(dt_regressor, param_grid, cv=5)
grid_search.fit(X_train_data, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
28/73:
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(dt_regressor, param_grid, cv=5)
grid_search.fit(X_train_data, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
print(best_params)
print(best_model)
28/74:
y_pred_test = best_model.predict(X_test_data)
mse_test = mean_squared_error(y_test, y_pred_test)
print(f'Final Mean Squared Error on Test Data: {mse_test}')
28/75:
# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Random Forest Regressor
rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train, y_train)
gb_predictions = gb_model.predict(X_test)
gb_mse = mean_squared_error(y_test, gb_predictions)

# Support Vector Machine Regressor
svm_model = SVR()
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)
svm_mse = mean_squared_error(y_test, svm_predictions)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
print(f"SVM MSE: {svm_mse}")
28/76:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error


dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_set, y_train)
y_pred = dt_regressor.predict(X_test_set)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
28/77:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error


dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
29/1:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
29/2:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
29/3:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
29/4:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
29/5:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
29/6:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
29/7:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
29/8:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
29/9:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
29/10:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
29/11:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
29/12:
from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display("Train X and test X:", train_X, "\n", test_X)
display(X_train_data, "\n", X_test_data)
display("Train Y and test Y:", train_Y, "\n", test_Y)
display(Y_train_data, "\n", Y_test_data)
29/13:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error


dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
29/14:
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plot_tree(dt_regressor, filled=True, rounded=True, feature_names=X_train.columns.tolist())
plt.show()
29/15:
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(dt_regressor, param_grid, cv=5)
grid_search.fit(X_train_data, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
print(best_params)
print(best_model)
29/16:
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(dt_regressor, X_train_data, y_train, cv=5, scoring='neg_mean_squared_error')
mse_cv = -cv_scores.mean()
print(cv_scores)
29/17:
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(dt_regressor, param_grid, cv=5)
grid_search.fit(X_train_data, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
print(best_params)
print(best_model)
29/18:
# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Random Forest Regressor
rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train, y_train)
gb_predictions = gb_model.predict(X_test)
gb_mse = mean_squared_error(y_test, gb_predictions)

# Support Vector Machine Regressor
svm_model = SVR()
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)
svm_mse = mean_squared_error(y_test, svm_predictions)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
print(f"SVM MSE: {svm_mse}")
29/19:
# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Random Forest Regressor
rf_model = RandomForestRegressor()
rf_model.fit(X_train_data, y_train)
rf_predictions = rf_model.predict(X_test_data)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train_data, y_train)
gb_predictions = gb_model.predict(X_test_data)
gb_mse = mean_squared_error(y_test, gb_predictions)

# Support Vector Machine Regressor
svm_model = SVR()
svm_model.fit(X_train_data, y_train)
svm_predictions = svm_model.predict(X_test_data)
svm_mse = mean_squared_error(y_test, svm_predictions)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
print(f"SVM MSE: {svm_mse}")
29/20:
# Checking for outliers
dataset.describe().T
29/21:
# Checking for outliers
import matplotlib.pyplot as plt

dataset.describe().T
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot()
29/22:
# Checking for outliers
import matplotlib.pyplot as plt

dataset.describe().T
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot()
Q1 = dataset.quantile(0.25)
Q3 = dataset.quantile(0.75)
IQR = Q3 - Q1
print(IQR)
29/23:
# Checking for outliers
import matplotlib.pyplot as plt

dataset.describe().T
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot()
z_scores = np.abs((dataset['Probability'] - dataset['Probability'].mean()) / dataset['Probability'].std())
29/24:
# Checking for outliers
import matplotlib.pyplot as plt

dataset.describe().T
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot()
z_scores = np.abs((dataset['Probability*'] - dataset['Probability*'].mean()) / dataset['Probability*'].std())
29/25:
# Checking for outliers
import matplotlib.pyplot as plt

dataset.describe().T
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot()
z_scores = np.abs((dataset['Probability*'] - dataset['Probability*'].mean()) / dataset['Probability*'].std())
threshold = 2
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
29/26:
# Checking for outliers
import matplotlib.pyplot as plt

dataset.describe().T
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot()
z_scores = np.abs((dataset['Probability*'] - dataset['Probability*'].mean()) / dataset['Probability*'].std())
threshold = 2
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
df_no_outliers = dataset[z_scores < threshold]
29/27:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot without outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers = dataset[z_scores < threshold]
dataset_no_outliers.boxplot(column='Probability*', ax=ax)

# Show the plot
plt.show()
29/28:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers
z_scores = np.abs((dataset['Probability*'] - dataset['Probability*'].mean()) / dataset['Probability*'].std())
threshold = 2
outliers = dataset[z_scores >= threshold]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers = dataset[z_scores < threshold]
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()

# Display outliers
print("Outliers:")
print(outliers)
29/29:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers and removing them
z_scores = np.abs((dataset['Probability*'] - dataset['Probability*'].mean()) / dataset['Probability*'].std())
threshold = 1  # Adjust the threshold as needed
dataset_no_outliers = dataset[z_scores < threshold]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()

# Display statistics after outlier removal
print("Statistics after outlier removal:")
print(dataset_no_outliers.describe())

# Display the removed outliers
outliers = dataset[z_scores >= threshold]
print("Removed Outliers:")
print(outliers)
29/30:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers and removing them
z_scores = np.abs((dataset['Probability*'] - dataset['Probability*'].mean()) / dataset['Probability*'].std())
threshold = 3  # Adjust the threshold as needed
dataset_no_outliers = dataset[z_scores < threshold]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()

# Display statistics after outlier removal
print("Statistics after outlier removal:")
print(dataset_no_outliers.describe())

# Display the removed outliers
outliers = dataset[z_scores >= threshold]
print("Removed Outliers:")
print(outliers)
29/31:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers and removing them
z_scores = np.abs((dataset['Probability*'] - dataset['Probability*'].mean()) / dataset['Probability*'].std())
threshold = 4  # Adjust the threshold as needed
dataset_no_outliers = dataset[z_scores < threshold]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()

# Display statistics after outlier removal
print("Statistics after outlier removal:")
print(dataset_no_outliers.describe())

# Display the removed outliers
outliers = dataset[z_scores >= threshold]
print("Removed Outliers:")
print(outliers)
29/32:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers and removing them
z_scores = np.abs((dataset['Probability*'] - dataset['Probability*'].mean()) / dataset['Probability*'].std())
threshold = 2  # Adjust the threshold as needed
dataset_no_outliers = dataset[z_scores < threshold]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()

# Display statistics after outlier removal
print("Statistics after outlier removal:")
print(dataset_no_outliers.describe())

# Display the removed outliers
outliers = dataset[z_scores >= threshold]
print("Removed Outliers:")
print(outliers)
29/33:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers and removing them
z_scores = np.abs((dataset['Probability*'] - dataset['Probability*'].mean()) / dataset['Probability*'].std())
threshold = 1  # Adjust the threshold as needed
dataset_no_outliers = dataset[z_scores < threshold]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()

# Display statistics after outlier removal
print("Statistics after outlier removal:")
print(dataset_no_outliers.describe())

# Display the removed outliers
outliers = dataset[z_scores >= threshold]
print("Removed Outliers:")
print(outliers)
29/34:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_no_outliers = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()

# Display statistics after outlier removal
print("Statistics after outlier removal:")
print(dataset_no_outliers.describe())

# Display the removed outliers
outliers = dataset[~((dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold))]
print("Removed Outliers:")
print(outliers)
29/35:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
29/36:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
29/37:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
29/38:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
29/39:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
29/40:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_no_outliers = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()

# Display statistics after outlier removal
print("Statistics after outlier removal:")
print(dataset_no_outliers.describe())

# Display the removed outliers
outliers = dataset[~((dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold))]
print("Removed Outliers:")
print(outliers)
29/41:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
29/42:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
29/43:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
29/44:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
29/45:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
29/46:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
29/47:
from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display("Train X and test X:", train_X, "\n", test_X)
display(X_train_data, "\n", X_test_data)
display("Train Y and test Y:", train_Y, "\n", test_Y)
display(Y_train_data, "\n", Y_test_data)
29/48:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error


dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
29/49:
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plot_tree(dt_regressor, filled=True, rounded=True, feature_names=X_train.columns.tolist())
plt.show()
29/50:
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(dt_regressor, param_grid, cv=5)
grid_search.fit(X_train_data, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
print(best_params)
print(best_model)
29/51:
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(dt_regressor, X_train_data, y_train, cv=5, scoring='neg_mean_squared_error')
mse_cv = -cv_scores.mean()
print(cv_scores)
29/52:
# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Random Forest Regressor
rf_model = RandomForestRegressor()
rf_model.fit(X_train_data, y_train)
rf_predictions = rf_model.predict(X_test_data)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train_data, y_train)
gb_predictions = gb_model.predict(X_test_data)
gb_mse = mean_squared_error(y_test, gb_predictions)

# Support Vector Machine Regressor
svm_model = SVR()
svm_model.fit(X_train_data, y_train)
svm_predictions = svm_model.predict(X_test_data)
svm_mse = mean_squared_error(y_test, svm_predictions)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
print(f"SVM MSE: {svm_mse}")
30/1:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
30/2:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
30/3:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
30/4:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
30/5:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
30/6:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_no_outliers = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()

# Display statistics after outlier removal
print("Statistics after outlier removal:")
print(dataset_no_outliers.describe())

# Display the removed outliers
outliers = dataset[~((dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold))]
print("Removed Outliers:")
print(outliers)
30/7:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
30/8:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
30/9:
import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
30/10:
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
30/11:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
30/12:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
30/13:
from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display("Train X and test X:", train_X, "\n", test_X)
display(X_train_data, "\n", X_test_data)
display("Train Y and test Y:", train_Y, "\n", test_Y)
display(Y_train_data, "\n", Y_test_data)
30/14:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error


dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
30/15:
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plot_tree(dt_regressor, filled=True, rounded=True, feature_names=X_train.columns.tolist())
plt.show()
30/16:
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(dt_regressor, param_grid, cv=5)
grid_search.fit(X_train_data, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
print(best_params)
print(best_model)
30/17:
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(dt_regressor, X_train_data, y_train, cv=5, scoring='neg_mean_squared_error')
mse_cv = -cv_scores.mean()
print(cv_scores)
30/18:
# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Random Forest Regressor
rf_model = RandomForestRegressor()
rf_model.fit(X_train_data, y_train)
rf_predictions = rf_model.predict(X_test_data)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train_data, y_train)
gb_predictions = gb_model.predict(X_test_data)
gb_mse = mean_squared_error(y_test, gb_predictions)

# Support Vector Machine Regressor
svm_model = SVR()
svm_model.fit(X_train_data, y_train)
svm_predictions = svm_model.predict(X_test_data)
svm_mse = mean_squared_error(y_test, svm_predictions)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
print(f"SVM MSE: {svm_mse}")
30/19:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_no_outliers = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
30/20:
# Scalimg for train and test set

from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display(X_train_data, "\n", X_test_data)
display(Y_train_data, "\n", Y_test_data)
30/21:
# Build a decision tree regressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error


dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
30/22:
# Visualizing decision tree regressor
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plot_tree(dt_regressor, filled=True, rounded=True, feature_names=X_train.columns.tolist())
plt.show()
30/23:
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Assuming dt_regressor is your trained decision tree regressor
# X_train and X_test are your feature sets for the train and test sets respectively

# Plotting for Train Set
plt.figure(figsize=(15, 10))
plot_tree(dt_regressor, filled=True, rounded=True, feature_names=X_train.columns.tolist())
plt.title("Decision Tree for Train Set")
plt.show()

# Plotting for Test Set
plt.figure(figsize=(15, 10))
plot_tree(dt_regressor, filled=True, rounded=True, feature_names=X_test.columns.tolist())
plt.title("Decision Tree for Test Set")
plt.show()
31/1:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
31/2:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
31/3:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
31/4:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
31/5:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_no_outliers = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
31/6:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
31/7:
# Label encoding
from sklearn.preprocessing import LabelEncoder

df1 = dataset['Target Class']  
label_encoder = LabelEncoder()
encoded_labels1 = label_encoder.fit_transform(df1)
print(encoded_labels1)
df2 = dataset['Target']  
label_encoder = LabelEncoder()
encoded_labels2 = label_encoder.fit_transform(df2)
print(encoded_labels2)
31/8:
# Visualizing target class dietribution

import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
31/9:
# Visualizing target class and Probability distribution
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
31/10:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
31/11:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
31/12:
# Scalimg for train and test set

from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display(X_train_data, "\n", X_test_data)
display(Y_train_data, "\n", Y_test_data)
31/13:
# Build a decision tree regressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

iris = dataset.load_iris()
X = iris.Taget
y = iris.target

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
31/14:
# Assuming you have your data in X and y
from sklearn.tree import DecisionTreeClassifier, export_text

X = dataset['Target Class']
y = dataset['Probability*']
# Create and train the classifier
clf = DecisionTreeClassifier(max_depth=3)  # You might want to limit the depth for visualization
clf.fit(X, y)

# Print textual representation of the tree
tree_rules = export_text(clf, feature_names=['Target Class', 'Target', 'Probability*'])
print(tree_rules)
31/15:
# Label encoding
from sklearn.preprocessing import LabelEncoder

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
31/16:
# Assuming you have your data in X and y
from sklearn.tree import DecisionTreeClassifier, export_text

X = df1
y = df2
# Create and train the classifier
clf = DecisionTreeClassifier(max_depth=3)  # You might want to limit the depth for visualization
clf.fit(X, y)

# Print textual representation of the tree
tree_rules = export_text(clf, feature_names=['Target Class', 'Target', 'Probability*'])
print(tree_rules)
31/17:
# Build a decision tree regressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

iris = dataset.load_iris()
X = iris.Taget
y = iris.target

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
31/18:
# Build a decision tree regressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
31/19:
# Assuming you have your data in X and y
from sklearn.tree import DecisionTreeClassifier, export_text

X = X_train_data, X_test_data
y = Y_train_data, Y_test_data
# Create and train the classifier
clf = DecisionTreeClassifier(max_depth=3)  # You might want to limit the depth for visualization
clf.fit(X, y)

# Print textual representation of the tree
tree_rules = export_text(clf, feature_names=['Target Class', 'Target', 'Probability*'])
print(tree_rules)
31/20:
# Assuming you have your data in X and y
from sklearn.tree import DecisionTreeClassifier, export_text

X = X_train_data, X_test_data
y = Y_train_data, Y_test_data

X_train_data, X_test_data, Y_train_data, Y_test_data = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the classifier
clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X_train_data, Y_train_data)

# Print textual representation of the tree
tree_rules = export_text(clf, feature_names=['Feature 1', 'Feature 2', 'Feature 3'])
print(tree_rules)
31/21:
# Assuming you have your data in X and y
from sklearn.tree import DecisionTreeClassifier, export_text

X = X_train_data, X_test_data
y = Y_train_data, Y_test_data

X_train_data, X_test_data, Y_train_data, Y_test_data = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the classifier
clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X_train_data, Y_train_data)

# Print textual representation of the tree
tree_rules = export_text(clf, feature_names=['Target Class', 'Target', 'Probability*'])
print(tree_rules)
31/22:
# Assuming you have your data in X and y
from sklearn.tree import DecisionTreeClassifier, export_text

X = X_train_data, X_test_data
y = Y_train_data, Y_test_data

X_train_data, X_test_data, Y_train_data, Y_test_data = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the classifier
clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X_train_data, Y_train_data)

# Print textual representation of the tree
tree_rules = export_text(clf, feature_names=['Target Class', 'Probability*'])
print(tree_rules)
31/23:
# Assuming you have your data in X and y
from sklearn.tree import DecisionTreeClassifier, export_text

X = X_train_data, X_test_data
y = Y_train_data, Y_test_data

X_train_data, X_test_data, Y_train_data, Y_test_data = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the classifier
clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X_train, y_train)

# Print textual representation of the tree
tree_rules = export_text(clf, feature_names=['Target Class', 'Probability*'])
print(tree_rules)
31/24:
# Assuming you have your data in X and y
from sklearn.tree import DecisionTreeClassifier, export_text

X = X_train_data, X_test_data
y = Y_train_data, Y_test_data

X_train_data, X_test_data, Y_train_data, Y_test_data = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the classifier
clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X_train_data, Y_train_data)

# Print textual representation of the tree
tree_rules = export_text(clf, feature_names=['Target Class', 'Probability*'])
print(tree_rules)
31/25:
# Import necessary libraries
from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import graphviz

# Create a Decision Tree Classifier
classifier = DecisionTreeClassifier(random_state=42)
classifier.fit(X_train_data, Y_train_data)

# Print text-based decision tree
tree_rules = export_text(classifier, feature_names=dataset.feature_names)
print(tree_rules)

# Visualize the tree
dot_data = export_graphviz(classifier, out_file=None, 
                           feature_names=dataset.feature_names,  
                           class_names=dataset.target_names,  
                           filled=True, rounded=True,  
                           special_characters=True)  
graph = graphviz.Source(dot_data)  
graph.render("iris_decision_tree", format="png")

# Display the tree
graph
31/26: print(X_train_data.shape, Y_train_data.shape)
31/27:
import numpy as np

# Convert lists to numpy arrays
X_train_data = np.array(X_train_data)
Y_train_data = np.array(Y_train_data)

# Check the shape
print(X_train_data.shape, Y_train_data.shape)
31/28:
# Convert to numpy arrays
X_train_data = np.array(X_train_data)
Y_train_data = np.array(Y_train_data)

# Check the shape
print(X_train_data.shape, Y_train_data.shape)
31/29:
# Import necessary libraries
import numpy as np
from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import graphviz

# Convert to numpy arrays
X_train_data = np.array(X_train_data)
Y_train_data = np.array(Y_train_data)

# Check the shape
print(X_train_data.shape, Y_train_data.shape)

# Create a Decision Tree Classifier
classifier = DecisionTreeClassifier(random_state=42)
classifier.fit(X_train_data, Y_train_data)

# Print text-based decision tree
tree_rules = export_text(classifier, feature_names=dataset.feature_names)
print(tree_rules)

# Visualize the tree
dot_data = export_graphviz(classifier, out_file=None, 
                           feature_names=dataset.feature_names,  
                           class_names=dataset.target_names,  
                           filled=True, rounded=True,  
                           special_characters=True)  
graph = graphviz.Source(dot_data)  
graph.render("iris_decision_tree", format="png")

# Display the tree
graph
31/30:
print(X_train_data[:5])  # Print the first 5 samples of X_train_data
print(Y_train_data[:5])  # Print the first 5 samples of Y_train_data
31/31:
import numpy as np

# Assuming your data is organized like this

target_class_labels = np.array(df1)
numerical_features = np.array(scaled_probs)

# Check shapes
print(target_class_labels.shape, numerical_features.shape)
31/32:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
from sklearn.tree import DecisionTreeClassifier

target_class_labels = np.array(X_train_set)
numerical_features = np.array(Y_train_data)

# Check shapes
print(target_class_labels.shape, numerical_features.shape)

# Create a Decision Tree Classifier
tree_classifier = DecisionTreeClassifier(max_depth=3, random_state=42)
tree_classifier.fit(X_train_set, Y_train_data)

# Visualize the tree
plt.figure(figsize=(15, 10))
plot_tree(tree_classifier, filled=True, feature_names=X_train_set.columns, class_names=label_encoder.classes_)
plt.show()
31/33:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
from sklearn.tree import DecisionTreeClassifier

target_class_labels = np.array(X_train_data)
numerical_features = np.array(Y_train_data)

# Check shapes
print(target_class_labels.shape, numerical_features.shape)

# Create a Decision Tree Classifier
tree_classifier = DecisionTreeClassifier(max_depth=3, random_state=42)
tree_classifier.fit(X_train_data, Y_train_data)

# Visualize the tree
plt.figure(figsize=(15, 10))
plot_tree(tree_classifier, filled=True, feature_names=X_train_data.columns, class_names=label_encoder.classes_)
plt.show()
31/34:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
from sklearn.tree import DecisionTreeClassifier

# Assuming X_train_data and Y_train_data are DataFrames
X_train_data = X_train_data.values
Y_train_data = Y_train_data.values

# Check shapes
print(X_train_data.shape, Y_train_data.shape)

# Create a Decision Tree Classifier
tree_classifier = DecisionTreeClassifier(max_depth=3, random_state=42)
tree_classifier.fit(X_train_data, Y_train_data)

# Visualize the tree
plt.figure(figsize=(15, 10))
plot_tree(tree_classifier, filled=True, feature_names=X_train_data.columns, class_names=label_encoder.classes_)
plt.show()
31/35:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
from sklearn.tree import DecisionTreeClassifier

# Assuming X_train_data and Y_train_data are numpy arrays
# Check shapes
print(X_train_data.shape, Y_train_data.shape)

# Create a Decision Tree Classifier
tree_classifier = DecisionTreeClassifier(max_depth=3, random_state=42)
tree_classifier.fit(X_train_data, Y_train_data)

# Visualize the tree
plt.figure(figsize=(15, 10))
plot_tree(tree_classifier, filled=True, feature_names=X_train_data.columns, class_names=label_encoder.classes_)
plt.show()
31/36:
import dask.dataframe as dd
import pandas as pd

# Define your file path (assuming it's a CSV file)
file_path = (r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# Read a large CSV file using Dask
ddf = dd.read_csv(file_path)

# Perform operations on the Dask DataFrame
# For example, you can filter rows based on a condition
filtered_df = ddf[ddf['Target Class'] > 100]

# Compute the result as a Pandas DataFrame
result_df = filtered_df.compute()

# Now you can work with the result_df using Pandas operations
# For example, you can save it to a new CSV file
result_df.to_csv('filtered_dataset.csv', index=False)
31/37:
import dask.dataframe as dd
import pandas as pd

# Define your file path (assuming it's a CSV file)
file_path = (r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# Read a large CSV file using Dask
ddf = dd.read_csv(file_path)

# Perform operations on the Dask DataFrame
# For example, you can filter rows based on a condition
filtered_df = ddf[ddf['Target Class'] > 100]

# Compute the result as a Pandas DataFrame
result_df = filtered_df.compute()

# Now you can work with the result_df using Pandas operations
# For example, you can save it to a new CSV file
result_df.to_csv('filtered_dataset.csv', index=False)
31/38:
import dask.dataframe as dd

# Define your file path (assuming it's a CSV file)
file_path = r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv"

# Read a large CSV file using Dask
ddf = dd.read_csv(file_path)

# Convert 'Target Class' column to numeric (assuming it contains numbers)
ddf['Target Class'] = ddf['Target Class'].astype(float)

# Perform operations on the Dask DataFrame
# For example, you can filter rows based on a condition
filtered_df = ddf[ddf['Target Class'] > 100]

# Compute the result as a Pandas DataFrame
result_df = filtered_df.compute()

# Now you can work with the result_df using Pandas operations
# For example, you can save it to a new CSV file
result_df.to_csv('filtered_dataset.csv', index=False)
31/39:
import dask.dataframe as dd

# Define your file path (assuming it's a CSV file)
file_path = r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv"

# Read a large CSV file using Dask
ddf = dd.read_csv(file_path)

# Remove rows with non-numeric values in 'Target Class' column
ddf = ddf[ddf['Target Class'].str.isnumeric()]

# Continue with your operations
filtered_df = ddf[ddf['Target Class'] > 100]
31/40:
import dask.dataframe as dd

# Define your file path (assuming it's a CSV file)
file_path = r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv"

# Read a large CSV file using Dask
ddf = dd.read_csv(file_path)

# Remove rows with non-numeric values in 'Target Class' column
ddf = ddf[ddf['Target Class'].str.isnumeric()]

# Continue with your operations
filtered_df = ddf[ddf['Target Class'] > 100]

# Convert 'Target Class' column to numeric, converting non-numeric values to NaN
ddf['Target Class'] = pd.to_numeric(ddf['Target Class'], errors='coerce')

# Continue with your operations
filtered_df = ddf[ddf['Target Class'] > 100]
31/41:
import dask.dataframe as dd

# Define your file path (assuming it's a CSV file)
file_path = r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv"

# Read a large CSV file using Dask
ddf = dd.read_csv(file_path)

# Remove rows with non-numeric values in 'Target Class' column
ddf = ddf[ddf['Target Class'].str.isnumeric()]

# Continue with your operations
filtered_df = ddf[ddf['Target Class'] > 100]

ddf['Target Class'] = pd.to_numeric(ddf['Target Class'], errors='coerce')

filtered_df = ddf[ddf['Target Class'].gt(100, fill_value=0)]
31/42:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
31/43:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay

rf = RandomForestClassifier()
rf.fit(X_train_data, Y_train_data)
31/44:
print(f"X_train_data shape: {X_train_data.shape}")
print(f"Y_train_data shape: {Y_train_data.shape}")
31/45:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
31/46:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_data, Y_train_data)
31/47:
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

svm_model = SVR()
svm_model.fit(X_train_data, y_train)
svm_predictions = svm_model.predict(X_test_data)
svm_mse = mean_squared_error(y_test, svm_predictions)
31/48:
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

svm_model = SVR()
svm_model.fit(X_train_data, y_train)
svm_predictions = svm_model.predict(X_test_data)
svm_mse = mean_squared_error(y_test, svm_predictions)
print(f"SVM MSE: {svm_mse}")
31/49:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
31/50:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
31/51:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
31/52:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
31/53:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_no_outliers = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
31/54:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
31/55:
# Label encoding
from sklearn.preprocessing import LabelEncoder

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
31/56:
# Visualizing target class dietribution

import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
31/57:
# Visualizing target class and Probability distribution
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
31/58:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
31/59:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
31/60:
# Scalimg for train and test set

from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display(X_train_data, "\n", X_test_data)
display(Y_train_data, "\n", Y_test_data)
31/61:
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

svm_model = SVR()
svm_model.fit(X_train_data, y_train)
svm_predictions = svm_model.predict(X_test_data)
svm_mse = mean_squared_error(y_test, svm_predictions)
print(f"SVM MSE: {svm_mse}")
31/62:
# Build a decision tree regressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
31/63:
# Visualizing decision tree regressor
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plot_tree(dt_regressor, filled=True, rounded=True, feature_names=X_train.columns.tolist())
plt.show()
31/64:
# Hyperparameter tuning
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(dt_regressor, param_grid, cv=5)
grid_search.fit(X_train_data, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
print(best_params)
print(best_model)
31/65:
# Cross validation method
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(dt_regressor, X_train_data, y_train, cv=5, scoring='neg_mean_squared_error')
mse_cv = -cv_scores.mean()
print(cv_scores)
31/66:
# Checking with other models
# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor


# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Random Forest Regressor
rf_model = RandomForestRegressor()
rf_model.fit(X_train_data, y_train)
rf_predictions = rf_model.predict(X_test_data)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train_data, y_train)
gb_predictions = gb_model.predict(X_test_data)
gb_mse = mean_squared_error(y_test, gb_predictions)

svm_model = SVR()
svm_model.fit(X_train_data, y_train)
svm_predictions = svm_model.predict(X_test_data)
svm_mse = mean_squared_error(y_test, svm_predictions)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
31/67:
print(X_train_data[:5])  # Print the first 5 samples of X_train_data
print(Y_train_data[:5])  # Print the first 5 samples of Y_train_data
31/68:
import numpy as np

# Convert lists to numpy arrays
X_train_data = np.array(X_train_data)
Y_train_data = np.array(Y_train_data)

# Check the shape
print(X_train_data.shape, Y_train_data.shape)
31/69:
# Load dataset (replace this with your own dataset)
data = load_iris(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
X = data.data
y = data.target
31/70:
# Checking with other models
# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Random Forest Regressor
rf_model = RandomForestRegressor()
rf_model.fit(X_train_data, y_train)
rf_predictions = rf_model.predict(X_test_data)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train_data, y_train)
gb_predictions = gb_model.predict(X_test_data)
gb_mse = mean_squared_error(y_test, gb_predictions)

# Build a decision tree regressor
dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
print(f'Decision tree MSE: {mse}')
31/71:
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svr_regressor = SVR()
svr_regressor.fit(X_train, y_train)
svr_pred = svr_regressor.predict(X_test)
mse_svr = mean_squared_error(y_test, svr_pred)
print(f"Support Vector Machine MSE: {mse_svr}")
31/72:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
31/73:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
31/74:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
31/75:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
31/76:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_no_outliers = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
31/77:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
31/78:
# Label encoding
from sklearn.preprocessing import LabelEncoder

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
31/79:
# Visualizing target class dietribution

import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
31/80:
# Visualizing target class and Probability distribution
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
31/81:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
31/82:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
31/83:
# Scalimg for train and test set

from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display(X_train_data, "\n", X_test_data)
display(Y_train_data, "\n", Y_test_data)
31/84:
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svr_regressor = SVR()
svr_regressor.fit(X_train, y_train)
svr_pred = svr_regressor.predict(X_test)
mse_svr = mean_squared_error(y_test, svr_pred)
print(f"Support Vector Machine MSE: {mse_svr}")
31/85:
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svr_regressor = SVR()
svr_regressor.fit(X_train, y_train)
svr_pred = svr_regressor.predict(X_test)
mse_svr = mean_squared_error(y_test, svr_pred)
print(f"Support Vector Machine MSE: {mse_svr}")
31/86:
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svr_regressor = SVR()
svr_regressor.fit(X_train_data, Y_train_data)
svr_pred = svr_regressor.predict(X_test_data)
mse_svr = mean_squared_error(Y_test_data, svr_pred)
print(f"Support Vector Machine MSE: {mse_svr}")
31/87:
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svr_regressor = SVR()
svr_regressor.fit(X_train_data, y_train.values.ravel())  # Convert to 1D array
svr_pred = svr_regressor.predict(X_test_data)
mse_svr = mean_squared_error(y_test, svr_pred)
print(f"Support Vector Machine MSE: {mse_svr}")
31/88:
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svr_regressor = SVR()
svr_regressor.fit(X_train_data, y_train.values.ravel())
svr_pred = svr_regressor.predict(X_test_data)
mse_svr = mean_squared_error(y_test, svr_pred)
print(f"Support Vector Machine MSE: {mse_svr}")
31/89:
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, svr_pred))
31/90:
plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)

# plot the decision function
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# create grid to evaluate model
xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = clf.decision_function(xy).reshape(XX.shape)

# plot decision boundary and margins
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
           linestyles=['--', '-', '--'])
31/91:
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto'],  # Only relevant for RBF kernel
    'degree': [2, 3, 4]  # Only relevant for polynomial kernel
}

# Create an SVR instance
svr = SVR()

# Instantiate GridSearchCV
grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)

# Fit the grid search to the data
grid_search.fit(X_train_data, y_train.values.ravel())

# Get the best parameters and best estimator
best_params = grid_search.best_params_
best_svr = grid_search.best_estimator_
31/92:
from sklearn.ensemble import RandomForestRegressor
from sklearn import tree
import matplotlib.pyplot as plt

# Assuming 'fn' is a list of feature names and 'cn' is a list of target class names
# You also have a trained Random Forest model 'rf'

# Define a sample data point for visualization
sample_data = df['Target Class', 'Target', 'Probability*']

# Get the predicted class and probability
predicted_class = rf.predict(sample_data)[0]
predicted_proba = rf.predict_proba(sample_data)[0]

# Get the feature importance for the selected tree
feature_importance = rf.estimators_[0].feature_importances_

# Visualize the decision tree
fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(4, 4), dpi=800)
tree.plot_tree(rf.estimators_[0],
               feature_names=df['Target Class', 'Target'],
               class_names=['Probability*'],
               filled=True);

# Display feature names, predicted class, and probabilities
print(f"Predicted Class: {cn[predicted_class]}")
for i, (feature, importance) in enumerate(zip(fn, feature_importance)):
    print(f"Feature: {feature}, Importance: {importance:.4f}")
print(f"Predicted Probabilities: {predicted_proba}")
31/93:
# Load packages
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

# Export the first three decision trees from the forest

for i in range(3):
    tree = rf.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=X_train.columns,  
                               filled=True,  
                               max_depth=2, 
                               impurity=False, 
                               proportion=True)
31/94:
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

# Assuming X_train and y_train are your training data and labels
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)

# Export the first three decision trees from the forest
for i in range(3):
    tree = rf.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=X_train.columns,  
                               filled=True,  
                               max_depth=2, 
                               impurity=False, 
                               proportion=True)
    # Render the tree as a graph
    graph = graphviz.Source(dot_data)
    
    # Save the graph as an image file (optional)
    graph.render(f'tree_{i+1}')
    
    # Display the graph
    display(graph)
31/95:
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

# Assuming X_train and y_train are your training data and labels
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train_data, y_train)

# Export the first three decision trees from the forest
for i in range(3):
    tree = rf.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=X_train_data.columns,  
                               filled=True,  
                               max_depth=2, 
                               impurity=False, 
                               proportion=True)
    # Render the tree as a graph
    graph = graphviz.Source(dot_data)
    
    # Save the graph as an image file (optional)
    graph.render(f'tree_{i+1}')
    
    # Display the graph
    display(graph)
31/96:
from sklearn.ensemble import RandomForestRegressor

# Assuming X_train, y_train, X_test, y_test are defined for regression
rf_regressor = RandomForestRegressor(n_estimators=100)
rf_regressor.fit(X_train_data, Y_train_data)
31/97:
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

# Assuming X_train and y_train are your training data and labels
rf_regressor = RandomForestRegressor(n_estimators=100)
rf_regressor.fit(X_train_data, Y_train_data)

# Export the first three decision trees from the forest
for i in range(3):
    tree = rf.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=X_train_data.columns,  
                               filled=True,  
                               max_depth=2, 
                               impurity=False, 
                               proportion=True)
    # Render the tree as a graph
    graph = graphviz.Source(dot_data)
    
    # Save the graph as an image file (optional)
    graph.render(f'tree_{i+1}')
    
    # Display the graph
    display(graph)
31/98:
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

# Assuming X_train_data and Y_train_data are your training data and labels for regression
rf_regressor = RandomForestRegressor(n_estimators=100)
rf_regressor.fit(X_train_data, Y_train_data)

# Export the first three decision trees from the forest
for i in range(3):
    tree = rf_regressor.estimators_[i]  # Use rf_regressor instead of rf
    dot_data = export_graphviz(tree,
                               feature_names=X_train_data.columns,  
                               filled=True,  
                               max_depth=2, 
                               impurity=False, 
                               proportion=True)
    # Render the tree as a graph
    graph = graphviz.Source(dot_data)
    
    # Save the graph as an image file (optional)
    graph.render(f'tree_{i+1}')
    
    # Display the graph
    display(graph)
31/99:
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

# Assuming X_train_data and Y_train_data are your training data and labels for regression
rf_regressor = RandomForestRegressor(n_estimators=100)
rf_regressor.fit(X_train_data, Y_train_data)

# Export the first three decision trees from the forest
for i in range(3):
    tree = rf_regressor.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=None,  # No need to specify for numpy arrays
                               filled=True,  
                               max_depth=2, 
                               impurity=False, 
                               proportion=True)
    # Render the tree as a graph
    graph = graphviz.Source(dot_data)
    
    # Save the graph as an image file (optional)
    graph.render(f'tree_{i+1}')
    
    # Display the graph
    display(graph)
31/100:
!pip install pydotplus
from sklearn.tree import export_graphviz
from IPython.display import Image
import pydotplus

for i in range(3):
    tree = rf_regressor.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=None,
                               filled=True,
                               max_depth=2,
                               impurity=False,
                               proportion=True)
    graph = pydotplus.graph_from_dot(dot_data)
    Image(graph.create_png())
31/101:
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz
import pydotplus

# Assuming X_train_data and Y_train_data are your training data and labels for regression
rf_regressor = RandomForestRegressor(n_estimators=100)
rf_regressor.fit(X_train_data, Y_train_data)

# Export the first three decision trees from the forest
for i in range(3):
    tree = rf_regressor.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=None,  # No need to specify for numpy arrays
                               filled=True,  
                               max_depth=2, 
                               impurity=False, 
                               proportion=True)
    # Render the tree as a graph
    graph = graphviz.Source(dot_data)
    
    # Save the graph as an image file (optional)
    graph.render(f'tree_{i+1}')
    
    # Display the graph
    display(graph)
31/102:
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import export_graphviz
import graphviz

# Assuming X_train_data and Y_train_data are your training data and labels for regression
rf_regressor = RandomForestRegressor(n_estimators=100)
rf_regressor.fit(X_train_data, Y_train_data)

# Export the first three decision trees from the forest
for i in range(3):
    tree = rf_regressor.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=None,  # No need to specify for numpy arrays
                               filled=True,  
                               max_depth=2, 
                               impurity=False, 
                               proportion=True)
    # Render the tree as a graph
    graph = graphviz.Source(dot_data)
    
    # Save the graph as an image file (optional)
    graph.render(f'tree_{i+1}')
    
    # Display the graph
    display(graph)
31/103:
from sklearn.tree import export_graphviz
from IPython.display import Image
import pydotplus

# Assuming rf_regressor is your RandomForestRegressor
for i in range(3):  # Visualize the first three trees
    tree = rf_regressor.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=['Target Class', 'Target'],  # Feature names
                               filled=True,  
                               max_depth=2, 
                               impurity=False, 
                               proportion=True,
                               special_characters=True)
    
    graph = pydotplus.graph_from_dot(dot_data)
    Image(graph.create_png())
31/104:
from sklearn.tree import export_graphviz
from IPython.display import Image
import pydotplus

# Assuming rf_regressor is your RandomForestRegressor
for i in range(3):  # Visualize the first three trees
    tree = rf_regressor.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=['Target'],  # Feature names
                               filled=True,  
                               max_depth=2, 
                               impurity=False, 
                               proportion=True,
                               special_characters=True)
    
    graph = pydotplus.graph_from_dot(dot_data)
    Image(graph.create_png())
31/105:
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

# Assuming rf_regressor is your RandomForestRegressor
for i in range(3):  # Visualize the first three trees
    tree = rf_regressor.estimators_[i]
    dot_data = export_graphviz(tree,
                               feature_names=['Target'],  # Feature names
                               filled=True,  
                               max_depth=2, 
                               impurity=False, 
                               proportion=True,
                               special_characters=True)
    
    graph = graphviz.Source(dot_data)
    graph.render(filename=f'tree_{i+1}')  # Save the graph as an image file (optional)
    display(Image(graph.render(format='png')))  # Display the graph
31/106:
class CustomTreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Usage example
root_node = CustomTreeNode("Class A", "Target X", 0.8)
root_node.add_child(CustomTreeNode("Class B", "Target Y", 0.6))
root_node.add_child(CustomTreeNode("Class C", "Target Z", 0.9))

# Accessing information
print(root_node.target_class)  # Output: Class A
print(root_node.target)        # Output: Target X
print(root_node.probability)   # Output: 0.8
print(len(root_node.children)) # Output: 2
31/107:
# Assuming you have a list of dictionaries where each dictionary represents a data point

# Create a tree-like data structure using a dictionary
tree_data = {}

for data_point in dataset:
    target_class = dataset['Target Class']
    target = dataset['Target']
    probability = dataset['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)
31/108:
# Assuming you have a pandas DataFrame named dataset

# Create a tree-like data structure using a dictionary
tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)
31/109:
# Assuming you have a pandas DataFrame named dataset

# Create a tree-like data structure using a dictionary
tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)

from graphviz import Digraph

# Define the TreeNode class
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Define the print_tree function
def print_tree(node, graph):
    if node is not None:
        label = f'{node.target_class} -> {node.target} ({node.probability})'
        graph.node(label)
        for child in node.children:
            child_label = f'{child.target_class} -> {child.target} ({child.probability})'
            graph.edge(label, child_label)
            print_tree(child, graph)

# Assuming you have constructed the tree_data dictionary
# Populate the tree with data from tree_data
root_node = TreeNode()
for target_class, targets in tree_data.items():
    for target, probabilities in targets.items():
        for probability in probabilities:
            root_node.add_child(TreeNode(target_class, target, probability))

# Initialize the graph
dot = Digraph(comment='The Tree')

# Call the print_tree function
print_tree(root_node, dot)

# Render the graph to a file (e.g., PNG, PDF, etc.)
dot.render('tree', format='png')
31/110:
from graphviz import Digraph

# Create a tree-like data structure using a dictionary
tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)


# Define the TreeNode class
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Define the print_tree function
def print_tree(node, graph):
    if node is not None:
        label = f'{node.target_class} -> {node.target} ({node.probability})'
        graph.node(label)
        for child in node.children:
            child_label = f'{child.target_class} -> {child.target} ({child.probability})'
            graph.edge(label, child_label)
            print_tree(child, graph)

# Assuming you have constructed the tree_data dictionary
# Populate the tree with data from tree_data
root_node = TreeNode()
for target_class, targets in tree_data.items():
    for target, probabilities in targets.items():
        for probability in probabilities:
            root_node.add_child(TreeNode(target_class, target, probability))

# Initialize the graph
dot = Digraph(comment='The Tree')

# Call the print_tree function
print_tree(root_node, dot)

# Render the graph to a file (e.g., PNG, PDF, etc.)
dot.render('tree', format='png')
31/111:
from graphviz import Digraph


# Add this line to specify the path
dot = Digraph(executable='c:\users\sound\appdata\local\packages\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\localcache\local-packages\python311\site-packages (0.20.1)')

# Create a tree-like data structure using a dictionary
tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)


# Define the TreeNode class
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Define the print_tree function
def print_tree(node, graph):
    if node is not None:
        label = f'{node.target_class} -> {node.target} ({node.probability})'
        graph.node(label)
        for child in node.children:
            child_label = f'{child.target_class} -> {child.target} ({child.probability})'
            graph.edge(label, child_label)
            print_tree(child, graph)

# Assuming you have constructed the tree_data dictionary
# Populate the tree with data from tree_data
root_node = TreeNode()
for target_class, targets in tree_data.items():
    for target, probabilities in targets.items():
        for probability in probabilities:
            root_node.add_child(TreeNode(target_class, target, probability))

# Initialize the graph
dot = Digraph(comment='The Tree')

# Call the print_tree function
print_tree(root_node, dot)

# Render the graph to a file (e.g., PNG, PDF, etc.)
dot.render('tree', format='png')
31/112:
from graphviz import Digraph


# Add this line to specify the path
dot = Digraph(executable=r'C:\users\sound\appdata\local\packages\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\localcache\local-packages\python311\site-packages (0.20.1)')

# Create a tree-like data structure using a dictionary
tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)


# Define the TreeNode class
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Define the print_tree function
def print_tree(node, graph):
    if node is not None:
        label = f'{node.target_class} -> {node.target} ({node.probability})'
        graph.node(label)
        for child in node.children:
            child_label = f'{child.target_class} -> {child.target} ({child.probability})'
            graph.edge(label, child_label)
            print_tree(child, graph)

# Assuming you have constructed the tree_data dictionary
# Populate the tree with data from tree_data
root_node = TreeNode()
for target_class, targets in tree_data.items():
    for target, probabilities in targets.items():
        for probability in probabilities:
            root_node.add_child(TreeNode(target_class, target, probability))

# Initialize the graph
dot = Digraph(comment='The Tree')

# Call the print_tree function
print_tree(root_node, dot)

# Render the graph to a file (e.g., PNG, PDF, etc.)
dot.render('tree', format='png')
31/113:
from graphviz import Digraph

# Create a tree-like data structure using a dictionary
tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)
31/114:
from graphviz import Digraph

# Create a tree-like data structure using a dictionary
tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)
display(tree_data)
31/115:
import matplotlib.pyplot as plt

def visualize_tree(node, level=0):
    if node is not None:
        print('    ' * level + f'{node.target_class} -> {node.target} ({node.probability})')
        for child in node.children:
            visualize_tree(child, level + 1)

# Assuming you have constructed the tree_data dictionary
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Populate the tree with data from tree_data
root_node = TreeNode()
for target_class, targets in tree_data.items():
    for target, probabilities in targets.items():
        for probability in probabilities:
            root_node.add_child(TreeNode(target_class, target, probability))

# Visualize the tree
visualize_tree(root_node)

# Display the plot (you may need to adjust figsize and layout depending on your tree size)
plt.figure(figsize=(10, 10))
plt.axis('off')  # Turn off axis labels and ticks
plt.show()
31/116:
import matplotlib.pyplot as plt

def visualize_tree(node, level=0):
    if node is not None:
        print('    ' * level + f'{node.target_class} -> {node.target} ({node.probability})')
        for child in node.children:
            visualize_tree(child, level + 1)

# Assuming you have constructed the tree_data dictionary
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Populate the tree with data from tree_data
root_node = TreeNode()
for target_class, targets in tree_data.items():
    for target, probabilities in targets.items():
        for probability in probabilities:
            root_node.add_child(TreeNode(target_class, target, probability))

# Visualize the tree
visualize_tree(root_node)

# Display the plot (you may need to adjust figsize and layout depending on your tree size)
plt.figure(figsize=(10, 10))
plt.axis('off')  # Turn off axis labels and ticks
plt.show()
31/117:
import matplotlib.pyplot as plt

def visualize_tree(node, level=0):
    if node is not None:
        print('    ' * level + f'{node.target_class} -> {node.target} ({node.probability})')
        for child in node.children:
            visualize_tree(child, level + 1)

# Assuming you have constructed the tree_data dictionary
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Populate the tree with data from tree_data
root_node = TreeNode()
for target_class, targets in tree_data.items():
    for target, probabilities in targets.items():
        for probability in probabilities:
            root_node.add_child(TreeNode(target_class, target, probability))

# Visualize the tree
visualize_tree(root_node)

# Display the plot (you may need to adjust figsize and layout depending on your tree size)
plt.figure(figsize=(20, 30))
plt.show()
31/118:
import matplotlib.pyplot as plt

def visualize_tree(node, level=0):
    if node is not None:
        print('    ' * level + f'{node.target_class} -> {node.target} ({node.probability})')
        for child in node.children:
            visualize_tree(child, level + 1)

# Assuming you have constructed the tree_data dictionary
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Populate the tree with data from tree_data
root_node = TreeNode()
for target_class, targets in tree_data.items():
    for target, probabilities in targets.items():
        for probability in probabilities:
            root_node.add_child(TreeNode(target_class, target, probability))

# Visualize the tree
visualize_tree(root_node)

# Display the plot (you may need to adjust figsize and layout depending on your tree size)
plt.figure(figsize=(20, 20))
plt.show()
31/119:
import matplotlib.pyplot as plt

def visualize_tree(node, level=0):
    if node is not None:
        print('    ' * level + f'{node.target_class} -> {node.target} ({node.probability})')
        for child in node.children:
            visualize_tree(child, level + 1)

# Assuming you have constructed the tree_data dictionary
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Populate the tree with data from tree_data
root_node = TreeNode()
for target_class, targets in tree_data.items():
    for target, probabilities in targets.items():
        for probability in probabilities:
            root_node.add_child(TreeNode(target_class, target, probability))

# Visualize the tree
visualize_tree(root_node)

# Display the plot (you may need to adjust figsize and layout depending on your tree size)
plt.figure(figsize=(20, 10))
plt.show()
31/120:
from anytree import Node, RenderTree

# Define the TreeNode class
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Populate the tree with data from tree_data
root_node = TreeNode()
for target_class, targets in tree_data.items():
    for target, probabilities in targets.items():
        for probability in probabilities:
            root_node.add_child(TreeNode(target_class, target, probability))

# Function to create anytree nodes
def create_node(tree_node, parent=None):
    node = Node(f"{tree_node.target_class} -> {tree_node.target} ({tree_node.probability})", parent=parent)
    for child in tree_node.children:
        create_node(child, parent=node)

# Create the anytree structure
create_node(root_node)

# Print the tree
for pre, _, node in RenderTree(Node(f"Root")):
    print(f"{pre}{node.name}")
31/121:
import networkx as nx
import matplotlib.pyplot as plt

# Define the TreeNode class
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Populate the tree with data from tree_data
root_node = TreeNode()
for target_class, targets in tree_data.items():
    for target, probabilities in targets.items():
        for probability in probabilities:
            root_node.add_child(TreeNode(target_class, target, probability))

# Create a directed graph
G = nx.DiGraph()

# Function to add nodes and edges
def add_nodes_edges(tree_node, parent_name=None):
    node_name = f"{tree_node.target_class} -> {tree_node.target} ({tree_node.probability})"
    G.add_node(node_name)
    
    if parent_name is not None:
        G.add_edge(parent_name, node_name)
        
    for child in tree_node.children:
        add_nodes_edges(child, node_name)

# Populate the graph
add_nodes_edges(root_node)

# Draw the graph
pos = nx.spring_layout(G)  # You can change the layout algorithm if needed
nx.draw(G, pos, with_labels=True, font_size=8, node_size=3000, node_color='lightblue', font_color='black')
plt.show()
31/122:
class TreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

def visualize_tree(node, level=0):
    if node is not None:
        prefix = '    ' * level
        print(f'{prefix}|-- {node.target_class}: {node.target} ({node.probability})')
        for i, child in enumerate(node.children):
            if i == len(node.children) - 1:
                visualize_tree(child, level + 1)
            else:
                print(f'{prefix}|   ')
                visualize_tree(child, level + 1)

# Assuming you have constructed the tree_data dictionary

# Populate the tree with data from tree_data
root_node = TreeNode()
# Add nodes to the tree

# Visualize the tree
visualize_tree(root_node)
31/123:
from graphviz import Digraph

# Define tree data with conditions
tree_data = {
    'Condition: X > 5': {
        'Class A': [0.8, 0.9, 0.7],
        'Class B': [0.2, 0.3, 0.1]
    },
    'Condition: X <= 5': {
        'Class C': [0.6, 0.5, 0.4],
        'Class D': [0.9, 0.8, 0.7]
    }
}

# Create a new Digraph
dot = Digraph(format='png')

# Define node styles
dot.attr('node', shape='box')

# Recursive function to add nodes and edges
def add_nodes_and_edges(data, parent_condition=None):
    for condition, targets in data.items():
        if parent_condition is not None:
            dot.edge(parent_condition, condition)
        dot.node(condition, condition)
        for target, probabilities in targets.items():
            dot.node(target, target)
            for i, probability in enumerate(probabilities):
                dot.node(f'{target}_{i}', f'Probability: {probability}')
                dot.edge(target, f'{target}_{i}')
        add_nodes_and_edges(targets, condition)

# Add nodes and edges to the Digraph
add_nodes_and_edges(tree_data)

# Save the flowchart as a PNG file
dot.render('decision_tree', view=True)
31/124:
import pydot
from networkx.drawing.nx_pydot import graphviz_layout

# Create a directed graph
G = nx.DiGraph()

# Populate the graph with the same tree data
add_nodes_edges(root_node)

# Define the layout for the tree
pos = graphviz_layout(G, prog='dot')

# Draw the graph with radial layout
nx.draw(G, pos, with_labels=True, font_size=8, node_size=3000, node_color='lightblue', font_color='black')
plt.show()
31/125:
import pydot
from networkx.drawing.nx_pydot import graphviz_layout

# Create a directed graph
G = nx.DiGraph()

# Populate the graph with the same tree data
add_nodes_edges(root_node)

# Define the layout for the tree
pos = graphviz_layout(G, prog='dot')

# Draw the graph with radial layout
nx.draw(G, pos, with_labels=True, font_size=8, node_size=3000, node_color='lightblue', font_color='black')
plt.show()
31/126:
import pydot
from sklearn.tree import export_graphviz

# Assuming you have a single tree_data in forest_data
# You can access a single tree from the forest_data like: tree_data = forest_data.estimators_[0]

# Assuming tree_data is a DecisionTreeClassifier instance
dot_data = export_graphviz(tree_data, out_file=None, 
                           feature_names=dataset.columns,  
                           class_names=tree_data.classes_,  
                           filled=True, rounded=True,  
                           special_characters=True)  
graph = pydot.graph_from_dot_data(dot_data)  
graph[0].write_png('single_tree.png')
31/127:
import pydot
from sklearn.tree import export_graphviz

# Assuming you have a single tree_data in forest_data
# You can access a single tree from the forest_data like: tree_data = forest_data.estimators_[0]

# Assuming tree_data is a DecisionTreeClassifier instance
dot_data = export_graphviz(tree_data, out_file=None, 
                           feature_names=dataset.columns,  
                           class_names=tree_data.classes_,  
                           filled=True, rounded=True,  
                           special_characters=True)  
graph = pydot.graph_from_dot_data(dot_data)  
graph[0].write_png('single_tree.png')
31/128:
import pydot
from sklearn.tree import export_graphviz

# Assuming you have a single tree_data in forest_data
# You can access a single tree from the forest_data like: tree_data = forest_data.estimators_[0]

# Assuming tree_data is a DecisionTreeClassifier instance
dot_data = export_graphviz(tree_data, out_file=None, 
                           feature_names=dataset.columns,  
                           class_names=tree_data.classes_,  
                           filled=True, rounded=True,  
                           special_characters=True)  
graph = pydot.graph_from_dot_data(dot_data)  
graph[0].write_png('single_tree.png')
31/129:
import pydot
from sklearn.tree import export_graphviz

# Assuming you have a single tree_data in forest_data
# You can access a single tree from the forest_data like: tree_data = forest_data.estimators_[0]

# Assuming tree_data is a DecisionTreeClassifier instance
dot_data = export_graphviz(tree_data, out_file=None, 
                           feature_names=dataset.columns,  
                           class_names=tree_data,  
                           filled=True, rounded=True,  
                           special_characters=True)  
graph = pydot.graph_from_dot_data(dot_data)  
graph[0].write_png('single_tree.png')
31/130:
import pydot
from sklearn.tree import export_graphviz

# Assuming you have a single tree_data in forest_data
# You can access a single tree from the forest_data like: tree_data = forest_data.estimators_[0]

# Assuming tree_data is a DecisionTreeClassifier instance
dot_data = export_graphviz(tree_data, out_file=None, 
                           feature_names=dataset.columns,  
                           class_names=tree_data,  
                           filled=True, rounded=True,  
                           special_characters=True)  
graph = pydot.graph_from_dot_data(dot_data)  
graph[0].write_png('single_tree.png')
31/131:
import pydot
from sklearn.tree import export_graphviz

# Assuming tree_data is a DecisionTreeClassifier instance
dot_data = export_graphviz(tree_data, out_file=None, 
                           feature_names=dataset.columns,  
                           class_names=tree_data,  
                           filled=True, rounded=True,  
                           special_characters=True)  
graph = pydot.graph_from_dot_data(dot_data)  
graph[0].write_png('single_tree.png')
31/132:
def visualize_tree(tree_data, parent_name=None):
    for condition, targets in tree_data.items():
        node_name = condition if parent_name is None else f"{parent_name} -> {condition}"
        print(node_name)
        
        for target, probabilities in targets.items():
            for probability in probabilities:
                print(f"  {target} ({probability})")
                
            visualize_tree(targets[target], node_name)

# Call the function to visualize the tree_data
visualize_tree(tree_data)
31/133:
def visualize_tree(tree_data, parent_name=None):
    for condition, targets in tree_data.items():
        node_name = condition if parent_name is None else f"{parent_name} -> {condition}"
        print(node_name)

        tree_data = {'Cytochrome P450': {'Cytochrome P450 19A1': [0.106165761],
  'Cytochrome P450 17A1': [0.106165761],
  'Cytochrome P450 11B1': [0.106165761],
  'Cytochrome P450 11B2': [0.106165761]},
 'Electrochemical transporter': {'Dopamine transporter': [0.106165761],
  'Solute carrier organic anion transporter family member 1B1': [0.106165761]},
 'Enzyme': {'11-beta-hydroxysteroid dehydrogenase 1': [0.106165761],
  'Nitric oxide synthase, inducible': [0.106165761],
  'Protein farnesyltransferase': [0.106165761],
  'Estradiol 17-beta-dehydrogenase 2': [0.106165761],
  '11-beta-hydroxysteroid dehydrogenase 2': [0.106165761],
  'Carboxylesterase 2': [0.106165761],
  'Phospholipase A2 group IIA': [0.106165761],
  'Geranylgeranyl transferase type I': [0.106165761],
  'Telomerase reverse transcriptase': [0.106165761],
  'dUTP pyrophosphatase': [0.106165761],
  'Myeloperoxidase': [0.106165761],
  'Thymidine kinase, mitochondrial': [0.106165761],
  'Geranylgeranyl transferase type I beta subunit': [0.106165761],
  'Nicotinamide phosphoribosyltransferase': [0.106165761],
  'Liver glycogen phosphorylase': [0.106165761],
  'Isoleucyl-tRNA synthetase': [0.106165761]},
 'Family A G protein-coupled receptor': {'Vasopressin V1a receptor': [0.106165761],
  'Oxytocin receptor': [0.106165761],
  'Neurokinin 1 receptor': [0.106165761],
  'Kappa Opioid receptor': [0.106165761],
  'C-C chemokine receptor type 1': [0.106165761],
  'Orexin receptor 2': [0.106165761],
  'Orexin receptor 1': [0.106165761],
  'Neurokinin 3 receptor': [0.106165761],
  'C5a anaphylatoxin chemotactic receptor': [0.106165761],
  'Serotonin 2a (5-HT2a) receptor': [0.106165761],
  'Serotonin 7 (5-HT7) receptor': [0.106165761],
  'Serotonin 6 (5-HT6) receptor': [0.106165761],
  'Melatonin receptor 1A': [0.0],
  'Melatonin receptor 1B': [0.0]},
 'Family C G protein-coupled receptor': {'Metabotropic glutamate receptor 5   (by homology)': [0.106165761]},
 'Hydrolase': {'Acetylcholinesterase': [0.106165761]},
 'Kinase': {'Glycogen synthase kinase-3 beta': [0.106165761],
  'Leucine-rich repeat serine/threonine-protein kinase 2': [0.106165761],
  'Serine/threonine-protein kinase MST2': [0.106165761],
  'c-Jun N-terminal kinase 1': [0.106165761],
  'c-Jun N-terminal kinase 3': [0.106165761],
  'Protein kinase C theta': [0.106165761],
  'Tyrosine-protein kinase JAK3': [0.106165761],
  'MAP kinase p38 alpha': [0.106165761],
  'Serine/threonine-protein kinase MST4': [0.106165761],
  'MAP kinase ERK2': [0.106165761],
  'Serine/threonine-protein kinase PIM1': [0.106165761],
  'Serine/threonine-protein kinase PIM2': [0.106165761],
  'Fibroblast growth factor receptor 1': [0.106165761],
  'Epidermal growth factor receptor erbB1': [0.106165761],
  'Serine/threonine-protein kinase Aurora-A': [0.106165761],
  'Tyrosine-protein kinase receptor FLT3': [0.106165761],
  'Tyrosine-protein kinase JAK1': [0.106165761],
  'Tyrosine-protein kinase JAK2': [0.106165761],
  'Tyrosine-protein kinase TYK2': [0.106165761],
  'Cyclin-dependent kinase 2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-4': [0.106165761],
  'Cyclin-dependent kinase 5/CDK5 activator 1': [0.0]},
 'Membrane receptor': {'Interleukin-6 receptor subunit beta': [0.106165761]},
 'Nuclear receptor': {'Mineralocorticoid receptor': [1.0],
  'Androgen Receptor': [0.189458322],
  'Glucocorticoid receptor': [0.189458322],
  'Progesterone receptor': [0.106165761]},
 'Other cytosolic protein': {'Cyclin-dependent kinase 2/cyclin A': [0.106165761]},
 'Other nuclear protein': {'p53-binding protein Mdm-2': [0.106165761]},
 'Oxidoreductase': {'Cyclooxygenase-2': [0.106165761],
  'HMG-CoA reductase   (by homology)': [0.106165761]},
 'Phosphatase': {'T-cell protein-tyrosine phosphatase': [0.106165761],
  'Protein-tyrosine phosphatase 1B': [0.106165761]},
 'Phosphodiesterase': {'Phosphodiesterase 10A': [0.106165761]},
 'Primary active transporter': {'P-glycoprotein 1': [0.106165761],
  'Sulfonylurea receptor 2': [0.106165761]},
 'Protease': {'Thrombin': [0.106165761],
  'Trypsin I': [0.106165761],
  'Subtilisin/kexin type 7': [0.106165761],
  'Matrix metalloproteinase 13': [0.106165761],
  'Methionine aminopeptidase 2': [0.106165761],
  'Cathepsin S': [0.106165761],
  'Matrix metalloproteinase 1': [0.106165761],
  'Calpain 2': [0.106165761],
  'Calpain 1': [0.106165761],
  'Cathepsin (B and K)': [0.106165761],
  'Cathepsin K': [0.106165761],
  'Leukotriene A4 hydrolase': [0.106165761],
  'Chymotrypsin C': [0.106165761],
  'Caspase-3': [0.106165761],
  'Caspase-7': [0.106165761]},
 'Secreted protein': {'TNF-alpha': [0.106165761]},
 'Toll-like and Il-1 receptors': {'Toll-like receptor (TLR7/TLR9)': [0.106165761]},
 'Transferase': {'Fatty acid synthase': [0.106165761],
  'Thymidine kinase, cytosolic': [0.106165761],
  'Thymidylate synthase': [0.106165761]},
 'Unclassified protein': {'Endothelial PAS domain-containing protein 1': [0.106165761],
  'Polyadenylate-binding protein 1': [0.106165761],
  'Proto-oncogene vav': [0.106165761]},
 'Voltage-gated ion channel': {'Voltage-gated potassium channel, IKs; KCNQ1(Kv7.1)/KCNE1(MinK)': [0.106165761]}}
        
        for target, probabilities in targets.items():
            for probability in probabilities:
                print(f"  {target} ({probability})")
                
            visualize_tree(targets[target], node_name)

# Call the function to visualize the tree_data
visualize_tree(tree_data)
31/134:
def visualize_tree(tree_data, indent=0):
    for condition, targets in tree_data.items():
        print('  ' * indent + condition)
        
        for target, probabilities in targets.items():
            for probability in probabilities:
                print('  ' * (indent + 1) + f"{target} ({probability})")
                
            visualize_tree(targets[target], indent + 1)

# Call the function to visualize the tree_data
visualize_tree(tree_data)
31/135:
def visualize_tree(tree_data, indent=0):
    for tree in tree_data:
        for condition, targets in tree.items():
            print('  ' * indent + condition)

            for target, probabilities in targets.items():
                for probability in probabilities:
                    print('  ' * (indent + 1) + f"{target} ({probability})")

            visualize_tree([targets], indent + 1)

# Call the function to visualize the tree_data
visualize_tree(tree_data)
31/136:
import networkx as nx
import matplotlib.pyplot as plt

# Define the tree data (replace this with your actual tree data)
tree_data = {'Cytochrome P450': {'Cytochrome P450 19A1': [0.106165761],
  'Cytochrome P450 17A1': [0.106165761],
  'Cytochrome P450 11B1': [0.106165761],
  'Cytochrome P450 11B2': [0.106165761]},
 'Electrochemical transporter': {'Dopamine transporter': [0.106165761],
  'Solute carrier organic anion transporter family member 1B1': [0.106165761]},
 'Enzyme': {'11-beta-hydroxysteroid dehydrogenase 1': [0.106165761],
  'Nitric oxide synthase, inducible': [0.106165761],
  'Protein farnesyltransferase': [0.106165761],
  'Estradiol 17-beta-dehydrogenase 2': [0.106165761],
  '11-beta-hydroxysteroid dehydrogenase 2': [0.106165761],
  'Carboxylesterase 2': [0.106165761],
  'Phospholipase A2 group IIA': [0.106165761],
  'Geranylgeranyl transferase type I': [0.106165761],
  'Telomerase reverse transcriptase': [0.106165761],
  'dUTP pyrophosphatase': [0.106165761],
  'Myeloperoxidase': [0.106165761],
  'Thymidine kinase, mitochondrial': [0.106165761],
  'Geranylgeranyl transferase type I beta subunit': [0.106165761],
  'Nicotinamide phosphoribosyltransferase': [0.106165761],
  'Liver glycogen phosphorylase': [0.106165761],
  'Isoleucyl-tRNA synthetase': [0.106165761]},
 'Family A G protein-coupled receptor': {'Vasopressin V1a receptor': [0.106165761],
  'Oxytocin receptor': [0.106165761],
  'Neurokinin 1 receptor': [0.106165761],
  'Kappa Opioid receptor': [0.106165761],
  'C-C chemokine receptor type 1': [0.106165761],
  'Orexin receptor 2': [0.106165761],
  'Orexin receptor 1': [0.106165761],
  'Neurokinin 3 receptor': [0.106165761],
  'C5a anaphylatoxin chemotactic receptor': [0.106165761],
  'Serotonin 2a (5-HT2a) receptor': [0.106165761],
  'Serotonin 7 (5-HT7) receptor': [0.106165761],
  'Serotonin 6 (5-HT6) receptor': [0.106165761],
  'Melatonin receptor 1A': [0.0],
  'Melatonin receptor 1B': [0.0]},
 'Family C G protein-coupled receptor': {'Metabotropic glutamate receptor 5   (by homology)': [0.106165761]},
 'Hydrolase': {'Acetylcholinesterase': [0.106165761]},
 'Kinase': {'Glycogen synthase kinase-3 beta': [0.106165761],
  'Leucine-rich repeat serine/threonine-protein kinase 2': [0.106165761],
  'Serine/threonine-protein kinase MST2': [0.106165761],
  'c-Jun N-terminal kinase 1': [0.106165761],
  'c-Jun N-terminal kinase 3': [0.106165761],
  'Protein kinase C theta': [0.106165761],
  'Tyrosine-protein kinase JAK3': [0.106165761],
  'MAP kinase p38 alpha': [0.106165761],
  'Serine/threonine-protein kinase MST4': [0.106165761],
  'MAP kinase ERK2': [0.106165761],
  'Serine/threonine-protein kinase PIM1': [0.106165761],
  'Serine/threonine-protein kinase PIM2': [0.106165761],
  'Fibroblast growth factor receptor 1': [0.106165761],
  'Epidermal growth factor receptor erbB1': [0.106165761],
  'Serine/threonine-protein kinase Aurora-A': [0.106165761],
  'Tyrosine-protein kinase receptor FLT3': [0.106165761],
  'Tyrosine-protein kinase JAK1': [0.106165761],
  'Tyrosine-protein kinase JAK2': [0.106165761],
  'Tyrosine-protein kinase TYK2': [0.106165761],
  'Cyclin-dependent kinase 2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-4': [0.106165761],
  'Cyclin-dependent kinase 5/CDK5 activator 1': [0.0]},
 'Membrane receptor': {'Interleukin-6 receptor subunit beta': [0.106165761]},
 'Nuclear receptor': {'Mineralocorticoid receptor': [1.0],
  'Androgen Receptor': [0.189458322],
  'Glucocorticoid receptor': [0.189458322],
  'Progesterone receptor': [0.106165761]},
 'Other cytosolic protein': {'Cyclin-dependent kinase 2/cyclin A': [0.106165761]},
 'Other nuclear protein': {'p53-binding protein Mdm-2': [0.106165761]},
 'Oxidoreductase': {'Cyclooxygenase-2': [0.106165761],
  'HMG-CoA reductase   (by homology)': [0.106165761]},
 'Phosphatase': {'T-cell protein-tyrosine phosphatase': [0.106165761],
  'Protein-tyrosine phosphatase 1B': [0.106165761]},
 'Phosphodiesterase': {'Phosphodiesterase 10A': [0.106165761]},
 'Primary active transporter': {'P-glycoprotein 1': [0.106165761],
  'Sulfonylurea receptor 2': [0.106165761]},
 'Protease': {'Thrombin': [0.106165761],
  'Trypsin I': [0.106165761],
  'Subtilisin/kexin type 7': [0.106165761],
  'Matrix metalloproteinase 13': [0.106165761],
  'Methionine aminopeptidase 2': [0.106165761],
  'Cathepsin S': [0.106165761],
  'Matrix metalloproteinase 1': [0.106165761],
  'Calpain 2': [0.106165761],
  'Calpain 1': [0.106165761],
  'Cathepsin (B and K)': [0.106165761],
  'Cathepsin K': [0.106165761],
  'Leukotriene A4 hydrolase': [0.106165761],
  'Chymotrypsin C': [0.106165761],
  'Caspase-3': [0.106165761],
  'Caspase-7': [0.106165761]},
 'Secreted protein': {'TNF-alpha': [0.106165761]},
 'Toll-like and Il-1 receptors': {'Toll-like receptor (TLR7/TLR9)': [0.106165761]},
 'Transferase': {'Fatty acid synthase': [0.106165761],
  'Thymidine kinase, cytosolic': [0.106165761],
  'Thymidylate synthase': [0.106165761]},
 'Unclassified protein': {'Endothelial PAS domain-containing protein 1': [0.106165761],
  'Polyadenylate-binding protein 1': [0.106165761],
  'Proto-oncogene vav': [0.106165761]},
 'Voltage-gated ion channel': {'Voltage-gated potassium channel, IKs; KCNQ1(Kv7.1)/KCNE1(MinK)': [0.106165761]}}

# Create a directed graph
G = nx.DiGraph(tree_data)

# Draw the graph
pos = nx.spring_layout(G)  # You can change the layout algorithm if needed
nx.draw(G, pos, with_labels=True, font_size=8, node_size=3000, node_color='lightblue', font_color='black')
plt.show()
31/137:
import networkx as nx
import matplotlib.pyplot as plt

# Define the tree data (replace this with your actual tree data)
tree_data = {'Cytochrome P450': {'Cytochrome P450 19A1': [0.106165761],
  'Cytochrome P450 17A1': [0.106165761],
  'Cytochrome P450 11B1': [0.106165761],
  'Cytochrome P450 11B2': [0.106165761]},
 'Electrochemical transporter': {'Dopamine transporter': [0.106165761],
  'Solute carrier organic anion transporter family member 1B1': [0.106165761]},
 'Enzyme': {'11-beta-hydroxysteroid dehydrogenase 1': [0.106165761],
  'Nitric oxide synthase, inducible': [0.106165761],
  'Protein farnesyltransferase': [0.106165761],
  'Estradiol 17-beta-dehydrogenase 2': [0.106165761],
  '11-beta-hydroxysteroid dehydrogenase 2': [0.106165761],
  'Carboxylesterase 2': [0.106165761],
  'Phospholipase A2 group IIA': [0.106165761],
  'Geranylgeranyl transferase type I': [0.106165761],
  'Telomerase reverse transcriptase': [0.106165761],
  'dUTP pyrophosphatase': [0.106165761],
  'Myeloperoxidase': [0.106165761],
  'Thymidine kinase, mitochondrial': [0.106165761],
  'Geranylgeranyl transferase type I beta subunit': [0.106165761],
  'Nicotinamide phosphoribosyltransferase': [0.106165761],
  'Liver glycogen phosphorylase': [0.106165761],
  'Isoleucyl-tRNA synthetase': [0.106165761]},
 'Family A G protein-coupled receptor': {'Vasopressin V1a receptor': [0.106165761],
  'Oxytocin receptor': [0.106165761],
  'Neurokinin 1 receptor': [0.106165761],
  'Kappa Opioid receptor': [0.106165761],
  'C-C chemokine receptor type 1': [0.106165761],
  'Orexin receptor 2': [0.106165761],
  'Orexin receptor 1': [0.106165761],
  'Neurokinin 3 receptor': [0.106165761],
  'C5a anaphylatoxin chemotactic receptor': [0.106165761],
  'Serotonin 2a (5-HT2a) receptor': [0.106165761],
  'Serotonin 7 (5-HT7) receptor': [0.106165761],
  'Serotonin 6 (5-HT6) receptor': [0.106165761],
  'Melatonin receptor 1A': [0.0],
  'Melatonin receptor 1B': [0.0]},
 'Family C G protein-coupled receptor': {'Metabotropic glutamate receptor 5   (by homology)': [0.106165761]},
 'Hydrolase': {'Acetylcholinesterase': [0.106165761]},
 'Kinase': {'Glycogen synthase kinase-3 beta': [0.106165761],
  'Leucine-rich repeat serine/threonine-protein kinase 2': [0.106165761],
  'Serine/threonine-protein kinase MST2': [0.106165761],
  'c-Jun N-terminal kinase 1': [0.106165761],
  'c-Jun N-terminal kinase 3': [0.106165761],
  'Protein kinase C theta': [0.106165761],
  'Tyrosine-protein kinase JAK3': [0.106165761],
  'MAP kinase p38 alpha': [0.106165761],
  'Serine/threonine-protein kinase MST4': [0.106165761],
  'MAP kinase ERK2': [0.106165761],
  'Serine/threonine-protein kinase PIM1': [0.106165761],
  'Serine/threonine-protein kinase PIM2': [0.106165761],
  'Fibroblast growth factor receptor 1': [0.106165761],
  'Epidermal growth factor receptor erbB1': [0.106165761],
  'Serine/threonine-protein kinase Aurora-A': [0.106165761],
  'Tyrosine-protein kinase receptor FLT3': [0.106165761],
  'Tyrosine-protein kinase JAK1': [0.106165761],
  'Tyrosine-protein kinase JAK2': [0.106165761],
  'Tyrosine-protein kinase TYK2': [0.106165761],
  'Cyclin-dependent kinase 2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-4': [0.106165761],
  'Cyclin-dependent kinase 5/CDK5 activator 1': [0.0]},
 'Membrane receptor': {'Interleukin-6 receptor subunit beta': [0.106165761]},
 'Nuclear receptor': {'Mineralocorticoid receptor': [1.0],
  'Androgen Receptor': [0.189458322],
  'Glucocorticoid receptor': [0.189458322],
  'Progesterone receptor': [0.106165761]},
 'Other cytosolic protein': {'Cyclin-dependent kinase 2/cyclin A': [0.106165761]},
 'Other nuclear protein': {'p53-binding protein Mdm-2': [0.106165761]},
 'Oxidoreductase': {'Cyclooxygenase-2': [0.106165761],
  'HMG-CoA reductase   (by homology)': [0.106165761]},
 'Phosphatase': {'T-cell protein-tyrosine phosphatase': [0.106165761],
  'Protein-tyrosine phosphatase 1B': [0.106165761]},
 'Phosphodiesterase': {'Phosphodiesterase 10A': [0.106165761]},
 'Primary active transporter': {'P-glycoprotein 1': [0.106165761],
  'Sulfonylurea receptor 2': [0.106165761]},
 'Protease': {'Thrombin': [0.106165761],
  'Trypsin I': [0.106165761],
  'Subtilisin/kexin type 7': [0.106165761],
  'Matrix metalloproteinase 13': [0.106165761],
  'Methionine aminopeptidase 2': [0.106165761],
  'Cathepsin S': [0.106165761],
  'Matrix metalloproteinase 1': [0.106165761],
  'Calpain 2': [0.106165761],
  'Calpain 1': [0.106165761],
  'Cathepsin (B and K)': [0.106165761],
  'Cathepsin K': [0.106165761],
  'Leukotriene A4 hydrolase': [0.106165761],
  'Chymotrypsin C': [0.106165761],
  'Caspase-3': [0.106165761],
  'Caspase-7': [0.106165761]},
 'Secreted protein': {'TNF-alpha': [0.106165761]},
 'Toll-like and Il-1 receptors': {'Toll-like receptor (TLR7/TLR9)': [0.106165761]},
 'Transferase': {'Fatty acid synthase': [0.106165761],
  'Thymidine kinase, cytosolic': [0.106165761],
  'Thymidylate synthase': [0.106165761]},
 'Unclassified protein': {'Endothelial PAS domain-containing protein 1': [0.106165761],
  'Polyadenylate-binding protein 1': [0.106165761],
  'Proto-oncogene vav': [0.106165761]},
 'Voltage-gated ion channel': {'Voltage-gated potassium channel, IKs; KCNQ1(Kv7.1)/KCNE1(MinK)': [0.106165761]}}

# Create a directed graph
G = nx.DiGraph(tree_data)

# Draw the graph
pos = nx.circular_layout(G)  # Use circular layout
nx.draw(G, pos, with_labels=True, font_size=8, node_size=3000, node_color='lightgreen', font_color='black')  # Change node_color
plt.show()
31/138:
import networkx as nx
import matplotlib.pyplot as plt

# Define the tree data (replace this with your actual tree data)
tree_data = {'Cytochrome P450': {'Cytochrome P450 19A1': [0.106165761],
  'Cytochrome P450 17A1': [0.106165761],
  'Cytochrome P450 11B1': [0.106165761],
  'Cytochrome P450 11B2': [0.106165761]},
 'Electrochemical transporter': {'Dopamine transporter': [0.106165761],
  'Solute carrier organic anion transporter family member 1B1': [0.106165761]},
 'Enzyme': {'11-beta-hydroxysteroid dehydrogenase 1': [0.106165761],
  'Nitric oxide synthase, inducible': [0.106165761],
  'Protein farnesyltransferase': [0.106165761],
  'Estradiol 17-beta-dehydrogenase 2': [0.106165761],
  '11-beta-hydroxysteroid dehydrogenase 2': [0.106165761],
  'Carboxylesterase 2': [0.106165761],
  'Phospholipase A2 group IIA': [0.106165761],
  'Geranylgeranyl transferase type I': [0.106165761],
  'Telomerase reverse transcriptase': [0.106165761],
  'dUTP pyrophosphatase': [0.106165761],
  'Myeloperoxidase': [0.106165761],
  'Thymidine kinase, mitochondrial': [0.106165761],
  'Geranylgeranyl transferase type I beta subunit': [0.106165761],
  'Nicotinamide phosphoribosyltransferase': [0.106165761],
  'Liver glycogen phosphorylase': [0.106165761],
  'Isoleucyl-tRNA synthetase': [0.106165761]},
 'Family A G protein-coupled receptor': {'Vasopressin V1a receptor': [0.106165761],
  'Oxytocin receptor': [0.106165761],
  'Neurokinin 1 receptor': [0.106165761],
  'Kappa Opioid receptor': [0.106165761],
  'C-C chemokine receptor type 1': [0.106165761],
  'Orexin receptor 2': [0.106165761],
  'Orexin receptor 1': [0.106165761],
  'Neurokinin 3 receptor': [0.106165761],
  'C5a anaphylatoxin chemotactic receptor': [0.106165761],
  'Serotonin 2a (5-HT2a) receptor': [0.106165761],
  'Serotonin 7 (5-HT7) receptor': [0.106165761],
  'Serotonin 6 (5-HT6) receptor': [0.106165761],
  'Melatonin receptor 1A': [0.0],
  'Melatonin receptor 1B': [0.0]},
 'Family C G protein-coupled receptor': {'Metabotropic glutamate receptor 5   (by homology)': [0.106165761]},
 'Hydrolase': {'Acetylcholinesterase': [0.106165761]},
 'Kinase': {'Glycogen synthase kinase-3 beta': [0.106165761],
  'Leucine-rich repeat serine/threonine-protein kinase 2': [0.106165761],
  'Serine/threonine-protein kinase MST2': [0.106165761],
  'c-Jun N-terminal kinase 1': [0.106165761],
  'c-Jun N-terminal kinase 3': [0.106165761],
  'Protein kinase C theta': [0.106165761],
  'Tyrosine-protein kinase JAK3': [0.106165761],
  'MAP kinase p38 alpha': [0.106165761],
  'Serine/threonine-protein kinase MST4': [0.106165761],
  'MAP kinase ERK2': [0.106165761],
  'Serine/threonine-protein kinase PIM1': [0.106165761],
  'Serine/threonine-protein kinase PIM2': [0.106165761],
  'Fibroblast growth factor receptor 1': [0.106165761],
  'Epidermal growth factor receptor erbB1': [0.106165761],
  'Serine/threonine-protein kinase Aurora-A': [0.106165761],
  'Tyrosine-protein kinase receptor FLT3': [0.106165761],
  'Tyrosine-protein kinase JAK1': [0.106165761],
  'Tyrosine-protein kinase JAK2': [0.106165761],
  'Tyrosine-protein kinase TYK2': [0.106165761],
  'Cyclin-dependent kinase 2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-4': [0.106165761],
  'Cyclin-dependent kinase 5/CDK5 activator 1': [0.0]},
 'Membrane receptor': {'Interleukin-6 receptor subunit beta': [0.106165761]},
 'Nuclear receptor': {'Mineralocorticoid receptor': [1.0],
  'Androgen Receptor': [0.189458322],
  'Glucocorticoid receptor': [0.189458322],
  'Progesterone receptor': [0.106165761]},
 'Other cytosolic protein': {'Cyclin-dependent kinase 2/cyclin A': [0.106165761]},
 'Other nuclear protein': {'p53-binding protein Mdm-2': [0.106165761]},
 'Oxidoreductase': {'Cyclooxygenase-2': [0.106165761],
  'HMG-CoA reductase   (by homology)': [0.106165761]},
 'Phosphatase': {'T-cell protein-tyrosine phosphatase': [0.106165761],
  'Protein-tyrosine phosphatase 1B': [0.106165761]},
 'Phosphodiesterase': {'Phosphodiesterase 10A': [0.106165761]},
 'Primary active transporter': {'P-glycoprotein 1': [0.106165761],
  'Sulfonylurea receptor 2': [0.106165761]},
 'Protease': {'Thrombin': [0.106165761],
  'Trypsin I': [0.106165761],
  'Subtilisin/kexin type 7': [0.106165761],
  'Matrix metalloproteinase 13': [0.106165761],
  'Methionine aminopeptidase 2': [0.106165761],
  'Cathepsin S': [0.106165761],
  'Matrix metalloproteinase 1': [0.106165761],
  'Calpain 2': [0.106165761],
  'Calpain 1': [0.106165761],
  'Cathepsin (B and K)': [0.106165761],
  'Cathepsin K': [0.106165761],
  'Leukotriene A4 hydrolase': [0.106165761],
  'Chymotrypsin C': [0.106165761],
  'Caspase-3': [0.106165761],
  'Caspase-7': [0.106165761]},
 'Secreted protein': {'TNF-alpha': [0.106165761]},
 'Toll-like and Il-1 receptors': {'Toll-like receptor (TLR7/TLR9)': [0.106165761]},
 'Transferase': {'Fatty acid synthase': [0.106165761],
  'Thymidine kinase, cytosolic': [0.106165761],
  'Thymidylate synthase': [0.106165761]},
 'Unclassified protein': {'Endothelial PAS domain-containing protein 1': [0.106165761],
  'Polyadenylate-binding protein 1': [0.106165761],
  'Proto-oncogene vav': [0.106165761]},
 'Voltage-gated ion channel': {'Voltage-gated potassium channel, IKs; KCNQ1(Kv7.1)/KCNE1(MinK)': [0.106165761]}}

# Create a directed graph
G = nx.DiGraph(tree_data)

from graphviz import Digraph

# Create a Digraph object
dot = Digraph(comment='The Flowchart')

# Define nodes
dot.node('A', 'Start')
dot.node('B', 'Process 1')
dot.node('C', 'Decision')
dot.node('D', 'Process 2')
dot.node('E', 'End')

# Define edges
dot.edges(['AB', 'BC', 'CD', 'DE'])

# Define attributes (optional)
dot.attr(rankdir='LR')

# Render the graph
dot.render('flowchart', format='png', cleanup=True)

# Display the graph
dot
31/139:
import matplotlib.pyplot as plt

# Define the flowchart elements
elements = {
    'Start': (1, 3),
    'Process 1': (2, 3),
    'Decision': (3, 3),
    'Process 2': (3, 2),
    'End': (2, 1)
}

# Define the connections between elements
connections = [
    ('Start', 'Process 1'),
    ('Process 1', 'Decision'),
    ('Decision', 'Process 1', 'yes'),
    ('Decision', 'Process 2', 'no'),
    ('Process 2', 'End')
]

# Create a figure
plt.figure(figsize=(6, 4))

# Draw the elements
for element, (x, y) in elements.items():
    plt.text(x, y, element, bbox=dict(facecolor='lightblue', edgecolor='black', boxstyle='round,pad=0.3'), ha='center', va='center', fontsize=10)

# Draw the connections
for start, end, condition in connections:
    plt.arrow(elements[start][0], elements[start][1], elements[end][0] - elements[start][0], elements[end][1] - elements[start][1], head_width=0.1, head_length=0.1, fc='black', ec='black')
    if condition:
        plt.text((elements[start][0] + elements[end][0]) / 2, (elements[start][1] + elements[end][1]) / 2, condition, fontsize=10, ha='center', va='center')

# Set axis limits and hide axes
plt.xlim(0, 4)
plt.ylim(0, 4)
plt.axis('off')

# Show the flowchart
plt.show()
31/140:
import networkx as nx
import matplotlib.pyplot as plt

# Define the tree data (replace this with your actual tree data)
tree_data = {'Cytochrome P450': {'Cytochrome P450 19A1': [0.106165761],
  'Cytochrome P450 17A1': [0.106165761],
  'Cytochrome P450 11B1': [0.106165761],
  'Cytochrome P450 11B2': [0.106165761]},
 'Electrochemical transporter': {'Dopamine transporter': [0.106165761],
  'Solute carrier organic anion transporter family member 1B1': [0.106165761]},
 'Enzyme': {'11-beta-hydroxysteroid dehydrogenase 1': [0.106165761],
  'Nitric oxide synthase, inducible': [0.106165761],
  'Protein farnesyltransferase': [0.106165761],
  'Estradiol 17-beta-dehydrogenase 2': [0.106165761],
  '11-beta-hydroxysteroid dehydrogenase 2': [0.106165761],
  'Carboxylesterase 2': [0.106165761],
  'Phospholipase A2 group IIA': [0.106165761],
  'Geranylgeranyl transferase type I': [0.106165761],
  'Telomerase reverse transcriptase': [0.106165761],
  'dUTP pyrophosphatase': [0.106165761],
  'Myeloperoxidase': [0.106165761],
  'Thymidine kinase, mitochondrial': [0.106165761],
  'Geranylgeranyl transferase type I beta subunit': [0.106165761],
  'Nicotinamide phosphoribosyltransferase': [0.106165761],
  'Liver glycogen phosphorylase': [0.106165761],
  'Isoleucyl-tRNA synthetase': [0.106165761]},
 'Family A G protein-coupled receptor': {'Vasopressin V1a receptor': [0.106165761],
  'Oxytocin receptor': [0.106165761],
  'Neurokinin 1 receptor': [0.106165761],
  'Kappa Opioid receptor': [0.106165761],
  'C-C chemokine receptor type 1': [0.106165761],
  'Orexin receptor 2': [0.106165761],
  'Orexin receptor 1': [0.106165761],
  'Neurokinin 3 receptor': [0.106165761],
  'C5a anaphylatoxin chemotactic receptor': [0.106165761],
  'Serotonin 2a (5-HT2a) receptor': [0.106165761],
  'Serotonin 7 (5-HT7) receptor': [0.106165761],
  'Serotonin 6 (5-HT6) receptor': [0.106165761],
  'Melatonin receptor 1A': [0.0],
  'Melatonin receptor 1B': [0.0]},
 'Family C G protein-coupled receptor': {'Metabotropic glutamate receptor 5   (by homology)': [0.106165761]},
 'Hydrolase': {'Acetylcholinesterase': [0.106165761]},
 'Kinase': {'Glycogen synthase kinase-3 beta': [0.106165761],
  'Leucine-rich repeat serine/threonine-protein kinase 2': [0.106165761],
  'Serine/threonine-protein kinase MST2': [0.106165761],
  'c-Jun N-terminal kinase 1': [0.106165761],
  'c-Jun N-terminal kinase 3': [0.106165761],
  'Protein kinase C theta': [0.106165761],
  'Tyrosine-protein kinase JAK3': [0.106165761],
  'MAP kinase p38 alpha': [0.106165761],
  'Serine/threonine-protein kinase MST4': [0.106165761],
  'MAP kinase ERK2': [0.106165761],
  'Serine/threonine-protein kinase PIM1': [0.106165761],
  'Serine/threonine-protein kinase PIM2': [0.106165761],
  'Fibroblast growth factor receptor 1': [0.106165761],
  'Epidermal growth factor receptor erbB1': [0.106165761],
  'Serine/threonine-protein kinase Aurora-A': [0.106165761],
  'Tyrosine-protein kinase receptor FLT3': [0.106165761],
  'Tyrosine-protein kinase JAK1': [0.106165761],
  'Tyrosine-protein kinase JAK2': [0.106165761],
  'Tyrosine-protein kinase TYK2': [0.106165761],
  'Cyclin-dependent kinase 2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-4': [0.106165761],
  'Cyclin-dependent kinase 5/CDK5 activator 1': [0.0]},
 'Membrane receptor': {'Interleukin-6 receptor subunit beta': [0.106165761]},
 'Nuclear receptor': {'Mineralocorticoid receptor': [1.0],
  'Androgen Receptor': [0.189458322],
  'Glucocorticoid receptor': [0.189458322],
  'Progesterone receptor': [0.106165761]},
 'Other cytosolic protein': {'Cyclin-dependent kinase 2/cyclin A': [0.106165761]},
 'Other nuclear protein': {'p53-binding protein Mdm-2': [0.106165761]},
 'Oxidoreductase': {'Cyclooxygenase-2': [0.106165761],
  'HMG-CoA reductase   (by homology)': [0.106165761]},
 'Phosphatase': {'T-cell protein-tyrosine phosphatase': [0.106165761],
  'Protein-tyrosine phosphatase 1B': [0.106165761]},
 'Phosphodiesterase': {'Phosphodiesterase 10A': [0.106165761]},
 'Primary active transporter': {'P-glycoprotein 1': [0.106165761],
  'Sulfonylurea receptor 2': [0.106165761]},
 'Protease': {'Thrombin': [0.106165761],
  'Trypsin I': [0.106165761],
  'Subtilisin/kexin type 7': [0.106165761],
  'Matrix metalloproteinase 13': [0.106165761],
  'Methionine aminopeptidase 2': [0.106165761],
  'Cathepsin S': [0.106165761],
  'Matrix metalloproteinase 1': [0.106165761],
  'Calpain 2': [0.106165761],
  'Calpain 1': [0.106165761],
  'Cathepsin (B and K)': [0.106165761],
  'Cathepsin K': [0.106165761],
  'Leukotriene A4 hydrolase': [0.106165761],
  'Chymotrypsin C': [0.106165761],
  'Caspase-3': [0.106165761],
  'Caspase-7': [0.106165761]},
 'Secreted protein': {'TNF-alpha': [0.106165761]},
 'Toll-like and Il-1 receptors': {'Toll-like receptor (TLR7/TLR9)': [0.106165761]},
 'Transferase': {'Fatty acid synthase': [0.106165761],
  'Thymidine kinase, cytosolic': [0.106165761],
  'Thymidylate synthase': [0.106165761]},
 'Unclassified protein': {'Endothelial PAS domain-containing protein 1': [0.106165761],
  'Polyadenylate-binding protein 1': [0.106165761],
  'Proto-oncogene vav': [0.106165761]},
 'Voltage-gated ion channel': {'Voltage-gated potassium channel, IKs; KCNQ1(Kv7.1)/KCNE1(MinK)': [0.106165761]}}

import matplotlib.pyplot as plt

# Define the flowchart elements
elements = {
    'Start': (1, 3),
    'Process 1': (2, 3),
    'Decision': (3, 3),
    'Process 2': (3, 2),
    'End': (2, 1)
}

# Define the connections between elements
connections = [
    ('Start', 'Process 1'),
    ('Process 1', 'Decision'),
    ('Decision', 'Process 1', 'yes'),
    ('Decision', 'Process 2', 'no'),
    ('Process 2', 'End')
]

# Create a figure
plt.figure(figsize=(6, 4))

# Draw the elements
for element, (x, y) in elements.items():
    plt.text(x, y, element, bbox=dict(facecolor='lightblue', edgecolor='black', boxstyle='round,pad=0.3'), ha='center', va='center', fontsize=10)

# Draw the connections
for start, end, condition in connections:
    plt.arrow(elements[start][0], elements[start][1], elements[end][0] - elements[start][0], elements[end][1] - elements[start][1], head_width=0.1, head_length=0.1, fc='black', ec='black')
    if condition:
        plt.text((elements[start][0] + elements[end][0]) / 2, (elements[start][1] + elements[end][1]) / 2, condition, fontsize=10, ha='center', va='center')

# Set axis limits and hide axes
plt.xlim(0, 4)
plt.ylim(0, 4)
plt.axis('off')
31/141:
import matplotlib.pyplot as plt

def draw_flowchart(tree_data, x, y, x_step, y_step):
    plt.figure(figsize=(12, 8))
    elements = {}
    connections = []

    def traverse_tree(tree, x, y):
        nonlocal x_step, y_step
        elements[tree] = (x, y)

        children = tree_data.get(tree, [])
        if children:
            y -= y_step
            for child, _ in children.items():
                connections.append((tree, child))
                traverse_tree(child, x + x_step, y)

    traverse_tree(list(tree_data.keys())[0], x, y)

    for element, (x, y) in elements.items():
        plt.text(x, y, element, bbox=dict(facecolor='lightblue', edgecolor='black', boxstyle='round,pad=0.3'), ha='center', va='center', fontsize=8)

    for start, end in connections:
        plt.arrow(elements[start][0], elements[start][1], elements[end][0] - elements[start][0], elements[end][1] - elements[start][1], head_width=0.1, head_length=0.1, fc='black', ec='black')

    plt.xlim(0, x + 2*x_step)
    plt.ylim(y - 2*y_step, y + y_step)
    plt.axis('off')
    plt.show()

# Define initial coordinates and step size
start_x = 1
start_y = 3
x_step = 2
y_step = 1

# Draw the flowchart
draw_flowchart(tree_data, start_x, start_y, x_step, y_step)
31/142:
import matplotlib.pyplot as plt

def draw_flowchart(tree_data, x, y, x_step, y_step):
    plt.figure(figsize=(12, 8))
    elements = {}
    connections = []

    def traverse_tree(tree, x, y):
        nonlocal x_step, y_step
        elements[tree] = (x, y)

        children = tree_data.get(tree, [])
        if children:
            y -= y_step
            for child, _ in children.items():
                connections.append((tree, child))
                traverse_tree(child, x + x_step, y)

    for root, children in tree_data.items():
        elements[root] = (x, y)
        for child, _ in children.items():
            connections.append((root, child))
            traverse_tree(child, x + x_step, y)

    for element, (x, y) in elements.items():
        plt.text(x, y, element, bbox=dict(facecolor='lightblue', edgecolor='black', boxstyle='round,pad=0.3'), ha='center', va='center', fontsize=8)

    for start, end in connections:
        plt.arrow(elements[start][0], elements[start][1], elements[end][0] - elements[start][0], elements[end][1] - elements[start][1], head_width=0.1, head_length=0.1, fc='black', ec='black')

    plt.xlim(0, x + 2*x_step)
    plt.ylim(y - 2*y_step, y + y_step)
    plt.axis('off')
    plt.show()

# Define initial coordinates and step size
start_x = 1
start_y = 3
x_step = 2
y_step = 1

# Draw the flowchart
draw_flowchart(tree_data, start_x, start_y, x_step, y_step)
31/143:
import matplotlib.pyplot as plt

def draw_flowchart(tree_data, x, y, x_step, y_step):
    plt.figure(figsize=(12, 8))
    elements = {}
    connections = []

    def traverse_tree(tree, x, y):
        nonlocal x_step, y_step
        elements[tree] = (x, y)

        children = tree_data.get(tree, [])
        if children:
            y -= y_step
            for child, _ in children.items():
                connections.append((tree, child))
                traverse_tree(child, x + x_step, y)

    for root, children in tree_data.items():
        elements[root] = (x, y)
        for child, _ in children.items():
            connections.append((root, child))
            traverse_tree(child, x + x_step, y)

    for element, (x, y) in elements.items():
        plt.text(x, y, element, bbox=dict(facecolor='lightblue', edgecolor='black', boxstyle='round,pad=0.3'), ha='center', va='center', fontsize=8)

    for start, end in connections:
        plt.arrow(elements[start][0], elements[start][1], elements[end][0] - elements[start][0], elements[end][1] - elements[start][1], head_width=0.1, head_length=0.1, fc='black', ec='black')

    plt.xlim(0, x + 2*x_step)
    plt.ylim(y - 2*y_step, y + y_step)
    plt.axis('off')
    plt.show()

# Define initial coordinates and step size
start_x = 1
start_y = 3
x_step = 2
y_step = 1

# Draw the flowchart
draw_flowchart(tree_data, start_x, start_y, x_step, y_step)
31/144:
import matplotlib.pyplot as plt

def draw_flowchart(tree_data, x, y, x_step, y_step):
    plt.figure(figsize=(12, 8))
    elements = {}
    connections = []

    def traverse_tree(tree, x, y):
        nonlocal x_step, y_step
        elements[tree] = (x, y)

        children = tree_data.get(tree, [])
        if children:
            y -= y_step
            for child, probabilities in children.items():
                connections.append((tree, child))
                traverse_tree(child, x + x_step, y)

    for root, children in tree_data.items():
        elements[root] = (x, y)
        for child, probabilities in children.items():
            connections.append((root, child))
            traverse_tree(child, x + x_step, y)

    for element, (x, y) in elements.items():
        plt.text(x, y, element, bbox=dict(facecolor='lightblue', edgecolor='black', boxstyle='round,pad=0.3'), ha='center', va='center', fontsize=8)
        probabilities = tree_data.get(element, [])
        if probabilities:
            plt.text(x, y-0.2, f"Prob: {probabilities[0]:.4f}", ha='center', va='center', fontsize=8)

    for start, end in connections:
        plt.arrow(elements[start][0], elements[start][1], elements[end][0] - elements[start][0], elements[end][1] - elements[start][1], head_width=0.1, head_length=0.1, fc='black', ec='black')

    plt.xlim(0, x + 2*x_step)
    plt.ylim(y - 2*y_step, y + y_step)
    plt.axis('off')
    plt.show()

# Define initial coordinates and step size
start_x = 1
start_y = 3
x_step = 2
y_step = 1

# Draw the flowchart
draw_flowchart(tree_data, start_x, start_y, x_step, y_step)
31/145:
import matplotlib.pyplot as plt
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist
import numpy as np

# Your data (as an example)
data = {'Cytochrome P450': {'Cytochrome P450 19A1': [0.106165761],
  'Cytochrome P450 17A1': [0.106165761],
  'Cytochrome P450 11B1': [0.106165761],
  'Cytochrome P450 11B2': [0.106165761]},
 'Electrochemical transporter': {'Dopamine transporter': [0.106165761],
  'Solute carrier organic anion transporter family member 1B1': [0.106165761]},
 'Enzyme': {'11-beta-hydroxysteroid dehydrogenase 1': [0.106165761],
  'Nitric oxide synthase, inducible': [0.106165761],
  'Protein farnesyltransferase': [0.106165761],
  'Estradiol 17-beta-dehydrogenase 2': [0.106165761],
  '11-beta-hydroxysteroid dehydrogenase 2': [0.106165761],
  'Carboxylesterase 2': [0.106165761],
  'Phospholipase A2 group IIA': [0.106165761],
  'Geranylgeranyl transferase type I': [0.106165761],
  'Telomerase reverse transcriptase': [0.106165761],
  'dUTP pyrophosphatase': [0.106165761],
  'Myeloperoxidase': [0.106165761],
  'Thymidine kinase, mitochondrial': [0.106165761],
  'Geranylgeranyl transferase type I beta subunit': [0.106165761],
  'Nicotinamide phosphoribosyltransferase': [0.106165761],
  'Liver glycogen phosphorylase': [0.106165761],
  'Isoleucyl-tRNA synthetase': [0.106165761]},
 'Family A G protein-coupled receptor': {'Vasopressin V1a receptor': [0.106165761],
  'Oxytocin receptor': [0.106165761],
  'Neurokinin 1 receptor': [0.106165761],
  'Kappa Opioid receptor': [0.106165761],
  'C-C chemokine receptor type 1': [0.106165761],
  'Orexin receptor 2': [0.106165761],
  'Orexin receptor 1': [0.106165761],
  'Neurokinin 3 receptor': [0.106165761],
  'C5a anaphylatoxin chemotactic receptor': [0.106165761],
  'Serotonin 2a (5-HT2a) receptor': [0.106165761],
  'Serotonin 7 (5-HT7) receptor': [0.106165761],
  'Serotonin 6 (5-HT6) receptor': [0.106165761],
  'Melatonin receptor 1A': [0.0],
  'Melatonin receptor 1B': [0.0]},
 'Family C G protein-coupled receptor': {'Metabotropic glutamate receptor 5   (by homology)': [0.106165761]},
 'Hydrolase': {'Acetylcholinesterase': [0.106165761]},
 'Kinase': {'Glycogen synthase kinase-3 beta': [0.106165761],
  'Leucine-rich repeat serine/threonine-protein kinase 2': [0.106165761],
  'Serine/threonine-protein kinase MST2': [0.106165761],
  'c-Jun N-terminal kinase 1': [0.106165761],
  'c-Jun N-terminal kinase 3': [0.106165761],
  'Protein kinase C theta': [0.106165761],
  'Tyrosine-protein kinase JAK3': [0.106165761],
  'MAP kinase p38 alpha': [0.106165761],
  'Serine/threonine-protein kinase MST4': [0.106165761],
  'MAP kinase ERK2': [0.106165761],
  'Serine/threonine-protein kinase PIM1': [0.106165761],
  'Serine/threonine-protein kinase PIM2': [0.106165761],
  'Fibroblast growth factor receptor 1': [0.106165761],
  'Epidermal growth factor receptor erbB1': [0.106165761],
  'Serine/threonine-protein kinase Aurora-A': [0.106165761],
  'Tyrosine-protein kinase receptor FLT3': [0.106165761],
  'Tyrosine-protein kinase JAK1': [0.106165761],
  'Tyrosine-protein kinase JAK2': [0.106165761],
  'Tyrosine-protein kinase TYK2': [0.106165761],
  'Cyclin-dependent kinase 2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-4': [0.106165761],
  'Cyclin-dependent kinase 5/CDK5 activator 1': [0.0]},
 'Membrane receptor': {'Interleukin-6 receptor subunit beta': [0.106165761]},
 'Nuclear receptor': {'Mineralocorticoid receptor': [1.0],
  'Androgen Receptor': [0.189458322],
  'Glucocorticoid receptor': [0.189458322],
  'Progesterone receptor': [0.106165761]},
 'Other cytosolic protein': {'Cyclin-dependent kinase 2/cyclin A': [0.106165761]},
 'Other nuclear protein': {'p53-binding protein Mdm-2': [0.106165761]},
 'Oxidoreductase': {'Cyclooxygenase-2': [0.106165761],
  'HMG-CoA reductase   (by homology)': [0.106165761]},
 'Phosphatase': {'T-cell protein-tyrosine phosphatase': [0.106165761],
  'Protein-tyrosine phosphatase 1B': [0.106165761]},
 'Phosphodiesterase': {'Phosphodiesterase 10A': [0.106165761]},
 'Primary active transporter': {'P-glycoprotein 1': [0.106165761],
  'Sulfonylurea receptor 2': [0.106165761]},
 'Protease': {'Thrombin': [0.106165761],
  'Trypsin I': [0.106165761],
  'Subtilisin/kexin type 7': [0.106165761],
  'Matrix metalloproteinase 13': [0.106165761],
  'Methionine aminopeptidase 2': [0.106165761],
  'Cathepsin S': [0.106165761],
  'Matrix metalloproteinase 1': [0.106165761],
  'Calpain 2': [0.106165761],
  'Calpain 1': [0.106165761],
  'Cathepsin (B and K)': [0.106165761],
  'Cathepsin K': [0.106165761],
  'Leukotriene A4 hydrolase': [0.106165761],
  'Chymotrypsin C': [0.106165761],
  'Caspase-3': [0.106165761],
  'Caspase-7': [0.106165761]},
 'Secreted protein': {'TNF-alpha': [0.106165761]},
 'Toll-like and Il-1 receptors': {'Toll-like receptor (TLR7/TLR9)': [0.106165761]},
 'Transferase': {'Fatty acid synthase': [0.106165761],
  'Thymidine kinase, cytosolic': [0.106165761],
  'Thymidylate synthase': [0.106165761]},
 'Unclassified protein': {'Endothelial PAS domain-containing protein 1': [0.106165761],
  'Polyadenylate-binding protein 1': [0.106165761],
  'Proto-oncogene vav': [0.106165761]},
 'Voltage-gated ion channel': {'Voltage-gated potassium channel, IKs; KCNQ1(Kv7.1)/KCNE1(MinK)': [0.106165761]}}

# Convert the data to a numpy array
data_array = np.array([values for values in data.values()])

# Calculate the pairwise distance between data points
distances = pdist(data_array)

# Perform hierarchical clustering
linkage = hierarchy.linkage(distances, method='complete')

# Plot the hierarchical tree
plt.figure(figsize=(10, 6))
dendrogram = hierarchy.dendrogram(linkage, labels=list(data.keys()), leaf_rotation=90)
plt.ylabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/146:
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist
from sklearn.metrics import jaccard_score

# Define a function to calculate Jaccard similarity
def jaccard_similarity(set1, set2):
    return 1 - jaccard_score(set1, set2)

# Calculate the Jaccard distances between data points
keys = list(dataset.keys())
num_samples = len(keys)
distances = np.zeros((num_samples, num_samples))

for i in range(num_samples):
    for j in range(num_samples):
        if i != j:
            distances[i, j] = jaccard_similarity(set(data[keys[i]].keys()), set(data[keys[j]].keys()))

# Perform hierarchical clustering
linkage = hierarchy.linkage(pdist(distances), method='complete')

# Plot the hierarchical tree
plt.figure(figsize=(10, 6))
dendrogram = hierarchy.dendrogram(linkage, labels=keys, leaf_rotation=90)
plt.ylabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/147:
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist
from sklearn.metrics import jaccard_score

# Define a function to calculate Jaccard similarity
def jaccard_similarity(set1, set2):
    return 1 - jaccard_score(set1, set2)

# Calculate the Jaccard distances between data points
keys = list(dataset.keys())
num_samples = len(keys)
distances = np.zeros((num_samples, num_samples))

for i in range(num_samples):
    for j in range(num_samples):
        if i != j:
            distances[i, j] = jaccard_similarity(set(dataset[keys[i]].keys()), set(dataset[keys[j]].keys()))

# Perform hierarchical clustering
linkage = hierarchy.linkage(pdist(distances), method='complete')

# Plot the hierarchical tree
plt.figure(figsize=(10, 6))
dendrogram = hierarchy.dendrogram(linkage, labels=keys, leaf_rotation=90)
plt.ylabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/148:
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist
from sklearn.metrics import jaccard_score

# Define a function to calculate Jaccard similarity
def jaccard_similarity(set1, set2):
    return 1 - jaccard_score(set1, set2)

# Assuming 'dataset' is your data
keys = list(dataset.keys())
num_samples = len(keys)
distances = np.zeros((num_samples, num_samples))

for i in range(num_samples):
    for j in range(num_samples):
        if i != j:
            distances[i, j] = jaccard_similarity(set(dataset[keys[i]].keys()), set(dataset[keys[j]].keys()))

# Perform hierarchical clustering
linkage = hierarchy.linkage(pdist(distances), method='complete')

# Plot the hierarchical tree
plt.figure(figsize=(10, 6))
dendrogram = hierarchy.dendrogram(linkage, labels=keys, leaf_rotation=90)
plt.ylabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/149:
import numpy as np
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist

# Define a function to calculate Jaccard similarity
def jaccard_similarity(set1, set2):
    return 1 - len(set1.intersection(set2)) / len(set1.union(set2))

# Convert the data into a format suitable for distance calculations
keys = list(data.keys())
num_samples = len(keys)
enzyme_names = []

# Create a list of enzyme names for each sample
for key in keys:
    enzyme_names.append(set(data[key].keys()))

# Calculate the Jaccard distances between data points
distances = np.zeros((num_samples, num_samples))

for i in range(num_samples):
    for j in range(num_samples):
        if i != j:
            distances[i, j] = jaccard_similarity(enzyme_names[i], enzyme_names[j])

# Perform hierarchical clustering
linkage = hierarchy.linkage(pdist(distances), method='complete')

# Plot the hierarchical tree
plt.figure(figsize=(10, 6))
dendrogram = hierarchy.dendrogram(linkage, labels=keys, leaf_rotation=90)
plt.ylabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/150:
import numpy as np
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist

# Extract enzyme names for each category
enzyme_sets = []

for category in data.values():
    enzyme_set = set()
    for enzymes in category.values():
        enzyme_set.update(enzymes)
    enzyme_sets.append(enzyme_set)

# Calculate the Jaccard distances between data points
num_samples = len(enzyme_sets)
distances = np.zeros((num_samples, num_samples))

for i in range(num_samples):
    for j in range(num_samples):
        if i != j:
            intersection = len(enzyme_sets[i].intersection(enzyme_sets[j]))
            union = len(enzyme_sets[i].union(enzyme_sets[j]))
            distances[i, j] = 1 - (intersection / union)

# Perform hierarchical clustering
linkage = hierarchy.linkage(pdist(distances), method='complete')

# Plot the hierarchical tree
plt.figure(figsize=(10, 6))
dendrogram = hierarchy.dendrogram(linkage, labels=list(data.keys()), leaf_rotation=90)
plt.ylabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/151:
import numpy as np
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist

# Extract enzyme names for each category
enzyme_sets = []

for category in dataset.values():
    enzyme_set = set()
    for enzymes in category.values():
        enzyme_set.update(enzymes)
    enzyme_sets.append(enzyme_set)

# Calculate the Jaccard distances between data points
num_samples = len(enzyme_sets)
distances = np.zeros((num_samples, num_samples))

for i in range(num_samples):
    for j in range(num_samples):
        if i != j:
            intersection = len(enzyme_sets[i].intersection(enzyme_sets[j]))
            union = len(enzyme_sets[i].union(enzyme_sets[j]))
            distances[i, j] = 1 - (intersection / union)

# Perform hierarchical clustering
linkage = hierarchy.linkage(pdist(distances), method='complete')

# Plot the hierarchical tree with x and y axes interchanged
plt.figure(figsize=(10, 6))
dendrogram = hierarchy.dendrogram(linkage, labels=list(dataset.keys()), leaf_rotation=0, orientation='right')
plt.xlabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/152:
import numpy as np
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt

# Extract enzyme names for each category
enzyme_sets = []

for category in dataset.values():
    enzyme_set = set()
    for enzymes in category.values():
        enzyme_set.update(enzymes)
    enzyme_sets.append(enzyme_set)

# Calculate the Jaccard distances between data points
num_samples = len(enzyme_sets)
distances = np.zeros((num_samples, num_samples))

for i in range(num_samples):
    for j in range(num_samples):
        if i != j:
            intersection = len(enzyme_sets[i].intersection(enzyme_sets[j]))
            union = len(enzyme_sets[i].union(enzyme_sets[j]))
            distances[i, j] = 1 - (intersection / union)

# Perform hierarchical clustering
linkage_result = hierarchy.linkage(pdist(distances), method='complete')

# Plot the hierarchical tree with x and y axes interchanged
plt.figure(figsize=(10, 6))
dendrogram = hierarchy.dendrogram(linkage_result, labels=list(dataset.keys()), leaf_rotation=0, orientation='right')
plt.xlabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/153:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
31/154:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
31/155:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
31/156:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
31/157:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_no_outliers = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
31/158:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
31/159:
# Label encoding
from sklearn.preprocessing import LabelEncoder

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
31/160:
# Visualizing target class dietribution

import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
31/161:
# Visualizing target class and Probability distribution
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
31/162:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
31/163:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
31/164:
# Scalimg for train and test set

from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display(X_train_data, "\n", X_test_data)
display(Y_train_data, "\n", Y_test_data)
31/165:
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svr_regressor = SVR()
svr_regressor.fit(X_train_data, y_train.values.ravel())
svr_pred = svr_regressor.predict(X_test_data)
mse_svr = mean_squared_error(y_test, svr_pred)
print(f"Support Vector Machine MSE: {mse_svr}")
31/166:
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto'],  # Only relevant for RBF kernel
    'degree': [2, 3, 4]  # Only relevant for polynomial kernel
}

# Create an SVR instance
svr = SVR()

# Instantiate GridSearchCV
grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)

# Fit the grid search to the data
grid_search.fit(X_train_data, y_train.values.ravel())

# Get the best parameters and best estimator
best_params = grid_search.best_params_
best_svr = grid_search.best_estimator_
31/167:
# Checking with other models
# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Random Forest Regressor
rf_model = RandomForestRegressor()
rf_model.fit(X_train_data, y_train)
rf_predictions = rf_model.predict(X_test_data)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train_data, y_train)
gb_predictions = gb_model.predict(X_test_data)
gb_mse = mean_squared_error(y_test, gb_predictions)

# Build a decision tree regressor
dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
print(f'Decision tree MSE: {mse}')
31/168:
class CustomTreeNode:
    def __init__(self, target_class=None, target=None, probability=None):
        self.target_class = target_class
        self.target = target
        self.probability = probability
        self.children = []

    def add_child(self, child):
        self.children.append(child)

# Usage example
root_node = CustomTreeNode("Class A", "Target X", 0.8)
root_node.add_child(CustomTreeNode("Class B", "Target Y", 0.6))
root_node.add_child(CustomTreeNode("Class C", "Target Z", 0.9))

# Accessing information
print(root_node.target_class)  # Output: Class A
print(root_node.target)        # Output: Target X
print(root_node.probability)   # Output: 0.8
print(len(root_node.children)) # Output: 2
31/169:
from graphviz import Digraph

# Create a tree-like data structure using a dictionary
tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)
display(tree_data)
31/170:
import numpy as np
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt

# Extract enzyme names for each category
enzyme_sets = []

for category in dataset.values():
    enzyme_set = set()
    for enzymes in category.values():
        enzyme_set.update(enzymes)
    enzyme_sets.append(enzyme_set)

# Calculate the Jaccard distances between data points
num_samples = len(enzyme_sets)
distances = np.zeros((num_samples, num_samples))

for i in range(num_samples):
    for j in range(num_samples):
        if i != j:
            intersection = len(enzyme_sets[i].intersection(enzyme_sets[j]))
            union = len(enzyme_sets[i].union(enzyme_sets[j]))
            distances[i, j] = 1 - (intersection / union)

# Perform hierarchical clustering
linkage_result = hierarchy.linkage(pdist(distances), method='complete')

# Plot the hierarchical tree with x and y axes interchanged
plt.figure(figsize=(10, 6))
dendrogram = hierarchy.dendrogram(linkage_result, labels=list(dataset.keys()), leaf_rotation=0, orientation='right')
plt.xlabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/171:
import numpy as np
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt

# Extract enzyme names for each category
enzyme_sets = []

for category in dataset.values():
    enzyme_set = set()
    for enzymes in category.values():
        enzyme_set.update(enzymes)
    enzyme_sets.append(enzyme_set)

# Calculate the Jaccard distances between data points
num_samples = len(enzyme_sets)
distances = np.zeros((num_samples, num_samples))

for i in range(num_samples):
    for j in range(num_samples):
        if i != j:
            intersection = len(enzyme_sets[i].intersection(enzyme_sets[j]))
            union = len(enzyme_sets[i].union(enzyme_sets[j]))
            distances[i, j] = 1 - (intersection / union)

# Perform hierarchical clustering
linkage_result = hierarchy.linkage(pdist(distances), method='complete')

# Plot the hierarchical tree with x and y axes interchanged
plt.figure(figsize=(10, 6))
dendrogram = hierarchy.dendrogram(linkage_result, labels=list(dataset.keys()), leaf_rotation=0, orientation='right')
plt.xlabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/172:
from graphviz import Digraph

# Define tree data with conditions
tree_data = {'Cytochrome P450': {'Cytochrome P450 19A1': [0.106165761],
  'Cytochrome P450 17A1': [0.106165761],
  'Cytochrome P450 11B1': [0.106165761],
  'Cytochrome P450 11B2': [0.106165761]},
 'Electrochemical transporter': {'Dopamine transporter': [0.106165761],
  'Solute carrier organic anion transporter family member 1B1': [0.106165761]},
 'Enzyme': {'11-beta-hydroxysteroid dehydrogenase 1': [0.106165761],
  'Nitric oxide synthase, inducible': [0.106165761],
  'Protein farnesyltransferase': [0.106165761],
  'Estradiol 17-beta-dehydrogenase 2': [0.106165761],
  '11-beta-hydroxysteroid dehydrogenase 2': [0.106165761],
  'Carboxylesterase 2': [0.106165761],
  'Phospholipase A2 group IIA': [0.106165761],
  'Geranylgeranyl transferase type I': [0.106165761],
  'Telomerase reverse transcriptase': [0.106165761],
  'dUTP pyrophosphatase': [0.106165761],
  'Myeloperoxidase': [0.106165761],
  'Thymidine kinase, mitochondrial': [0.106165761],
  'Geranylgeranyl transferase type I beta subunit': [0.106165761],
  'Nicotinamide phosphoribosyltransferase': [0.106165761],
  'Liver glycogen phosphorylase': [0.106165761],
  'Isoleucyl-tRNA synthetase': [0.106165761]},
 'Family A G protein-coupled receptor': {'Vasopressin V1a receptor': [0.106165761],
  'Oxytocin receptor': [0.106165761],
  'Neurokinin 1 receptor': [0.106165761],
  'Kappa Opioid receptor': [0.106165761],
  'C-C chemokine receptor type 1': [0.106165761],
  'Orexin receptor 2': [0.106165761],
  'Orexin receptor 1': [0.106165761],
  'Neurokinin 3 receptor': [0.106165761],
  'C5a anaphylatoxin chemotactic receptor': [0.106165761],
  'Serotonin 2a (5-HT2a) receptor': [0.106165761],
  'Serotonin 7 (5-HT7) receptor': [0.106165761],
  'Serotonin 6 (5-HT6) receptor': [0.106165761],
  'Melatonin receptor 1A': [0.0],
  'Melatonin receptor 1B': [0.0]},
 'Family C G protein-coupled receptor': {'Metabotropic glutamate receptor 5   (by homology)': [0.106165761]},
 'Hydrolase': {'Acetylcholinesterase': [0.106165761]},
 'Kinase': {'Glycogen synthase kinase-3 beta': [0.106165761],
  'Leucine-rich repeat serine/threonine-protein kinase 2': [0.106165761],
  'Serine/threonine-protein kinase MST2': [0.106165761],
  'c-Jun N-terminal kinase 1': [0.106165761],
  'c-Jun N-terminal kinase 3': [0.106165761],
  'Protein kinase C theta': [0.106165761],
  'Tyrosine-protein kinase JAK3': [0.106165761],
  'MAP kinase p38 alpha': [0.106165761],
  'Serine/threonine-protein kinase MST4': [0.106165761],
  'MAP kinase ERK2': [0.106165761],
  'Serine/threonine-protein kinase PIM1': [0.106165761],
  'Serine/threonine-protein kinase PIM2': [0.106165761],
  'Fibroblast growth factor receptor 1': [0.106165761],
  'Epidermal growth factor receptor erbB1': [0.106165761],
  'Serine/threonine-protein kinase Aurora-A': [0.106165761],
  'Tyrosine-protein kinase receptor FLT3': [0.106165761],
  'Tyrosine-protein kinase JAK1': [0.106165761],
  'Tyrosine-protein kinase JAK2': [0.106165761],
  'Tyrosine-protein kinase TYK2': [0.106165761],
  'Cyclin-dependent kinase 2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-2': [0.106165761],
  'Receptor protein-tyrosine kinase erbB-4': [0.106165761],
  'Cyclin-dependent kinase 5/CDK5 activator 1': [0.0]},
 'Membrane receptor': {'Interleukin-6 receptor subunit beta': [0.106165761]},
 'Nuclear receptor': {'Mineralocorticoid receptor': [1.0],
  'Androgen Receptor': [0.189458322],
  'Glucocorticoid receptor': [0.189458322],
  'Progesterone receptor': [0.106165761]},
 'Other cytosolic protein': {'Cyclin-dependent kinase 2/cyclin A': [0.106165761]},
 'Other nuclear protein': {'p53-binding protein Mdm-2': [0.106165761]},
 'Oxidoreductase': {'Cyclooxygenase-2': [0.106165761],
  'HMG-CoA reductase   (by homology)': [0.106165761]},
 'Phosphatase': {'T-cell protein-tyrosine phosphatase': [0.106165761],
  'Protein-tyrosine phosphatase 1B': [0.106165761]},
 'Phosphodiesterase': {'Phosphodiesterase 10A': [0.106165761]},
 'Primary active transporter': {'P-glycoprotein 1': [0.106165761],
  'Sulfonylurea receptor 2': [0.106165761]},
 'Protease': {'Thrombin': [0.106165761],
  'Trypsin I': [0.106165761],
  'Subtilisin/kexin type 7': [0.106165761],
  'Matrix metalloproteinase 13': [0.106165761],
  'Methionine aminopeptidase 2': [0.106165761],
  'Cathepsin S': [0.106165761],
  'Matrix metalloproteinase 1': [0.106165761],
  'Calpain 2': [0.106165761],
  'Calpain 1': [0.106165761],
  'Cathepsin (B and K)': [0.106165761],
  'Cathepsin K': [0.106165761],
  'Leukotriene A4 hydrolase': [0.106165761],
  'Chymotrypsin C': [0.106165761],
  'Caspase-3': [0.106165761],
  'Caspase-7': [0.106165761]},
 'Secreted protein': {'TNF-alpha': [0.106165761]},
 'Toll-like and Il-1 receptors': {'Toll-like receptor (TLR7/TLR9)': [0.106165761]},
 'Transferase': {'Fatty acid synthase': [0.106165761],
  'Thymidine kinase, cytosolic': [0.106165761],
  'Thymidylate synthase': [0.106165761]},
 'Unclassified protein': {'Endothelial PAS domain-containing protein 1': [0.106165761],
  'Polyadenylate-binding protein 1': [0.106165761],
  'Proto-oncogene vav': [0.106165761]},
 'Voltage-gated ion channel': {'Voltage-gated potassium channel, IKs; KCNQ1(Kv7.1)/KCNE1(MinK)': [0.106165761]}}

# Create a new Digraph
dot = Digraph(format='png')

# Define node styles
dot.attr('node', shape='box')

# Recursive function to add nodes and edges
def add_nodes_and_edges(data, parent_condition=None):
    for condition, targets in data.items():
        if parent_condition is not None:
            dot.edge(parent_condition, condition)
        dot.node(condition, condition)
        for target, probabilities in targets.items():
            dot.node(target, target)
            for i, probability in enumerate(probabilities):
                dot.node(f'{target}_{i}', f'Probability: {probability}')
                dot.edge(target, f'{target}_{i}')
        add_nodes_and_edges(targets, condition)

# Add nodes and edges to the Digraph
add_nodes_and_edges(tree_data)

# Save the flowchart as a PNG file
dot.render('decision_tree', view=True)
31/173:
def convert_to_list(data):
    result = []
    for key, value in data.items():
        if isinstance(value, dict):
            result.append({key: convert_to_list(value)})
        else:
            result.append({key: value})
    return result

nested_list = convert_to_list(tree_data)
31/174:
import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.show()
31/175:
import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plt.text(level, level, f"{key}", ha='left', va='center', fontsize=8)  # Plotting the node
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")
            plt.text(level, level, f"{key}: {value[0]}", ha='left', va='center', fontsize=8)  # Plotting the leaf node

# Create a figure
plt.figure(figsize=(8, 8))

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.axis('off')  # Turn off axis for better visualization
plt.show()
31/176:
import matplotlib.pyplot as plt
import networkx as nx

def plot_tree(tree, parent_name, graph, level=0, pos=None, parent_pos=None):
    if pos is None:
        pos = {parent_name: (0, 1)}
    else:
        pos[parent_name] = (pos[parent_pos][0] + 1, 1 - level*0.1)
    for node, children in tree.items():
        graph.add_node(node)
        graph.add_edge(parent_name, node)
        pos = plot_tree(children, node, graph, level + 1, pos, parent_name)
    return pos

# Assuming `nested_list` contains your hierarchical data
G = nx.DiGraph()
pos = plot_tree(nested_list[0], 'Root', G)

# Draw the tree
plt.figure(figsize=(10, 6))
nx.draw(G, pos, with_labels=True, node_size=3000, node_color="skyblue", font_size=10, font_color="black", font_weight="bold", arrows=False)
plt.show()
31/177:
import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0, ypos=0):
    for key, value in data.items():
        if isinstance(value, dict):
            plt.text(level, ypos, f"{key}", ha='left', va='center', fontsize=8)  # Plotting the node
            plot_tree(value, parent=key, level=level+1, ypos=ypos-1)
        else:
            plt.text(level, ypos, f"{key}: {value[0]:.2f}", ha='left', va='center', fontsize=8)  # Plotting the leaf node

# Create a figure
plt.figure(figsize=(8, 8))

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.axis('off')  # Turn off axis for better visualization
plt.show()
31/178:
import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0, ypos=0):
    for key, value in data.items():
        if isinstance(value, dict):
            plt.text(level, ypos, f"{key}", ha='left', va='center', fontsize=8)  # Plotting the node
            plot_tree(value, parent=key, level=level+1, ypos=ypos-1)
        else:
            plt.text(level, ypos, f"{key}: {value[0]:.2f}", ha='left', va='center', fontsize=8)  # Plotting the leaf node

# Create a figure
plt.figure(figsize=(8, 8))

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.axis('on')  # Turn off axis for better visualization
plt.show()
31/179:
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt
import numpy as np

def plot_tree(data, parent=None, level=0, ypos=0):
    for key, value in data.items():
        if isinstance(value, dict):
            plt.text(level, ypos, f"{key}", ha='left', va='center', fontsize=8)  # Plotting the node
            plot_tree(value, parent=key, level=level+1, ypos=ypos-1)
        else:
            plt.text(level, ypos, f"{key}: {value[0]:.2f}", ha='left', va='center', fontsize=8)  # Plotting the leaf node

# Plot the hierarchical tree
plt.figure(figsize=(10, 6))
hierarchy.dendrogram(linkage, labels=list(dataset.keys()), leaf_rotation=90, orientation='right')
plt.xlabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/180:
from scipy.cluster import hierarchy
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt
import numpy as np

def plot_tree(data, parent=None, level=0, ypos=0):
    for key, value in data.items():
        if isinstance(value, dict):
            plt.text(level, ypos, f"{key}", ha='left', va='center', fontsize=8)  # Plotting the node
            plot_tree(value, parent=key, level=level+1, ypos=ypos-1)
        else:
            plt.text(level, ypos, f"{key}: {value[0]:.2f}", ha='left', va='center', fontsize=8)  # Plotting the leaf node

# Confirm the dimensions of Z and labels
print(linkage.shape)  # This should print the dimensions of your linkage matrix
print(len(list(dataset.keys())))  # This should print the number of labels

# Plot the hierarchical tree
plt.figure(figsize=(10, 6))
hierarchy.dendrogram(linkage, labels=list(dataset.keys()), leaf_rotation=90, orientation='right')
plt.xlabel('Distance')
plt.title('Hierarchical Tree')
plt.show()
31/181:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
31/182:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
31/183:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
31/184:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
31/185:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_no_outliers = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
31/186:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
31/187:
# Label encoding
from sklearn.preprocessing import LabelEncoder

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
31/188:
# Visualizing target class dietribution

import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
31/189:
# Visualizing target class and Probability distribution
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
31/190:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
31/191:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
31/192:
# Scalimg for train and test set

from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display(X_train_data, "\n", X_test_data)
display(Y_train_data, "\n", Y_test_data)
31/193:
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svr_regressor = SVR()
svr_regressor.fit(X_train_data, y_train.values.ravel())
svr_pred = svr_regressor.predict(X_test_data)
mse_svr = mean_squared_error(y_test, svr_pred)
print(f"Support Vector Machine MSE: {mse_svr}")
31/194:
# Checking with other models
# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Random Forest Regressor
rf_model = RandomForestRegressor()
rf_model.fit(X_train_data, y_train)
rf_predictions = rf_model.predict(X_test_data)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train_data, y_train)
gb_predictions = gb_model.predict(X_test_data)
gb_mse = mean_squared_error(y_test, gb_predictions)

# Build a decision tree regressor
dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
print(f'Decision tree MSE: {mse}')
31/195:
from graphviz import Digraph

# Create a tree-like data structure using a dictionary
tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)
display(tree_data)
31/196:
import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.show()
33/1:
# Build a Support vector machines

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svr_regressor = SVR()
svr_regressor.fit(X_train_data, y_train.values.ravel())
svr_pred = svr_regressor.predict(X_test_data)
mse_svr = mean_squared_error(y_test, svr_pred)
print(f"Support Vector Machine MSE: {mse_svr}")
33/2:
# Read the CSV file
from IPython.display import display
import pandas as pd
dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
33/3:
# Check for missing values
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
33/4:
# Check for duplicate rows
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
33/5:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
33/6:
import matplotlib.pyplot as plt

# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_no_outliers = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_no_outliers.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
33/7:
# Scaling and Normalization
from sklearn.preprocessing import StandardScaler
import numpy as np

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
33/8:
# Label encoding
from sklearn.preprocessing import LabelEncoder

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
33/9:
# Visualizing target class dietribution

import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
33/10:
# Visualizing target class and Probability distribution
import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
33/11:
# Splitting train and test data

# import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
33/12:
from sklearn.preprocessing import LabelEncoder

# Perform label encoding on the training set
X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
33/13:
# Scalimg for train and test set

from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display(X_train_data, "\n", X_test_data)
display(Y_train_data, "\n", Y_test_data)
33/14:
# Build a Support vector machines

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svr_regressor = SVR()
svr_regressor.fit(X_train_data, y_train.values.ravel())
svr_pred = svr_regressor.predict(X_test_data)
mse_svr = mean_squared_error(y_test, svr_pred)
print(f"Support Vector Machine MSE: {mse_svr}")
33/15:
# Checking with other models
# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Random Forest Regressor
rf_model = RandomForestRegressor()
rf_model.fit(X_train_data, y_train)
rf_predictions = rf_model.predict(X_test_data)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train_data, y_train)
gb_predictions = gb_model.predict(X_test_data)
gb_mse = mean_squared_error(y_test, gb_predictions)

# Build a decision tree regressor
dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
print(f'Decision tree MSE: {mse}')
33/16:
from graphviz import Digraph

# Create a tree-like data structure using a dictionary
tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)
display(tree_data)
33/17:
import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.show()
33/18:
from graphviz import Digraph

# Create a tree-like data structure using a dictionary
tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
print(tree_data)
display(tree_data)
33/19:
# Create a tree-like data structure using a dictionary

from graphviz import Digraph

tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
display(tree_data)
33/20:
import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.show()
33/21:
# Create a tree-like data structure using a dictionary

from graphviz import Digraph

tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
display(tree_data)
33/22:
# Build a decision tree regressor

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
33/23:
# Build a decision tree regressor

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Decision tree MSE: {mse}')
33/24:
# Checking with other models

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

# Random Forest Regressor

rf_model = RandomForestRegressor()
rf_model.fit(X_train_data, y_train)
rf_predictions = rf_model.predict(X_test_data)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor

gb_model = GradientBoostingRegressor()
gb_model.fit(X_train_data, y_train)
gb_predictions = gb_model.predict(X_test_data)
gb_mse = mean_squared_error(y_test, gb_predictions)

# Build a Support vector machines

svr_regressor = SVR()
svr_regressor.fit(X_train_data, y_train.values.ravel())
svr_pred = svr_regressor.predict(X_test_data)
mse_svr = mean_squared_error(y_test, svr_pred)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
print(f"Support Vector Machine MSE: {mse_svr}")
33/25:
# Build a decision tree regressor

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Decision tree MSE: {mse}')
33/26:
# Cross Validation

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(dt_regressor, X_train_data, y_train, cv=5, scoring='neg_mean_squared_error')
mse_cv = -cv_scores.mean()
33/27:
# Cross Validation

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(dt_regressor, X_train_data, y_train, cv=5, scoring='neg_mean_squared_error')
mse_cv = -cv_scores.mean()
print(mse_cv)
34/1:
# Read the CSV file

import pandas as pd
from IPython.display import display

data = pd.read_excel(r"C:\Users\sound\OneDrive\Desktop\main project\python project 2\CANCER DRUF INFO.xlsx")
display(data)
34/2:
# Check for duplicate rows

duplicates = data.duplicated()
duplicate_rows = data[duplicates]
print(duplicate_rows)
df_cleaned = data.drop_duplicates()
display(df_cleaned)
34/3:
# Check for missing values

missing_values = data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
34/4:
# Check for missing values

missing_values = data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
bool_series = pd.isnull(data['Pharmakodynamics'])  
data[bool_series]
34/5:
# Check for missing values

missing_values = data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
bool_series = pd.isnull(data['Pharmakodynamics'])  
data[bool_series] 
data_filled = data.fillna(data.mean())
data[data_filled]
34/6:
# Check for missing values

missing_values = data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
df = pd.DataFrame(dict)  
df.fillna(method ='bfill')
34/7:
# Check for missing values

missing_values = data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
data.dropna()
34/8:
#Inspect Data Types

data_types = data.dtypes
display(data_types)
34/9:
# Check for missing values

missing_values = data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
34/10:
# Check for missing values

missing_values = data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
data['Pharmakodynamics'].fillna('Not available', inplace=True)
34/11:
# Check for missing values

missing_values = data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
data_replace = data['Pharmakodynamics'].fillna('Not available', inplace=True)
print(data_replace)
34/12:
# Check for missing values

missing_values = data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
data_replace = data['Pharmakodynamics'].fillna('Not available', inplace=True)
display(data_replace)
34/13:
# Data integration

import pandas as pd

data_combined = data['Generic name'] + ' (' + data['Brand name'] + ')'
34/14:
# Data integration

import pandas as pd

data_combined = data['Generic name'] + ' (' + data['Brand name'] + ')'
display(data_combined)
34/15:
# Standardize dosage recommendation

import pandas as pd

def convert_to_mg(dosage):
 
    unit_conversions = data['Dosage Recommendation']
    
    # Split the dosage into value and unit
    value, unit = dosage.split()
    
    # Convert the dosage to milligrams
    dosage_in_mg = float(value) * unit_conversions.get(unit, 1)
    
    return dosage_in_mg
Dosage_Recommendation = data['Dosage Recommendation'].apply(convert_to_mg)
34/16:
def convert_to_mg(dosage):
    unit_conversions = data['Dosage Recommendation']
    print(f'dosage: {dosage}')
    values = dosage.split()
    print(f'values: {values}')
    value, unit = values
    dosage_in_mg = float(value) * unit_conversions.get(unit, 1)
    return dosage_in_mg

Dosage_Recommendation = data['Dosage Recommendation'].apply(convert_to_mg)
34/17:
def convert_to_mg(dosage):
    unit_conversions = data['Dosage Recommendation']
    print(f'dosage: {dosage}')
    values = dosage.split()
    print(f'values: {values}')
    value, unit = values
    dosage_in_mg = float(value) * unit_conversions.get(unit, 1)
    return dosage_in_mg

Dosage_Recommendation = data['Dosage Recommendation'].apply(convert_to_mg)
34/18:
def convert_to_mg(dosage):
    unit_conversions = data['Dosage Recommendation']
    
    # Check if the dosage string contains numbers and units
    if any(char.isdigit() for char in dosage) and any(char.isalpha() for char in dosage):
        value, unit = dosage.split()
        dosage_in_mg = float(value) * unit_conversions.get(unit, 1)
        return dosage_in_mg
    else:
        return None 

Dosage_Recommendation = data['Dosage Recommendation'].apply(convert_to_mg)
34/19:
def convert_to_mg(dosage):
    unit_conversions = data['Dosage Recommendation']
    
    # Check if the dosage string contains numbers and units
    if any(char.isdigit() for char in dosage) and any(char.isalpha() for char in dosage):
        try:
            value, unit = dosage.split()
            dosage_in_mg = float(value) * unit_conversions.get(unit, 1)
            return dosage_in_mg
        except ValueError as e:
            print(f"Error processing dosage '{dosage}': {e}")
            return None
    else:
        print(f"Invalid dosage format: {dosage}")
        return None 

Dosage_Recommendation = data['Dosage Recommendation'].apply(convert_to_mg)
34/20:
def convert_to_mg(dosage):
    unit_conversions = data['Dosage Recommendation']
    
    # Check if the dosage string contains numbers and units
    if any(char.isdigit() for char in dosage) and any(char.isalpha() for char in dosage):
        try:
            value, unit = dosage.split()
            dosage_in_mg = float(value) * unit_conversions.get(unit, 1)
            return dosage_in_mg
    else:
        print(f"Invalid dosage format: {dosage}")
        return None 

Dosage_Recommendation = data['Dosage Recommendation'].apply(convert_to_mg)
34/21:
def convert_to_mg(dosage):
    unit_conversions = data['Dosage Recommendation']
    
    # Check if the dosage string contains numbers and units
    if any(char.isdigit() for char in dosage) and any(char.isalpha() for char in dosage):
        try:
            value, unit = dosage.split()
            dosage_in_mg = float(value) * unit_conversions.get(unit, 1)
            return dosage_in_mg
        except ValueError as e:
            print(f"Error processing dosage '{dosage}': {e}")
            return None
    else:
        print(f"Invalid dosage format: {dosage}")
        return None 

Dosage_Recommendation = data['Dosage Recommendation'].apply(convert_to_mg)
34/22:
def convert_to_mg(dosage):
    unit_conversions = data['Dosage Recommendation']
    
    # Check if the dosage string contains numbers and units
    if any(char.isdigit() for char in dosage) and any(char.isalpha() for char in dosage):
        try:
            value, unit = dosage.split()
            dosage_in_mg = float(value) * unit_conversions.get(unit, 1)
            return dosage_in_mg
        except ValueError as e:
            print(f"dosage '{dosage}': {e}")
            return None
    else:
        print(f"Invalid dosage format: {dosage}")
        return None 

Dosage_Recommendation = data['Dosage Recommendation'].apply(convert_to_mg)
34/23:
# Label encoding

from sklearn.preprocessing import LabelEncoder

encoded_labels = data.drop['Sl.No']
label_encoder = LabelEncoder()
df = label_encoder.fit_transform(encoded_labels)
print(df)
34/24:
# Label encoding

from sklearn.preprocessing import LabelEncoder

encoded_labels = data
label_encoder = LabelEncoder()
df = label_encoder.fit_transform(encoded_labels)
print(df)
34/25:
from sklearn.preprocessing import LabelEncoder

# Assuming 'data' is your DataFrame
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(data['Dosage Recommendation'])

print(encoded_labels)
34/26:
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

data_encode = ['Disease name', 'Generic name', 'Brand name', 
                     'Uses', 'Dosage Recommendation', 'Pharmakodynamics', 
                     'Mechanism of Action', 'Citiations']

# Iterate over each column and encode its values
for col in data_encode:
    data[col] = label_encoder.fit_transform(data[col])
34/27:
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

data_encode = ['Disease name', 'Generic name', 'Brand name', 
                     'Uses', 'Dosage Recommendation', 'Pharmakodynamics', 
                     'Mechanism of Action', 'Citiations']

# Iterate over each column and encode its values
for col in data_encode:
    data[col] = label_encoder.fit_transform(data[col])
    print(data_encode)
34/28:
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

data_encode = ['Disease name', 'Generic name', 'Brand name', 
                     'Uses', 'Dosage Recommendation', 'Pharmakodynamics', 
                     'Mechanism of Action', 'Citiations']

# Iterate over each column and encode its values
for col in data_encode:
    data[col] = label_encoder.fit_transform(data[col])
    print(data[col])
35/1:
# Read the CSV file

import pandas as pd
from IPython.display import display

data = pd.read_excel(r"C:\Users\sound\OneDrive\Desktop\main project\python project 2\CANCER DRUF INFO.xlsx")
display(data)
35/2:
# Check for duplicate rows

duplicates = data.duplicated()
duplicate_rows = data[duplicates]
print(duplicate_rows)
df_cleaned = data.drop_duplicates()
display(df_cleaned)
35/3:
# Check for missing values

missing_values = data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
data_replace = data['Pharmakodynamics'].fillna('Not available', inplace=True)
display(data_replace)
35/4:
#Inspect Data Types

data_types = data.dtypes
display(data_types)
35/5:
# Data integration

import pandas as pd

data_combined = data['Generic name'] + ' (' + data['Brand name'] + ')'
display(data_combined)
35/6:
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

data_encode = ['Disease name', 'Generic name', 'Brand name', 
                     'Uses', 'Dosage Recommendation', 'Pharmakodynamics', 
                     'Mechanism of Action', 'Citiations']

# Iterate over each column and encode its values
for col in data_encode:
    data[col] = label_encoder.fit_transform(data[col])
    print(data[col])
35/7:
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def process_text(text):
    # Tokenization
    tokens = word_tokenize(text.lower())  # Convert to lowercase for consistency

    # Removing stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return tokens

text = data['']
processed_text = process_text(text)
print(processed_text)
35/8:
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def process_text(text):
    # Tokenization
    tokens = word_tokenize(text.lower())  # Convert to lowercase for consistency

    # Removing stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return tokens

text = data['Pharmakodynamics', 'Mechanism of Action']
processed_text = process_text(text)
print(processed_text)
35/9:
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Assuming 'data' is your DataFrame
# Process 'Pharmacodynamics' column
def process_text(text):
    tokens = word_tokenize(text.lower())
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return tokens

data['Pharmacodynamics'] = data['Pharmacodynamics'].apply(process_text)

# Process 'Mechanism of Action' column
data['Mechanism of Action'] = data['Mechanism of Action'].apply(process_text)
35/10:
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Assuming 'data' is your DataFrame
# Process 'Pharmacodynamics' column
def process_text(text):
    tokens = word_tokenize(text.lower())
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return tokens

data['Pharmakodynamics'] = data['Pharmakodynamics'].apply(process_text)

# Process 'Mechanism of Action' column
data['Mechanism of Action'] = data['Mechanism of Action'].apply(process_text)
35/11:
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download NLTK resources (you only need to do this once)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Define the function to process text
def process_text(text):
    # Tokenization
    tokens = word_tokenize(text.lower())  
    
    # Removing stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    
    return tokens

# Apply the process_text function to the 'Pharmacodynamics' column
data['Pharmakodynamics_processed'] = data['Pharmakodynamics'].apply(process_text)

# Process 'Mechanism of Action' column
data['Mechanism of Action'] = data['Mechanism of Action'].apply(process_text)
35/12:
# Remove all special characters

val = 'abckjdi#$9'
pattern = re.compile(r'[A-Za-z]+')
matches = pattern.finditer(val)
for mat in matches:
    print(mat)
35/13:
# Remove all special characters

import re

val = 'abckjdi#$9'
pattern = re.compile(r'[A-Za-z]+')
matches = pattern.finditer(val)
for mat in matches:
    print(mat)
35/14:
# Remove all Stopwords, duplicate words, and punctuations

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
data.text = data.text.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
data['text']=data['text'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))
data['text'] = data['text'].str.replace('[^\w\s]','')
35/15:
# Remove all Stopwords, duplicate words, and punctuations

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
data.text = data.text.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
data['text']= data['Disease name', 'Generic name', 'Brand name', 'Uses', 'Dosage Recommendation', 'Pharmakodynamics', 'Mechanism of Action', 'Citiations'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))
data['text'] = data['Disease name', 'Generic name', 'Brand name', 'Uses', 'Dosage Recommendation', 'Pharmakodynamics', 'Mechanism of Action', 'Citiations'].str.replace('[^\w\s]','')
35/16:
# Remove all duplicate words, and punctuations

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
data['text']= data['Disease name', 'Generic name', 'Brand name', 'Uses', 'Dosage Recommendation', 'Pharmakodynamics', 'Mechanism of Action', 'Citiations'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))
data['text'] = data['Disease name', 'Generic name', 'Brand name', 'Uses', 'Dosage Recommendation', 'Pharmakodynamics', 'Mechanism of Action', 'Citiations'].str.replace('[^\w\s]','')
35/17:
# Remove all duplicate words, and punctuations

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
data['text']= data['Disease name', 'Generic name', 'Brand name', 'Uses', 'Dosage Recommendation', 'Pharmakodynamics', 'Mechanism of Action', 'Citiations'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))
data['text'] = data['Disease name', 'Generic name', 'Brand name', 'Uses', 'Dosage Recommendation', 'Pharmakodynamics', 'Mechanism of Action', 'Citiations'].str.replace('[^\w\s]','')
35/18:
import nltk
from nltk.corpus import stopwords

# Assuming 'data' is your DataFrame
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Combine the relevant columns into one 'text' column
data['text'] = data['Disease name'] + ' ' + data['Generic name'] + ' ' + data['Brand name'] + ' ' + data['Uses'] + ' ' + data['Dosage Recommendation'] + ' ' + data['Pharmakodynamics'] + ' ' + data['Mechanism of Action'] + ' ' + data['Citiations']

# Remove all duplicate words
data['text'] = data['text'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))

# Remove punctuations
data['text'] = data['text'].str.replace('[^\w\s]', '')

# Remove stopwords
data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
35/19:
import nltk
from nltk.corpus import stopwords

# Assuming 'data' is your DataFrame
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Convert columns to strings and then combine them into a single 'text' column
data['text'] = data['Disease name'].astype(str) + ' ' + data['Generic name'].astype(str) + ' ' + data['Brand name'].astype(str) + ' ' + data['Uses'].astype(str) + ' ' + data['Dosage Recommendation'].astype(str) + ' ' + data['Pharmakodynamics'].astype(str) + ' ' + data['Mechanism of Action'].astype(str) + ' ' + data['Citiations'].astype(str)

# Remove all duplicate words
data['text'] = data['text'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))

# Remove punctuations
data['text'] = data['text'].str.replace('[^\w\s]', '')

# Remove stopwords
data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
35/20:
import nltk
from nltk.corpus import stopwords

# Assuming 'data' is your DataFrame
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Convert columns to strings and then combine them into a single 'text' column
data['text'] = data['Disease name'].astype(str) + ' ' + data['Generic name'].astype(str) + ' ' + data['Brand name'].astype(str) + ' ' + data['Uses'].astype(str) + ' ' + data['Dosage Recommendation'].astype(str) + ' ' + data['Pharmakodynamics'].astype(str) + ' ' + data['Mechanism of Action'].astype(str) + ' ' + data['Citiations'].astype(str)

# Remove all duplicate words
data['text'] = data['text'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))

# Remove punctuations
data['text'] = data['text'].str.replace('[^\w\s]', '')

# Remove stopwords
data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
print(data['text'])
35/21:
import nltk
from nltk.corpus import stopwords

# Assuming 'data' is your DataFrame
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Convert columns to strings and then combine them into a single 'text' column
data['text'] = data['Disease name'].astype(str) + ' ' + data['Generic name'].astype(str) + ' ' + data['Brand name'].astype(str) + ' ' + data['Uses'].astype(str) + ' ' + data['Dosage Recommendation'].astype(str) + ' ' + data['Pharmakodynamics'].astype(str) + ' ' + data['Mechanism of Action'].astype(str) + ' ' + data['Citiations'].astype(str)

# Remove all duplicate words
data['text'] = data['text'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))

# Remove punctuations
data['text'] = data['text'].str.replace('[^\w\s]', '')

# Remove stopwords
data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
35/22:
# Removing all duplicate words, punctuations, stopwords and extra white spaces

import nltk
from nltk.corpus import stopwords

# Assuming 'data' is your DataFrame
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Convert columns to strings and then combine them into a single 'text' column
data['text'] = data['Disease name'].astype(str) + ' ' + data['Generic name'].astype(str) + ' ' + data['Brand name'].astype(str) + ' ' + data['Uses'].astype(str) + ' ' + data['Dosage Recommendation'].astype(str) + ' ' + data['Pharmakodynamics'].astype(str) + ' ' + data['Mechanism of Action'].astype(str) + ' ' + data['Citiations'].astype(str)

# Remove all duplicate words
data['text'] = data['text'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))

# Remove punctuations
data['text'] = data['text'].str.replace('[^\w\s]', '')

# Remove stopwords
data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Remove all extra white spaces
data.text = data.text.apply(lambda x: x.strip())
data.to_csv('C:\Users\sound\OneDrive\Desktop\main project\python project 2\Disease-drug.csv', index=False)
35/23:
# Removing all duplicate words, punctuations, stopwords and extra white spaces

import nltk
from nltk.corpus import stopwords

# Assuming 'data' is your DataFrame
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Convert columns to strings and then combine them into a single 'text' column
data['text'] = data['Disease name'].astype(str) + ' ' + data['Generic name'].astype(str) + ' ' + data['Brand name'].astype(str) + ' ' + data['Uses'].astype(str) + ' ' + data['Dosage Recommendation'].astype(str) + ' ' + data['Pharmakodynamics'].astype(str) + ' ' + data['Mechanism of Action'].astype(str) + ' ' + data['Citiations'].astype(str)

# Remove all duplicate words
data['text'] = data['text'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))

# Remove punctuations
data['text'] = data['text'].str.replace('[^\w\s]', '')

# Remove stopwords
data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Remove all extra white spaces
data.text = data.text.apply(lambda x: x.strip())
data.to_csv("C:\Users\sound\OneDrive\Desktop\main project\python project 2\Disease-drug.csv", index=False)
35/24:
# Removing all duplicate words, punctuations, stopwords and extra white spaces

import nltk
from nltk.corpus import stopwords

# Assuming 'data' is your DataFrame
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Convert columns to strings and then combine them into a single 'text' column
data['text'] = data['Disease name'].astype(str) + ' ' + data['Generic name'].astype(str) + ' ' + data['Brand name'].astype(str) + ' ' + data['Uses'].astype(str) + ' ' + data['Dosage Recommendation'].astype(str) + ' ' + data['Pharmakodynamics'].astype(str) + ' ' + data['Mechanism of Action'].astype(str) + ' ' + data['Citiations'].astype(str)

# Remove all duplicate words
data['text'] = data['text'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))

# Remove punctuations
data['text'] = data['text'].str.replace('[^\w\s]', '')

# Remove stopwords
data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Remove all extra white spaces
data.text = data.text.apply(lambda x: x.strip())
data.to_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project 2\Disease-drug.csv", index=False)
35/25:
# Read the CSV file

import pandas as pd
from IPython.display import display

data = pd.read_excel(r"C:\Users\sound\OneDrive\Desktop\main project\python project 2\CANCER DRUF INFO.xlsx")
display(data)
35/26:
# Check for duplicate rows

duplicates = data.duplicated()
duplicate_rows = data[duplicates]
print(duplicate_rows)
df_cleaned = data.drop_duplicates()
display(df_cleaned)
35/27:
# Check for missing values

missing_values = data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
data_replace = data['Pharmakodynamics'].fillna('Not available', inplace=True)
display(data_replace)
35/28:
#Inspect Data Types

data_types = data.dtypes
display(data_types)
35/29:
# Data integration

import pandas as pd

data_combined = data['Generic name'] + ' (' + data['Brand name'] + ')'
display(data_combined)
35/30:
# Remove all special characters

import re

val = 'abckjdi#$9'
pattern = re.compile(r'[A-Za-z]+')
matches = pattern.finditer(val)
for mat in matches:
    print(mat)
35/31:
# Removing all duplicate words, punctuations, stopwords and extra white spaces

import nltk
from nltk.corpus import stopwords

# Assuming 'data' is your DataFrame
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Convert columns to strings and then combine them into a single 'text' column
data['text'] = data['Disease name'].astype(str) + ' ' + data['Generic name'].astype(str) + ' ' + data['Brand name'].astype(str) + ' ' + data['Uses'].astype(str) + ' ' + data['Dosage Recommendation'].astype(str) + ' ' + data['Pharmakodynamics'].astype(str) + ' ' + data['Mechanism of Action'].astype(str) + ' ' + data['Citiations'].astype(str)

# Remove all duplicate words
data['text'] = data['text'].apply(lambda x: ' '.join(dict.fromkeys(x.split())))

# Remove punctuations
data['text'] = data['text'].str.replace('[^\w\s]', '')

# Remove stopwords
data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Remove all extra white spaces
data.text = data.text.apply(lambda x: x.strip())
data.to_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project 2\Disease.drug.csv", index=False)
35/32: df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project 2\Disease.drug.csv", index=False)
35/33:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project 2\Disease.drug.csv", index=False)
35/34:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project 2\Disease.drug.csv")
35/35:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project 2\Disease.drug.csv")
display(df)
35/36:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project 2\Disease.drug.csv")
display(df)
print(data['text'])
35/37:
import nltk
from nltk.corpus import stopwords

# Assuming 'data' is your DataFrame
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# List of columns to clean
columns_to_clean = ['Disease name', 'Generic name', 'Brand name', 'Uses', 'Dosage Recommendation', 'Pharmakodynamics', 'Mechanism of Action', 'Citiations']

for column in columns_to_clean:
    # Convert the column to string
    data[column] = data[column].astype(str)
    
    # Remove all duplicate words
    data[column] = data[column].apply(lambda x: ' '.join(dict.fromkeys(x.split())))
    
    # Remove punctuations
    data[column] = data[column].str.replace('[^\w\s]', '')
    
    # Remove stopwords
    data[column] = data[column].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
    
    # Remove all extra white spaces
    data[column] = data[column].apply(lambda x: x.strip())

# Save the cleaned data to a CSV file
data.to_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project 2\diseases.csv", index=False)
37/1:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]
print(dataset_NO)

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
37/2:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
37/3:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
37/4:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]
print(dataset_NO)

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
37/5:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
37/6:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target.csv")
display(dataset)
37/7:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]
print(dataset_NO)

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
37/8:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability.'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]
print(dataset_NO)

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
37/9:
# Check for missing values

missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
37/10:
# Check for duplicate rows

duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
37/11:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
37/12:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
37/13:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
37/14:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]
print(dataset_NO)

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
37/15:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
37/16:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
37/17:
# Check for missing values

missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
37/18:
# Check for duplicate rows

duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
37/19:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
37/20:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
37/21:
# Scaling and Normalization

import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
37/22:
# Scaling and Normalization

import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
37/23:
# Label encoding

from sklearn.preprocessing import LabelEncoder

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
37/24:
# Visualizing target class dietribution

import matplotlib.pyplot as plt

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
37/25:
# Visualizing target class and Probability distribution

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
37/26:
# Splitting train and test data

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\drug targets\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
37/27:
# Splitting train and test data

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# importing data
df = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")

# head of the data
print(df.head())

# Assuming the actual column names are 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'
X = df[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
37/28:
# Perform label encoding on the training set

from sklearn.preprocessing import LabelEncoder

X_train_LE = X_train.copy()

label_encodings_train = {}

for col in X_train_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set)

# Perform label encoding on the testing set
X_test_LE = X_test.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set)
37/29:
# Scalimg for train and test set

from sklearn.preprocessing import StandardScaler

train_X = X_train_set
test_X = X_train_set
train_Y = y_train
test_Y = y_test

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
X_test_data = scaler.transform(test_Y.values.reshape(-1, 1))
Y_train_data = scaler.fit_transform(train_Y.values.reshape(-1, 1))
Y_test_data = scaler.transform(test_Y.values.reshape(-1, 1))

display(X_train_data, "\n", X_test_data)
display(Y_train_data, "\n", Y_test_data)
37/30:
# Build a decision tree regressor

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Decision tree MSE: {mse}')
37/31:
# Cross Validation

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(dt_regressor, X_train_data, y_train, cv=5, scoring='neg_mean_squared_error')
mse_cv = -cv_scores.mean()
print(mse_cv)
37/32:
# Checking with other models

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

# Random Forest Regressor

rf_model = RandomForestRegressor()
rf_model.fit(X_train_data, y_train)
rf_predictions = rf_model.predict(X_test_data)
rf_mse = mean_squared_error(y_test, rf_predictions)

# Gradient Boosting Regressor

gb_model = GradientBoostingRegressor()
gb_model.fit(X_train_data, y_train)
gb_predictions = gb_model.predict(X_test_data)
gb_mse = mean_squared_error(y_test, gb_predictions)

# Build a Support vector machines

svr_regressor = SVR()
svr_regressor.fit(X_train_data, y_train.values.ravel())
svr_pred = svr_regressor.predict(X_test_data)
mse_svr = mean_squared_error(y_test, svr_pred)

# Compare the performance
print(f"Random Forest MSE: {rf_mse}")
print(f"Gradient Boosting MSE: {gb_mse}")
print(f"Support Vector Machine MSE: {mse_svr}")
37/33:
# Create a tree-like data structure using a dictionary

from graphviz import Digraph

tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
display(tree_data)
37/34:
# Visualizing tree data

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.show()
37/35:
# Scaling and Normalization

import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset_NO['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset_NO['Probability*'].values)
print(scaled_probabilities)
37/36:
# Scaling and Normalization

import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset_NO['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data
37/37:
# Scaling and Normalization

import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset_NO['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)
37/38:
# Scaling and Normalization

import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset_NO['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data
37/39:
# Label encoding

from sklearn.preprocessing import LabelEncoder

encoded_labels_1 = dataset_NO['Target Class']  
label_encoder = LabelEncoder()
df_1 = label_encoder.fit_transform(encoded_labels_1)
print(df_1)
encoded_labels_2 = dataset_NO['Target']  
label_encoder = LabelEncoder()
df_2 = label_encoder.fit_transform(encoded_labels_2)
print(df_2)
37/40:
# Visualizing target class dietribution

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2  2  2  3  3
  3  3  3  3  3  3  3  3  3  3  4  5  6  6  6  6  6  6  6  6  6  6  6  6
  6  6  6  6  6  6  6  6  6  6  7  8  9 10 11 11 12 12 13 14 14 15 15 15
 15 15 15 15 15 15 15 15 15 15 15 15 16 17 18 18 18 19 19 19 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
37/41:
# Visualizing target class dietribution

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
37/42:
# Dataset with outliers

import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset_NO['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data

# Dataset without outliers

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
37/43:
# Dataset with outliers

import pandas as pd
from IPython.display import display
import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset_NO['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data

# Dataset without outliers

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
print(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
print(scaled_probabilities)
37/44:
# Dataset with outliers

import pandas as pd
from IPython.display import display
import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset_NO['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
display(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data

# Dataset without outliers

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
display(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
display(scaled_probabilities)
37/45:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

encoded_labels_1 = dataset_NO['Target Class']  
label_encoder = LabelEncoder()
df_1 = label_encoder.fit_transform(encoded_labels_1)
print(df_1)
encoded_labels_2 = dataset_NO['Target']  
label_encoder = LabelEncoder()
df_2 = label_encoder.fit_transform(encoded_labels_2)
print(df_2)

# Dataset without outliers

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
37/46:
# Dataset with outliers

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
37/47:
# Dataset with outliers

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITH OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITHOUT OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
37/48:
# Dataset with outliers

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITH OUTLIERS: TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
37/49:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())

# Dataset without outliers

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train : ')
print(X_train.head())
print('')
print('X_test : ')
print(X_test.head())
print('')
print('y_train : ')
print(y_train.head())
print('')
print('y_test : ')
print(y_test.head())
37/50:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train : ')
display(X_train.head())
display('X_test : ')
display(X_test.head())
display('y_train : ')
display(y_train.head())
display('y_test : ')
display(y_test.head())

# Dataset without outliers

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train : ')
display(X_train.head())
display('X_test : ')
display(X_test.head())
display('y_train : ')
display(y_train.head())
display('y_test : ')
display(y_test.head())
37/51:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train : ')
display(X_train.head())
display('X_test : ')
display(X_test.head())
display('y_train : ')
display(y_train.head())
display('y_test : ')
display(y_test.head())

# Dataset without outliers

print("Dataset without outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train : ')
display(X_train.head())
display('X_test : ')
display(X_test.head())
display('y_train : ')
display(y_train.head())
display('y_test : ')
display(y_test.head())
37/52:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset without outliers

print("Dataset without outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
37/53:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset with outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset without outliers

print("Dataset without outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
37/54:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder
"\n"
print("Dataset with outliers")
"\n"

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset without outliers
"\n"
print("Dataset without outliers")
"\n"
X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
37/55:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display(X_train_data1, "\n", X_test_data1)
display(Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display(X_train_data2, "\n", X_test_data2)
display(Y_train_data2, "\n", Y_test_data2)
38/1:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
38/2:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
38/3:
# Check for missing values

missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
38/4:
# Check for duplicate rows

duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
38/5:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
38/6:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
38/7:
# Dataset with outliers

import pandas as pd
from IPython.display import display
import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset_NO['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
display(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data

# Dataset without outliers

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
display(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
display(scaled_probabilities)
38/8:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

encoded_labels_1 = dataset_NO['Target Class']  
label_encoder = LabelEncoder()
df_1 = label_encoder.fit_transform(encoded_labels_1)
print(df_1)
encoded_labels_2 = dataset_NO['Target']  
label_encoder = LabelEncoder()
df_2 = label_encoder.fit_transform(encoded_labels_2)
print(df_2)

# Dataset without outliers

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
38/9:
# Dataset with outliers

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITH OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITHOUT OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
38/10:
# Dataset with outliers

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITH OUTLIERS: TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
38/11:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset without outliers

print("Dataset without outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
38/12:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset with outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset without outliers

print("Dataset without outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
38/13:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display(X_train_data1, "\n", X_test_data1)
display(Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display(X_train_data2, "\n", X_test_data2)
display(Y_train_data2, "\n", Y_test_data2)
38/14:
# Build a decision tree regressor

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data, y_train)
y_pred = dt_regressor.predict(X_test_data)
mse = mean_squared_error(y_test, y_pred)
print(f'Decision tree MSE: {mse}')
38/15:
# Dataset with outliers

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data1, y_train1)
y_pred1 = dt_regressor.predict(X_test_data1)
mse = mean_squared_error(y_test1, y_pred1)
print(f'Decision tree MSE-1: {mse}')

# Dataset without outliers

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data2, y_train2)
y_pred2 = dt_regressor.predict(X_test_data2)
mse = mean_squared_error(y_test2, y_pred2)
print(f'Decision tree MSE-2: {mse}')
38/16:
# Dataset with outliers

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data1, y_train1)
y_pred1 = dt_regressor.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_df1 = pd.DataFrame({
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})

print(results_df1)

# Dataset without outliers

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data2, y_train2)
y_pred2 = dt_regressor.predict(X_test_data2)
mse = mean_squared_error(y_test2, y_pred2)
print(f'Decision tree MSE-2: {mse}')
38/17:
# Dataset with outliers

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

print("Dataset with outliers")

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data1, y_train1)
y_pred1 = dt_regressor.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_df1 = pd.DataFrame({
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})

print(results_df1)

# Dataset without outliers

print("Dataset without outliers")

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train_data2, y_train2)
y_pred2 = dt_regressor.predict(X_test_data2)
mse = mean_squared_error(y_test2, y_pred2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)

results_df2 = pd.DataFrame({
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})

print(results_df2)
40/1:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores.mean()
print(mse_cv1)

# Dataset without outliers

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores.mean()
print(mse_cv2)
40/2:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
40/3:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
40/4:
# Check for missing values

missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
40/5:
# Check for duplicate rows

duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
40/6:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
40/7:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
40/8:
# Dataset with outliers

import pandas as pd
from IPython.display import display
import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset_NO['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
display(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data

# Dataset without outliers

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
display(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
display(scaled_probabilities)
40/9:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

encoded_labels_1 = dataset_NO['Target Class']  
label_encoder = LabelEncoder()
df_1 = label_encoder.fit_transform(encoded_labels_1)
print(df_1)
encoded_labels_2 = dataset_NO['Target']  
label_encoder = LabelEncoder()
df_2 = label_encoder.fit_transform(encoded_labels_2)
print(df_2)

# Dataset without outliers

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
40/10:
# Dataset with outliers

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITH OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITHOUT OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
40/11:
# Dataset with outliers

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITH OUTLIERS: TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
40/12:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset without outliers

print("Dataset without outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
40/13:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset with outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset without outliers

print("Dataset without outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
40/14:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display(X_train_data1, "\n", X_test_data1)
display(Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display(X_train_data2, "\n", X_test_data2)
display(Y_train_data2, "\n", Y_test_data2)
40/15:
# Dataset with outliers

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
y_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_df1 = pd.DataFrame({
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})

print(results_df1)

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
y_pred2 = dt_regressor2.predict(X_test_data2)
mse = mean_squared_error(y_test2, y_pred2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)

results_df2 = pd.DataFrame({
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})

print(results_df2)
40/16:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring1='neg_mean_squared_error')
mse_cv1 = -cv_scores.mean()
print(mse_cv1)

# Dataset without outliers

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring2='neg_mean_squared_error')
mse_cv2 = -cv_scores.mean()
print(mse_cv2)
40/17:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores.mean()
print(mse_cv1)

# Dataset without outliers

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores.mean()
print(mse_cv2)
40/18:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)

# Dataset without outliers

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
40/19:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

print("Dataset with outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)

# Dataset without outliers

print("Dataset with outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
40/20:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

print("Dataset with outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)

# Dataset without outliers

print("Dataset without outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
40/21:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

# Dataset with outliers

print("Dataset without outliers")
40/22:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:" 
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)



# Dataset with outliers

print("Dataset without outliers")
40/23:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:" 
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)



# Dataset with outliers

print("Dataset without outliers")
40/24:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:" 
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model1)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)



# Dataset with outliers

print("Dataset without outliers")
40/25:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:" "\n"
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model1)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)



# Dataset with outliers

print("Dataset without outliers")
40/26:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:" " "
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model1)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)



# Dataset with outliers

print("Dataset without outliers")
40/27:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model1)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)



# Dataset with outliers

print("Dataset without outliers")
40/28:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model1, ' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)



# Dataset with outliers

print("Dataset without outliers")
40/29:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:" ' '
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model1,)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)



# Dataset with outliers

print("Dataset without outliers")
40/30:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model1,)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)
results_gb_model1 = pd.DataFrame({ "gb_model1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_gb_model1)
# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)



# Dataset with outliers

print("Dataset without outliers")
40/31:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model1,)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)
results_gb_model1 = pd.DataFrame({ "gb_model1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_gb_model1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)
results_svr_regressor1 = pd.DataFrame({ "svr_regressor1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_svr_regressor1)


# Dataset with outliers

print("Dataset without outliers")
40/32:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model1)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)
results_gb_model1 = pd.DataFrame({ "gb_model1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_gb_model1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)
results_svr_regressor1 = pd.DataFrame({ "svr_regressor1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_svr_regressor1)


# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)

results_rf_model2 = pd.DataFrame({ "rf_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model2)

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)
results_gb_model2 = pd.DataFrame({ "gb_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_gb_model2)

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)
results_svr_regressor2 = pd.DataFrame({ "svr_regressor2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_svr_regressor2)
40/33:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({ "rf_model1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model1)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)
results_gb_model1 = pd.DataFrame({ "gb_model1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_gb_model1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)
results_svr_regressor1 = pd.DataFrame({ "svr_regressor1:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_svr_regressor1)


# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)

results_rf_model2 = pd.DataFrame({ "rf_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model2)

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)
results_gb_model2 = pd.DataFrame({ "gb_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_gb_model2)

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, y_pred2))
mse = mean_squared_error(y_test2, y_pred2)
results_svr_regressor2 = pd.DataFrame({ "svr_regressor2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_svr_regressor2)
40/34:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_gb_model1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, y_pred1)
rmse = mean_squared_error(y_test1, y_pred1, squared=False)
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_svr_regressor1)


# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)

results_rf_model2 = pd.DataFrame({ "rf_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model2)

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)
results_gb_model2 = pd.DataFrame({ "gb_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_gb_model2)

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, y_pred2))
mse = mean_squared_error(y_test2, y_pred2)
results_svr_regressor2 = pd.DataFrame({ "svr_regressor2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_svr_regressor2)
40/35:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)


# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)

results_rf_model2 = pd.DataFrame({ "rf_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model2)

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)
results_gb_model2 = pd.DataFrame({ "gb_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_gb_model2)

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, y_pred2))
mse = mean_squared_error(y_test2, y_pred2)
results_svr_regressor2 = pd.DataFrame({ "svr_regressor2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_svr_regressor2)
40/36:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)

results_rf_model2 = pd.DataFrame({ "rf_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model2)

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)
results_gb_model2 = pd.DataFrame({ "gb_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_gb_model2)

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, y_pred2))
mse = mean_squared_error(y_test2, y_pred2)
results_svr_regressor2 = pd.DataFrame({ "svr_regressor2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_svr_regressor2)
40/37:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)

results_rf_model2 = pd.DataFrame({ "rf_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_rf_model2)

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = mean_absolute_error(y_test2, y_pred2)
mse = mean_squared_error(y_test2, y_pred2)
results_gb_model2 = pd.DataFrame({ "gb_model2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_gb_model2)

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, y_pred2)
rmse = mean_squared_error(y_test2, y_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, y_pred2))
mse = mean_squared_error(y_test2, y_pred2)
results_svr_regressor2 = pd.DataFrame({ "svr_regressor2:"
                                  
    'R2': [r2],
    'RMSE': [rmse],
    'MAE': [mae],
    'MSE': [mse]
})
print(results_svr_regressor2)
40/38:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_regressor2)
rmse = mean_squared_error(y_test2, svr_regressor2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_regressor2))
mse = mean_squared_error(y_test2, svr_regressor2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_regressor2)],
    'RMSE': [mean_squared_error(y_test2, svr_regressor2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_regressor2)],
    'MSE': [mean_squared_error(y_test2, svr_regressor2)]
})
print(results_svr_regressor2)
40/39:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_regressor2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_regressor2))
mse = mean_squared_error(y_test2, svr_regressor2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_regressor2)],
    'RMSE': [mean_squared_error(y_test2, svr_regressor2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_regressor2)],
    'MSE': [mean_squared_error(y_test2, svr_regressor2)]
})
print(results_svr_regressor2)
print(' ')
40/40:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
40/41:
results_combined = pd.concat([results_df1, results_df2, 
                              results_rf_model1, results_gb_model1, results_svr_regressor1, 
                              results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_sorted = results_combined.sort_values(by='RMSE')

print(results_sorted)
40/42:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

print("Dataset with outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)

# Dataset without outliers

print("Dataset without outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
40/43:
# Dataset with outliers

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test1, dt_pred2)],
    'RMSE': [mean_squared_error(y_test1, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred2)],
    'MSE': [mean_squared_error(y_test1, dt_pred2)]
})

print(results_dt_model2)
40/44:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(results_dt_model1)

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(results_dt_model2)
40/45:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(' ')
print(results_dt_model1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(' ')
print(results_dt_model2)
40/46:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

print("Dataset with outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)

# Dataset without outliers

print("Dataset without outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
40/47:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

print("Dataset with outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
40/48:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
40/49:
results_combined = pd.concat([results_dt_model1, results_dt_model2, 
                              results_rf_model1, results_gb_model1, results_svr_regressor1, 
                              results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_sorted = results_combined.sort_values(by='RMSE')

print(results_sorted)
40/50:
results_combined = pd.concat([results_dt_model1, results_dt_model2, 
                             results_rf_model1, results_rf_model2, 
                             results_gb_model1, results_gb_model2,
                             results_svr_regressor1, results_svr_regressor2], ignore_index=True)

# Find the best model for each dataset
best_model_with_outliers = results_combined[results_combined['Model'] != 'Decision Tree'].sort_values(by=['RMSE']).iloc[0]
best_model_without_outliers = results_combined[results_combined['Model'] != 'Decision Tree'].sort_values(by=['RMSE']).iloc[1]

print(f"The best model for dataset with outliers is {best_model_with_outliers['Model']} with RMSE: {best_model_with_outliers['RMSE']}")
print(f"The best model for dataset without outliers is {best_model_without_outliers['Model']} with RMSE: {best_model_without_outliers['RMSE']}")
40/51:
results_combined = pd.concat([results_dt_model1, results_dt_model2, 
                             results_rf_model1, results_rf_model2, 
                             results_gb_model1, results_gb_model2,
                             results_svr_regressor1, results_svr_regressor2], ignore_index=True)

# Find the best model for each dataset
best_model_with_outliers = results_combined[results_combined['Model'] != 'Decision Tree'].sort_values(by=['MSE']).iloc[0]
best_model_without_outliers = results_combined[results_combined['Model'] != 'Decision Tree'].sort_values(by=['MSE']).iloc[1]

print(f"The best model for dataset with outliers is {best_model_with_outliers['Model']} with RMSE: {best_model_with_outliers['MSE']}")
print(f"The best model for dataset without outliers is {best_model_without_outliers['Model']} with RMSE: {best_model_without_outliers['MSE']}")
40/52:
results_combined = pd.concat([results_dt_model1, results_dt_model2, 
                              results_rf_model1, results_gb_model1, results_svr_regressor1, 
                              results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_sorted = results_combined.sort_values(by='MSE')

print(results_sorted)
40/53:
import pygraphviz as pgv
import networkx as nx
from networkx.drawing.nx_agraph import graphviz_layout

def plot_tree_as_flowchart(data, parent=None, graph=None):
    if graph is None:
        graph = pgv.AGraph(directed=True)

    for key, value in data.items():
        if isinstance(value, dict):
            graph.add_node(key, label=key)
            if parent is not None:
                graph.add_edge(parent, key)
            plot_tree_as_flowchart(value, parent=key, graph=graph)
        else:
            graph.add_node(f"{key}: {value[0]}", label=f"{key}: {value[0]}", shape='box')
            graph.add_edge(parent, f"{key}: {value[0]}", label=key)

    return graph

# Assuming `tree_data` contains your hierarchical data
tree_graph = plot_tree_as_flowchart(tree_data)

# Draw the graph
pos = graphviz_layout(tree_graph, prog='dot')
nx.draw(tree_graph, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold')
edge_labels = {(u, v): d['label'] for u, v, d in tree_graph.edges(data=True)}
nx.draw_networkx_edge_labels(tree_graph, pos, edge_labels=edge_labels, font_size=8)

plt.show()
40/54:
import networkx as nx
import matplotlib.pyplot as plt

def plot_tree_as_flowchart(data, parent=None, graph=None):
    if graph is None:
        graph = nx.DiGraph()

    for key, value in data.items():
        if isinstance(value, dict):
            graph.add_node(key, label=key)
            if parent is not None:
                graph.add_edge(parent, key)
            plot_tree_as_flowchart(value, parent=key, graph=graph)
        else:
            graph.add_node(f"{key}: {value[0]}", label=f"{key}: {value[0]}", shape='box')
            graph.add_edge(parent, f"{key}: {value[0]}", label=key)

    return graph

# Assuming `tree_data` contains your hierarchical data
tree_graph = plot_tree_as_flowchart(tree_data)

# Draw the graph
pos = nx.multipartite_layout(tree_graph, subset_key="layer")
nx.draw(tree_graph, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold')

edge_labels = {(u, v): d['label'] for u, v, d in tree_graph.edges(data=True)}
nx.draw_networkx_edge_labels(tree_graph, pos, edge_labels=edge_labels, font_size=8)

plt.show()
40/55:
# Visualizing tree data

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.show()
40/56:
# Create a tree-like data structure using a dictionary

from graphviz import Digraph

tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
display(tree_data)
40/57:
# Visualizing tree data

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.show()
40/58:
import networkx as nx
import matplotlib.pyplot as plt

def plot_tree_as_flowchart(data, parent=None, graph=None):
    if graph is None:
        graph = nx.DiGraph()

    for key, value in data.items():
        if isinstance(value, dict):
            graph.add_node(key, label=key)
            if parent is not None:
                graph.add_edge(parent, key)
            plot_tree_as_flowchart(value, parent=key, graph=graph)
        else:
            graph.add_node(f"{key}: {value[0]}", label=f"{key}: {value[0]}", shape='box')
            graph.add_edge(parent, f"{key}: {value[0]}", label=key)

    return graph

# Assuming `tree_data` contains your hierarchical data
tree_graph = plot_tree_as_flowchart(tree_data)

# Draw the graph
pos = nx.multipartite_layout(tree_graph, subset_key="layer")
nx.draw(tree_graph, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold')

edge_labels = {(u, v): d['label'] for u, v, d in tree_graph.edges(data=True)}
nx.draw_networkx_edge_labels(tree_graph, pos, edge_labels=edge_labels, font_size=8)

plt.show()
40/59:
import networkx as nx
import matplotlib.pyplot as plt

def plot_tree_as_flowchart(data, parent=None, graph=None):
    if graph is None:
        graph = nx.DiGraph()

    for key, value in data.items():
        if isinstance(value, dict):
            graph.add_node(key, label=key)
            if parent is not None:
                graph.add_edge(parent, key)
            plot_tree_as_flowchart(value, parent=key, graph=graph)
        else:
            graph.add_node(f"{key}: {value[0]}", label=f"{key}: {value[0]}", shape='box')
            graph.add_edge(parent, f"{key}: {value[0]}", label=key)

    return graph

# Assuming `tree_data` contains your hierarchical data
tree_graph = plot_tree_as_flowchart(tree_data)

# Draw the graph
pos = nx.spring_layout(tree_graph, seed=42)  # Changed the layout method to spring_layout
nx.draw(tree_graph, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold')

edge_labels = {(u, v): d['label'] for u, v, d in tree_graph.edges(data=True)}
nx.draw_networkx_edge_labels(tree_graph, pos, edge_labels=edge_labels, font_size=8)

plt.show()
40/60:
import networkx as nx
import matplotlib.pyplot as plt

def plot_tree_as_flowchart(data, parent=None, graph=None):
    if graph is None:
        graph = nx.DiGraph()

    for key, value in data.items():
        if isinstance(value, dict):
            graph.add_node(key, label=key)
            if parent is not None:
                graph.add_edge(parent, key)
            plot_tree_as_flowchart(value, parent=key, graph=graph)
        else:
            graph.add_node(f"{key}: {value[0]}", label=f"{key}: {value[0]}", shape='box')
            graph.add_edge(parent, f"{key}: {value[0]}", label=key)

    return graph

# Assuming `tree_data` contains your hierarchical data
tree_graph = plot_tree_as_flowchart(tree_data)

# Draw the graph with 'dot' layout
pos = nx.nx_agraph.graphviz_layout(tree_graph, prog='dot')
nx.draw(tree_graph, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold')

edge_labels = {(u, v): d['label'] for u, v, d in tree_graph.edges(data=True)}
nx.draw_networkx_edge_labels(tree_graph, pos, edge_labels=edge_labels, font_size=8)

plt.show()
40/61:
import networkx as nx
import matplotlib.pyplot as plt
import pydot

def plot_tree_as_flowchart(data, parent=None, graph=None):
    if graph is None:
        graph = nx.DiGraph()

    for key, value in data.items():
        if isinstance(value, dict):
            graph.add_node(key, label=key)
            if parent is not None:
                graph.add_edge(parent, key)
            plot_tree_as_flowchart(value, parent=key, graph=graph)
        else:
            graph.add_node(f"{key}: {value[0]}", label=f"{key}: {value[0]}", shape='box')
            graph.add_edge(parent, f"{key}: {value[0]}", label=key)

    return graph

# Assuming `tree_data` contains your hierarchical data
tree_graph = plot_tree_as_flowchart(tree_data)

# Draw the graph with hierarchial layout
pos = pydot.graph_from_dot(nx.nx_pydot.to_dot(tree_graph))
nx.draw(tree_graph, pos=pos[0], with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold')

edge_labels = {(u, v): d['label'] for u, v, d in tree_graph.edges(data=True)}
nx.draw_networkx_edge_labels(tree_graph, pos=pos[0], edge_labels=edge_labels, font_size=8)

plt.show()
40/62:
import networkx as nx
import matplotlib.pyplot as plt
import pydot

def plot_tree_as_flowchart(data, parent=None, graph=None):
    if graph is None:
        graph = nx.DiGraph()

    for key, value in data.items():
        if isinstance(value, dict):
            graph.add_node(key, label=key)
            if parent is not None:
                graph.add_edge(parent, key)
            plot_tree_as_flowchart(value, parent=key, graph=graph)
        else:
            graph.add_node(f"{key}: {value[0]}", label=f"{key}: {value[0]}", shape='box')
            graph.add_edge(parent, f"{key}: {value[0]}", label=key)

    return graph

# Assuming `tree_data` contains your hierarchical data
tree_graph = plot_tree_as_flowchart(tree_data)

# Draw the graph with hierarchial layout
pos = pydot.Dot(graph_type='digraph', rankdir='LR')
for node in tree_graph.nodes():
    new_node = pydot.Node(node, label=tree_graph.nodes[node]['label'], shape=tree_graph.nodes[node]['shape'])
    pos.add_node(new_node)
for edge in tree_graph.edges():
    pos.add_edge(pydot.Edge(edge[0], edge[1], label=tree_graph.get_edge_data(edge[0], edge[1])['label']))

pos.write_png('tree.png')  # Save to a file
40/63:
import networkx as nx
import matplotlib.pyplot as plt

def plot_tree_as_flowchart(data, parent=None, graph=None):
    if graph is None:
        graph = nx.DiGraph()

    for key, value in data.items():
        if isinstance(value, dict):
            graph.add_node(key, label=key)
            if parent is not None:
                graph.add_edge(parent, key)
            plot_tree_as_flowchart(value, parent=key, graph=graph)
        else:
            graph.add_node(f"{key}: {value[0]}", label=f"{key}: {value[0]}")
            graph.add_edge(parent, f"{key}: {value[0]}", label=key)

    return graph

# Assuming `tree_data` contains your hierarchical data
tree_graph = plot_tree_as_flowchart(tree_data)

# Draw the graph
pos = nx.nx_agraph.graphviz_layout(tree_graph, prog='dot')
nx.draw(tree_graph, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold')

edge_labels = {(u, v): d['label'] for u, v, d in tree_graph.edges(data=True)}
nx.draw_networkx_edge_labels(tree_graph, pos, edge_labels=edge_labels, font_size=8)

plt.show()
41/1:
results_combined = pd.concat([results_dt_model1, results_dt_model2, 
                             results_rf_model1, results_rf_model2, 
                             results_gb_model1, results_gb_model2,
                             results_svr_regressor1, results_svr_regressor2], ignore_index=True)

# Find the best model for each dataset
best_model_with_outliers = results_combined[results_combined['Model'] != 'Decision Tree'].sort_values(by=['RMSE']).iloc[0]
best_model_without_outliers = results_combined[results_combined['Model'] != 'Decision Tree'].sort_values(by=['RMSE']).iloc[1]

print(f"The best model for dataset with outliers is {best_model_with_outliers['Model']} with RMSE: {best_model_with_outliers['RMSE']}")
print(f"The best model for dataset without outliers is {best_model_without_outliers['Model']} with RMSE: {best_model_without_outliers['RMSE']}")
41/2:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
41/3:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
41/4:
# Check for missing values

missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
41/5:
# Check for duplicate rows

duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
41/6:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
41/7:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
41/8:
# Dataset with outliers

import pandas as pd
from IPython.display import display
import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset_NO['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
display(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data

# Dataset without outliers

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
display(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
display(scaled_probabilities)
41/9:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

encoded_labels_1 = dataset_NO['Target Class']  
label_encoder = LabelEncoder()
df_1 = label_encoder.fit_transform(encoded_labels_1)
print(df_1)
encoded_labels_2 = dataset_NO['Target']  
label_encoder = LabelEncoder()
df_2 = label_encoder.fit_transform(encoded_labels_2)
print(df_2)

# Dataset without outliers

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
41/10:
# Dataset with outliers

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITH OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITHOUT OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
41/11:
# Dataset with outliers

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITH OUTLIERS: TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
41/12:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset without outliers

print("Dataset without outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
41/13:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset with outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset without outliers

print("Dataset without outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
41/14:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display(X_train_data1, "\n", X_test_data1)
display(Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display(X_train_data2, "\n", X_test_data2)
display(Y_train_data2, "\n", Y_test_data2)
41/15:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(' ')
print(results_dt_model1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(' ')
print(results_dt_model2)
41/16:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

print("Dataset with outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
41/17:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
41/18:
results_combined1 = pd.concat([results_dt_model1, results_dt_model2, 
                              results_rf_model1, results_gb_model1, results_svr_regressor1, 
                              results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_combined2 = pd.concat([results_dt_model1, results_dt_model2, 
                              results_rf_model1, results_gb_model1, results_svr_regressor1, 
                              results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)
results_sorted = results_combined1.sort_values(by='RMSE')

print(results_sorted)
41/19:
results_combined = pd.concat([results_dt_model1, results_dt_model2, 
                             results_rf_model1, results_rf_model2, 
                             results_gb_model1, results_gb_model2,
                             results_svr_regressor1, results_svr_regressor2], ignore_index=True)

# Find the best model for each dataset
best_model_with_outliers = results_combined[results_combined['Model'] != 'Decision Tree'].sort_values(by=['RMSE']).iloc[0]
best_model_without_outliers = results_combined[results_combined['Model'] != 'Decision Tree'].sort_values(by=['RMSE']).iloc[1]

print(f"The best model for dataset with outliers is {best_model_with_outliers['Model']} with RMSE: {best_model_with_outliers['RMSE']}")
print(f"The best model for dataset without outliers is {best_model_without_outliers['Model']} with RMSE: {best_model_without_outliers['RMSE']}")
41/20:
# Create a tree-like data structure using a dictionary

from graphviz import Digraph

tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
display(tree_data)
41/21:
# Visualizing tree data

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.show()
41/22:
results_combined1 = pd.concat([results_dt_model1,  
                              results_rf_model1, results_gb_model1, results_svr_regressor1], ignore_index=True)

results_combined2 = pd.concat([results_dt_model2, results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_sorted1 = results_combined1.sort_values(by='RMSE')
results_sorted2 = results_combined2.sort_values(by='RMSE')

print(results_sorted1)
print(' ')
print(results_sorted2)
41/23:
results_combined = pd.concat([results_dt_model1, results_dt_model2, 
                             results_rf_model1, results_rf_model2, 
                             results_gb_model1, results_gb_model2,
                             results_svr_regressor1, results_svr_regressor2], ignore_index=True)

# Find the best model for each dataset
best_model_with_outliers = results_combined[results_combined['Model'] != 'Decision Tree'].sort_values(by=['RMSE']).iloc[0]
best_model_without_outliers = results_combined[results_combined['Model'] != 'Decision Tree'].sort_values(by=['RMSE']).iloc[1]

print(f"The best model for dataset with outliers is {best_model_with_outliers['Model']} with RMSE: {best_model_with_outliers['RMSE']}")
print(f"The best model for dataset without outliers is {best_model_without_outliers['Model']} with RMSE: {best_model_without_outliers['RMSE']}")
41/24:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
41/25:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
41/26:
# Check for missing values

missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
41/27:
# Check for duplicate rows

duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
41/28:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
41/29:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
41/30:
# Dataset with outliers

import pandas as pd
from IPython.display import display
import numpy as np
from sklearn.preprocessing import StandardScaler


probs = np.array(dataset_NO['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
display(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data

# Dataset without outliers

probs = np.array(dataset['Probability*'])
probs_2d = probs.reshape(-1, 1)
scaler = StandardScaler()
scaled_probs = scaler.fit_transform(probs_2d)
display(scaled_probs)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
display(scaled_probabilities)
41/31:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

encoded_labels_1 = dataset_NO['Target Class']  
label_encoder = LabelEncoder()
df_1 = label_encoder.fit_transform(encoded_labels_1)
print(df_1)
encoded_labels_2 = dataset_NO['Target']  
label_encoder = LabelEncoder()
df_2 = label_encoder.fit_transform(encoded_labels_2)
print(df_2)

# Dataset without outliers

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
41/32:
# Dataset with outliers

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITH OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITHOUT OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
41/33:
# Dataset with outliers

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITH OUTLIERS: TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
41/34:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset without outliers

print("Dataset without outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
41/35:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset with outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset without outliers

print("Dataset without outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
41/36:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display(X_train_data1, "\n", X_test_data1)
display(Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display(X_train_data2, "\n", X_test_data2)
display(Y_train_data2, "\n", Y_test_data2)
41/37:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(' ')
print(results_dt_model1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(' ')
print(results_dt_model2)
41/38:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

print("Dataset with outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
41/39:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
41/40:
results_combined1 = pd.concat([results_dt_model1,  
                              results_rf_model1, results_gb_model1, results_svr_regressor1], ignore_index=True)

results_combined2 = pd.concat([results_dt_model2, results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_sorted1 = results_combined1.sort_values(by='RMSE')
results_sorted2 = results_combined2.sort_values(by='RMSE')

print(results_sorted1)
print(' ')
print(results_sorted2)
41/41:
# Create a tree-like data structure using a dictionary

from graphviz import Digraph

tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
display(tree_data)
41/42:
# Visualizing tree data

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.show()
41/43:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display(X_train_data1, "\n", X_test_data1)
display(Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data

Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))


display(Y_train_data2, "\n", Y_test_data2)
41/44:
# Dataset with outliers

import pandas as pd
from IPython.display import display
import numpy as np
from sklearn.preprocessing import StandardScaler


probs1 = np.array(dataset_NO['Probability*'])
probs_2d = probs1.reshape(-1, 1)
scaler = StandardScaler()
Probability1 = scaler.fit_transform(probs_2d)
display(Probability1)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data

# Dataset without outliers

probs2 = np.array(dataset['Probability*'])
probs_2d = probs2.reshape(-1, 1)
scaler = StandardScaler()
Probability2 = scaler.fit_transform(probs_2d)
display(Probability2)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
display(scaled_probabilities)
41/45:
# Dataset with outliers

import pandas as pd
from IPython.display import display
import numpy as np
from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

probs1 = np.array(dataset_NO['Probability*'])
probs_2d = probs1.reshape(-1, 1)
scaler = StandardScaler()
Probability1 = scaler.fit_transform(probs_2d)
display(Probability1)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data

# Dataset without outliers

print("Dataset without outliers")

probs2 = np.array(dataset['Probability*'])
probs_2d = probs2.reshape(-1, 1)
scaler = StandardScaler()
Probability2 = scaler.fit_transform(probs_2d)
display(Probability2)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
display(scaled_probabilities)
41/46:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = Probability1

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset without outliers

print("Dataset without outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = Probability2

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
41/47:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert NumPy arrays to pandas DataFrames
X_train1_df = pd.DataFrame(X_train1, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
X_test1_df = pd.DataFrame(X_test1, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
y_train1_df = pd.DataFrame(y_train1, columns=['Probability1'])
y_test1_df = pd.DataFrame(y_test1, columns=['Probability1'])

# Now you can use the head() method
display('X_train1 : ')
display(X_train1_df.head())
display('X_test1 : ')
display(X_test1_df.head())
display('y_train1 : ')
display(y_train1_df.head())
display('y_test1 : ')
display(y_test1_df.head())

# Dataset without outliers

print("Dataset without outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = Probability2

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
41/48:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert NumPy arrays to pandas DataFrames
X_train1_df = pd.DataFrame(X_train1, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
X_test1_df = pd.DataFrame(X_test1, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
y_train1_df = pd.DataFrame(y_train1, columns=['Probability1'])
y_test1_df = pd.DataFrame(y_test1, columns=['Probability1'])

# Now you can use the head() method
display('X_train1 : ')
display(X_train1_df.head())
display('X_test1 : ')
display(X_test1_df.head())
display('y_train1 : ')
display(y_train1_df.head())
display('y_test1 : ')
display(y_test1_df.head())

# Dataset without outliers

print("Dataset without outliers")

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

X_train2_df = pd.DataFrame(X_train2, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
X_test2_df = pd.DataFrame(X_test2, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
y_train2_df = pd.DataFrame(y_train2, columns=['Probability2'])
y_test2_df = pd.DataFrame(y_test2, columns=['Probability2'])

display('X_train2 : ')
display(X_train2_df.head())
display('X_test2 : ')
display(X_test2_df.head())
display('y_train2 : ')
display(y_train2_df.head())
display('y_test2 : ')
display(y_test2_df.head())
41/49:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(' ')
print(results_dt_model1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(' ')
print(results_dt_model2)
41/50:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1

scaler = StandardScaler()

X_train_data1 = scaler.fit_transform(train_X1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_X1.values.reshape(-1, 1))

display(X_train_data1, "\n", X_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data

X_train_data2 = scaler.fit_transform(train_X2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_X2.values.reshape(-1, 1))


display(X_train_data2, "\n", X_test_data2)
41/51:
from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1

scaler = StandardScaler()

X_train_data1 = scaler.fit_transform(train_X1.reshape(-1, 1))
X_test_data1 = scaler.transform(test_X1.reshape(-1, 1))

display(X_train_data1, "\n", X_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data

X_train_data2 = scaler.fit_transform(train_X2.reshape(-1, 1))
X_test_data2 = scaler.transform(test_X2.reshape(-1, 1))

display(X_train_data2, "\n", X_test_data2)
41/52:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(' ')
print(results_dt_model1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(' ')
print(results_dt_model2)
41/53:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X1 = pd.DataFrame(['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
y1 = pd.DataFrame(['Probability1'])

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)

# Convert NumPy arrays to pandas DataFrames
X_train1_df = pd.DataFrame(X_train1, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
X_test1_df = pd.DataFrame(X_test1, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
y_train1_df = pd.DataFrame(y_train1, columns=['Probability1'])
y_test1_df = pd.DataFrame(y_test1, columns=['Probability1'])

# Now you can use the head() method
display('X_train1 : ')
display(X_train1_df.head())
display('X_test1 : ')
display(X_test1_df.head())
display('y_train1 : ')
display(y_train1_df.head())
display('y_test1 : ')
display(y_test1_df.head())

# Dataset without outliers

print("Dataset without outliers")

X2 = pd.DataFrame(['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
y2 = pd.DataFrame(['Probability2'])

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)

X_train2_df = pd.DataFrame(X_train2, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
X_test2_df = pd.DataFrame(X_test2, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
y_train2_df = pd.DataFrame(y_train2, columns=['Probability2'])
y_test2_df = pd.DataFrame(y_test2, columns=['Probability2'])

display('X_train2 : ')
display(X_train2_df.head())
display('X_test2 : ')
display(X_test2_df.head())
display('y_train2 : ')
display(y_train2_df.head())
display('y_test2 : ')
display(y_test2_df.head())
41/54:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X1 = pd.DataFrame(['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
y1 = pd.DataFrame(['Probability1'])

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)

# Now you can use the head() method
display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset without outliers

print("Dataset without outliers")

X2 = pd.DataFrame(['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
y2 = pd.DataFrame(['Probability2'])

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
41/55:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X1_data = np.random.rand(100, 5)
y1_data = np.random.rand(100)

X1 = pd.DataFrame(X1_data, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
y1 = pd.DataFrame(y1_data, columns=['Probability1'])

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)

# Now you can use the head() method
display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset without outliers

print("Dataset without outliers")


X2_data = np.random.rand(100, 5)  
y2_data = np.random.rand(100)    

X2 = pd.DataFrame(X2_data, columns=['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID'])
y2 = pd.DataFrame(y2_data, columns=['Probability2'])

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
41/56:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset with outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset without outliers

print("Dataset without outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
41/57:
from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1

scaler = StandardScaler()

X_train_data1 = scaler.fit_transform(train_X1.reshape(-1, 1))
X_test_data1 = scaler.transform(test_X1.reshape(-1, 1))

display(X_train_data1, "\n", X_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data

X_train_data2 = scaler.fit_transform(train_X2.reshape(-1, 1))
X_test_data2 = scaler.transform(test_X2.reshape(-1, 1))

display(X_train_data2, "\n", X_test_data2)
41/58:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(' ')
print(results_dt_model1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(' ')
print(results_dt_model2)
41/59: len(X_train_data1, y_train1)
41/60:
print("Shape of X_train_data1:", X_train_data1.shape)
print("Shape of y_train1:", y_train1.shape)
41/61:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)
print("Shape of:", y_test1.shape)
print("Shape of:", dt_pred1.shape)
results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(' ')
print(results_dt_model1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(' ')
print(results_dt_model2)
41/62:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_test_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(' ')
print(results_dt_model1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(' ')
print(results_dt_model2)
41/63:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(' ')
print(results_dt_model1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(' ')
print(results_dt_model2)
41/64:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(' ')
print(results_dt_model1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(' ')
print(results_dt_model2)
41/65:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Dataset with outliers

print("Dataset with outliers")

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)
dt_pred1 = dt_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, dt_pred1)
rmse = mean_squared_error(y_test1, dt_pred1, squared=False)
mae = mean_absolute_error(y_test1, dt_pred1)
mse = mean_squared_error(y_test1, dt_pred1)

results_dt_model1 = pd.DataFrame({
    "Model": "dt_pred1",
    'R2': [r2_score(y_test1, dt_pred1)],
    'RMSE': [mean_squared_error(y_test1, dt_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, dt_pred1)],
    'MSE': [mean_squared_error(y_test1, dt_pred1)]
})

print(dt_pred1)
print(' ')
print(results_dt_model1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)
dt_pred2 = dt_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, dt_pred2)
rmse = mean_squared_error(y_test2, dt_pred2, squared=False)
mae = mean_absolute_error(y_test2, dt_pred2)
mse = mean_squared_error(y_test2, dt_pred2)

results_dt_model2 = pd.DataFrame({
    "Model": "dt_pred2",
    'R2': [r2_score(y_test2, dt_pred2)],
    'RMSE': [mean_squared_error(y_test2, dt_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, dt_pred2)],
    'MSE': [mean_squared_error(y_test2, dt_pred2)]
})

print(dt_pred2)
print(' ')
print(results_dt_model2)
41/66:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Define your datasets here
# Replace X_train_data1, y_train1, X_test_data1, y_test1, X_train_data2, y_train2, X_test_data2, y_test2 with your actual data

# Dataset with outliers

# Initialize and train the Decision Tree Regressor
dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)

# Make predictions on the test data
dt_pred1 = dt_regressor1.predict(X_test_data1)

# Calculate evaluation metrics
r2_1 = r2_score(y_test1, dt_pred1)
rmse_1 = mean_squared_error(y_test1, dt_pred1, squared=False)
mae_1 = mean_absolute_error(y_test1, dt_pred1)
mse_1 = mean_squared_error(y_test1, dt_pred1)

# Display results
results_dt_model1 = pd.DataFrame({
    "Model": ["Decision Tree (with outliers)"],
    'R2': [r2_1],
    'RMSE': [rmse_1],
    'MAE': [mae_1],
    'MSE': [mse_1]
})

print(results_dt_model1)

# Dataset without outliers

# Initialize and train the Decision Tree Regressor
dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)

# Make predictions on the test data
dt_pred2 = dt_regressor2.predict(X_test_data2)

# Calculate evaluation metrics
r2_2 = r2_score(y_test2, dt_pred2)
rmse_2 = mean_squared_error(y_test2, dt_pred2, squared=False)
mae_2 = mean_absolute_error(y_test2, dt_pred2)
mse_2 = mean_squared_error(y_test2, dt_pred2)

# Display results
results_dt_model2 = pd.DataFrame({
    "Model": ["Decision Tree (without outliers)"],
    'R2': [r2_2],
    'RMSE': [rmse_2],
    'MAE': [mae_2],
    'MSE': [mse_2]
})

print(results_dt_model2)
41/67:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Define your datasets here
# Replace X_train_data1, y_train1, X_test_data1, y_test1, X_train_data2, y_train2, X_test_data2, y_test2 with your actual data

# Dataset with outliers

# Initialize and train the Decision Tree Regressor
dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)

# Make predictions on the test data
dt_pred1 = dt_regressor1.predict(X_test_data1)

# Calculate evaluation metrics
r2_1 = r2_score(y_test1, dt_pred1)
rmse_1 = mean_squared_error(y_test1, dt_pred1, squared=False)
mae_1 = mean_absolute_error(y_test1, dt_pred1)
mse_1 = mean_squared_error(y_test1, dt_pred1)

# Display results
results_dt_model1 = pd.DataFrame({
    "Model": ["Decision Tree (with outliers)"],
    'R2': [r2_1],
    'RMSE': [rmse_1],
    'MAE': [mae_1],
    'MSE': [mse_1]
})

print(results_dt_model1)

# Dataset without outliers

# Initialize and train the Decision Tree Regressor
dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)

# Make predictions on the test data
dt_pred2 = dt_regressor2.predict(X_test_data2)

# Calculate evaluation metrics
r2_2 = r2_score(y_test2, dt_pred2)
rmse_2 = mean_squared_error(y_test2, dt_pred2, squared=False)
mae_2 = mean_absolute_error(y_test2, dt_pred2)
mse_2 = mean_squared_error(y_test2, dt_pred2)

# Display results
results_dt_model2 = pd.DataFrame({
    "Model": ["Decision Tree (without outliers)"],
    'R2': [r2_2],
    'RMSE': [rmse_2],
    'MAE': [mae_2],
    'MSE': [mse_2]
})

print(results_dt_model2)
41/68:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
41/69:
# Dataset with outliers

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITH OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITHOUT OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
41/70:
# Dataset with outliers

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITH OUTLIERS: TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
41/71:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset without outliers

print("Dataset without outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
41/72:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset with outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset without outliers

print("Dataset without outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
41/73:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display(X_train_data1, "\n", X_test_data1)
display(Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display(X_train_data2, "\n", X_test_data2)
display(Y_train_data2, "\n", Y_test_data2)
41/74:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(1, -1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(1, -1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(1, -1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(1, -1))

display(X_train_data1, "\n", X_test_data1)
display(Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(1, -1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(1, -1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(1, -1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(1, -1))

display(X_train_data2, "\n", X_test_data2)
display(Y_train_data2, "\n", Y_test_data2)
41/75:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(1, -1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(1, -1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(1, -1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(1, -1))

display(X_train_data1, "\n", X_test_data1)
display(Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(1, -1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(1, -1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(1, -1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(1, -1))

display(X_train_data2, "\n", X_test_data2)
display(Y_train_data2, "\n", Y_test_data2)
41/76:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display(X_train_data1, "\n", X_test_data1)
display(Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display(X_train_data2, "\n", X_test_data2)
display(Y_train_data2, "\n", Y_test_data2)
41/77:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display("X train1 and X test1 scaling:", X_train_data1, "\n", X_test_data1)
display("Y train1 and Y test1 scaling:",Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display("X train2 and X test2 scaling:", X_train_data2, "\n", X_test_data2)
display("Y train2 and Y test2 scaling:",Y_train_data2, "\n", Y_test_data2)
41/78:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)

# Make predictions on the test data
dt_pred1 = dt_regressor1.predict(X_test_data1)

# Calculate evaluation metrics
r2_1 = r2_score(y_test1, dt_pred1)
rmse_1 = mean_squared_error(y_test1, dt_pred1, squared=False)
mae_1 = mean_absolute_error(y_test1, dt_pred1)
mse_1 = mean_squared_error(y_test1, dt_pred1)

# Display results
results_dt_model1 = pd.DataFrame({
    "Model": ["Decision Tree (with outliers)"],
    'R2': [r2_1],
    'RMSE': [rmse_1],
    'MAE': [mae_1],
    'MSE': [mse_1]
})

print(results_dt_model1)

# Dataset without outliers

# Initialize and train the Decision Tree Regressor
dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)

# Make predictions on the test data
dt_pred2 = dt_regressor2.predict(X_test_data2)

# Calculate evaluation metrics
r2_2 = r2_score(y_test2, dt_pred2)
rmse_2 = mean_squared_error(y_test2, dt_pred2, squared=False)
mae_2 = mean_absolute_error(y_test2, dt_pred2)
mse_2 = mean_squared_error(y_test2, dt_pred2)

# Display results
results_dt_model2 = pd.DataFrame({
    "Model": ["Decision Tree (without outliers)"],
    'R2': [r2_2],
    'RMSE': [rmse_2],
    'MAE': [mae_2],
    'MSE': [mse_2]
})

print(results_dt_model2)
41/79:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

print("Dataset with outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
41/80:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
41/81:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
41/82:
results_combined1 = pd.concat([results_dt_model1,  
                              results_rf_model1, results_gb_model1, results_svr_regressor1], ignore_index=True)

results_combined2 = pd.concat([results_dt_model2, results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_sorted1 = results_combined1.sort_values(by='RMSE')
results_sorted2 = results_combined2.sort_values(by='RMSE')

print(results_sorted1)
print(' ')
print(results_sorted2)
44/1:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
44/2:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
44/3:
# Check for missing values

missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
44/4:
# Check for duplicate rows

duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
44/5:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
44/6:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
44/7:
# Dataset with outliers

import pandas as pd
from IPython.display import display
import numpy as np
from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

probs1 = np.array(dataset_NO['Probability*'])
probs_2d = probs1.reshape(-1, 1)
scaler = StandardScaler()
Probability1 = scaler.fit_transform(probs_2d)
display(Probability1)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    epsilon = 1e-10  # Small constant to prevent division by zero
    scaled_data = (data - min_val) / (max_val - min_val + epsilon)
    return scaled_data

# Dataset without outliers

print("Dataset without outliers")

probs2 = np.array(dataset['Probability*'])
probs_2d = probs2.reshape(-1, 1)
scaler = StandardScaler()
Probability2 = scaler.fit_transform(probs_2d)
display(Probability2)

def min_max_scaling(data):
    min_val = np.min(data)
    max_val = np.max(data)
    scaled_data = (data - min_val) / (max_val - min_val)
    return scaled_data
scaled_probabilities = min_max_scaling(dataset['Probability*'].values)
display(scaled_probabilities)
44/8:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

encoded_labels_1 = dataset_NO['Target Class']  
label_encoder = LabelEncoder()
df_1 = label_encoder.fit_transform(encoded_labels_1)
print(df_1)
encoded_labels_2 = dataset_NO['Target']  
label_encoder = LabelEncoder()
df_2 = label_encoder.fit_transform(encoded_labels_2)
print(df_2)

# Dataset without outliers

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df1 = label_encoder.fit_transform(encoded_labels1)
print(df1)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df2 = label_encoder.fit_transform(encoded_labels2)
print(df2)
44/9:
# Dataset with outliers

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITH OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITHOUT OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
44/10:
# Dataset with outliers

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITH OUTLIERS: TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Dataset without outliers

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
44/11:
# Dataset with outliers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset without outliers

print("Dataset without outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
44/12:
# Dataset with outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset with outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset without outliers

print("Dataset without outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
44/13:
# Dataset with outliers

from sklearn.preprocessing import StandardScaler

print("Dataset with outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display("X train1 and X test1 scaling:", X_train_data1, "\n", X_test_data1)
display("Y train1 and Y test1 scaling:",Y_train_data1, "\n", Y_test_data1)

# Dataset without outliers

print("Dataset without outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display("X train2 and X test2 scaling:", X_train_data2, "\n", X_test_data2)
display("Y train2 and Y test2 scaling:",Y_train_data2, "\n", Y_test_data2)
44/14:
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)

# Make predictions on the test data
dt_pred1 = dt_regressor1.predict(X_test_data1)

# Calculate evaluation metrics
r2_1 = r2_score(y_test1, dt_pred1)
rmse_1 = mean_squared_error(y_test1, dt_pred1, squared=False)
mae_1 = mean_absolute_error(y_test1, dt_pred1)
mse_1 = mean_squared_error(y_test1, dt_pred1)

# Display results
results_dt_model1 = pd.DataFrame({
    "Model": ["Decision Tree (with outliers)"],
    'R2': [r2_1],
    'RMSE': [rmse_1],
    'MAE': [mae_1],
    'MSE': [mse_1]
})

print(results_dt_model1)

# Dataset without outliers

# Initialize and train the Decision Tree Regressor
dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)

# Make predictions on the test data
dt_pred2 = dt_regressor2.predict(X_test_data2)

# Calculate evaluation metrics
r2_2 = r2_score(y_test2, dt_pred2)
rmse_2 = mean_squared_error(y_test2, dt_pred2, squared=False)
mae_2 = mean_absolute_error(y_test2, dt_pred2)
mse_2 = mean_squared_error(y_test2, dt_pred2)

# Display results
results_dt_model2 = pd.DataFrame({
    "Model": ["Decision Tree (without outliers)"],
    'R2': [r2_2],
    'RMSE': [rmse_2],
    'MAE': [mae_2],
    'MSE': [mse_2]
})

print(results_dt_model2)
44/15:
# Dataset with outliers

from sklearn.model_selection import cross_val_score

print("Dataset with outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)
print(' ')

# Dataset without outliers

print("Dataset without outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
44/16:
# Dataset with outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset with outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset without outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
44/17:
results_combined1 = pd.concat([results_dt_model1,  
                              results_rf_model1, results_gb_model1, results_svr_regressor1], ignore_index=True)

results_combined2 = pd.concat([results_dt_model2, results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_sorted1 = results_combined1.sort_values(by='RMSE')
results_sorted2 = results_combined2.sort_values(by='RMSE')

print(results_sorted1)
print(' ')
print(results_sorted2)
44/18:
# Create a tree-like data structure using a dictionary

from graphviz import Digraph

tree_data = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data:
        tree_data[target_class] = {}

    if target not in tree_data[target_class]:
        tree_data[target_class][target] = []

    tree_data[target_class][target].append(probability)

# Accessing information
display(tree_data)
44/19:
# Visualizing tree data

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data)
plt.show()
44/20:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(10,20))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
44/21:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
44/22:
# Dataset without outliers

print("Dataset without outliers")

from graphviz import Digraph

tree_data1 = {}

for index, row in dataset_NO.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data1:
        tree_data1[target_class] = {}

    if target not in tree_data1[target_class]:
        tree_data1[target_class][target] = []

    tree_data1[target_class][target].append(probability)

# Accessing information
display(tree_data1)

# Dataset with outliers

print("Dataset with outliers")

from graphviz import Digraph

tree_data2 = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data2:
        tree_data2[target_class] = {}

    if target not in tree_data2[target_class]:
        tree_data2[target_class][target] = []

    tree_data2[target_class][target].append(probability)

# Accessing information
display(tree_data2)
44/23:
# Dataset without outliers

print("Dataset without outliers")


import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data1)
plt.show()

# Dataset with outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data2)
plt.show()
44/24:
# Dataset without outliers

print("Dataset without outliers")


import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data1)
plt.show()
display(tree_data1)

# Dataset with outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data2)
plt.show()
display(tree_data2)
44/25:
# Dataset without outliers

print("Dataset without outliers")


import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data1)
display(tree_data1)

# Dataset with outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data2)
display(tree_data2)
44/26:
# Dataset without outliers

print("Dataset without outliers")


import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data1)
plt.show()

# Dataset with outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data2)
plt.show()
44/27:
# Dataset without outliers

print("Dataset without outliers")


import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data1)
plt.show()
print(' ')

# Dataset with outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data2)
plt.show()
44/28:
# Dataset without outliers

print("Dataset without outliers")

from sklearn.preprocessing import LabelEncoder

encoded_labels_1 = dataset_NO['Target Class']  
label_encoder = LabelEncoder()
df_TC_1 = label_encoder.fit_transform(encoded_labels_1)
print(df_TC_1 )
encoded_labels_2 = dataset_NO['Target']  
label_encoder = LabelEncoder()
df_T_1 = label_encoder.fit_transform(encoded_labels_2)
print(df_T_1)

# Dataset with outliers

print("Dataset with outliers")

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df_TC_2 = label_encoder.fit_transform(encoded_labels1)
print(df_TC_2)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df_T_2 = label_encoder.fit_transform(encoded_labels2)
print(df_T_2)
44/29:
# Dataset without outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITHOUT OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset with outliers

print("Dataset with outliers")

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITH OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
44/30:
# Dataset without outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITHOUT OUTLIERS: TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Dataset with outliers

print("Dataset with outliers")

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITH OUTLIERS:TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
44/31:
# Dataset without outliers

print("Dataset without outliers")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset with outliers

print("Dataset with outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
44/32:
# Dataset without outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset without outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset with outliers

print("Dataset with outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
44/33:
# Dataset without outliers

from sklearn.preprocessing import StandardScaler

print("Dataset without outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display("X train1 and X test1 scaling:", X_train_data1, "\n", X_test_data1)
display("Y train1 and Y test1 scaling:",Y_train_data1, "\n", Y_test_data1)

# Dataset with outliers

print("Dataset with outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display("X train2 and X test2 scaling:", X_train_data2, "\n", X_test_data2)
display("Y train2 and Y test2 scaling:",Y_train_data2, "\n", Y_test_data2)
44/34:
# Dataset without outliers

print("Dataset without outliers")

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)

# Make predictions on the test data
dt_pred1 = dt_regressor1.predict(X_test_data1)

# Calculate evaluation metrics
r2_1 = r2_score(y_test1, dt_pred1)
rmse_1 = mean_squared_error(y_test1, dt_pred1, squared=False)
mae_1 = mean_absolute_error(y_test1, dt_pred1)
mse_1 = mean_squared_error(y_test1, dt_pred1)

# Display results
results_dt_model1 = pd.DataFrame({
    "Model": ["Decision Tree (with outliers)"],
    'R2': [r2_1],
    'RMSE': [rmse_1],
    'MAE': [mae_1],
    'MSE': [mse_1]
})

print(results_dt_model1)

# Dataset with outliers

print("Dataset with outliers")

# Initialize and train the Decision Tree Regressor
dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)

# Make predictions on the test data
dt_pred2 = dt_regressor2.predict(X_test_data2)

# Calculate evaluation metrics
r2_2 = r2_score(y_test2, dt_pred2)
rmse_2 = mean_squared_error(y_test2, dt_pred2, squared=False)
mae_2 = mean_absolute_error(y_test2, dt_pred2)
mse_2 = mean_squared_error(y_test2, dt_pred2)

# Display results
results_dt_model2 = pd.DataFrame({
    "Model": ["Decision Tree (without outliers)"],
    'R2': [r2_2],
    'RMSE': [rmse_2],
    'MAE': [mae_2],
    'MSE': [mse_2]
})

print(results_dt_model2)
44/35:
# Dataset without outliers

print("Dataset without outliers")

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)

# Make predictions on the test data
dt_pred1 = dt_regressor1.predict(X_test_data1)

# Calculate evaluation metrics
r2_1 = r2_score(y_test1, dt_pred1)
rmse_1 = mean_squared_error(y_test1, dt_pred1, squared=False)
mae_1 = mean_absolute_error(y_test1, dt_pred1)
mse_1 = mean_squared_error(y_test1, dt_pred1)

# Display results
results_dt_model1 = pd.DataFrame({
    "Model": ["Decision Tree (without outliers)"],
    'R2': [r2_1],
    'RMSE': [rmse_1],
    'MAE': [mae_1],
    'MSE': [mse_1]
})

print(results_dt_model1)

# Dataset with outliers

print("Dataset with outliers")

# Initialize and train the Decision Tree Regressor
dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)

# Make predictions on the test data
dt_pred2 = dt_regressor2.predict(X_test_data2)

# Calculate evaluation metrics
r2_2 = r2_score(y_test2, dt_pred2)
rmse_2 = mean_squared_error(y_test2, dt_pred2, squared=False)
mae_2 = mean_absolute_error(y_test2, dt_pred2)
mse_2 = mean_squared_error(y_test2, dt_pred2)

# Display results
results_dt_model2 = pd.DataFrame({
    "Model": ["Decision Tree (with outliers)"],
    'R2': [r2_2],
    'RMSE': [rmse_2],
    'MAE': [mae_2],
    'MSE': [mse_2]
})

print(results_dt_model2)
44/36:
# Dataset without outliers

from sklearn.model_selection import cross_val_score

print("Dataset without outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)
print(' ')

# Dataset with outliers

print("Dataset with outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
44/37:
# Dataset without outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset without outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset with outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
44/38:
results_combined1 = pd.concat([results_dt_model1,  
                              results_rf_model1, results_gb_model1, results_svr_regressor1], ignore_index=True)

results_combined2 = pd.concat([results_dt_model2, results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_sorted1 = results_combined1.sort_values(by='RMSE')
results_sorted2 = results_combined2.sort_values(by='RMSE')

print(results_sorted1)
print(' ')
print(results_sorted2)
46/1:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
# from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
46/2:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
46/3:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
46/4:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
46/5:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
46/6:
# Check for missing values

missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
46/7:
# Check for duplicate rows

duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
46/8:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
46/9:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
46/10:
# Dataset without outliers

print("Dataset without outliers")

from sklearn.preprocessing import LabelEncoder

encoded_labels_1 = dataset_NO['Target Class']  
label_encoder = LabelEncoder()
df_TC_1 = label_encoder.fit_transform(encoded_labels_1)
print(df_TC_1 )
encoded_labels_2 = dataset_NO['Target']  
label_encoder = LabelEncoder()
df_T_1 = label_encoder.fit_transform(encoded_labels_2)
print(df_T_1)

# Dataset with outliers

print("Dataset with outliers")

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df_TC_2 = label_encoder.fit_transform(encoded_labels1)
print(df_TC_2)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df_T_2 = label_encoder.fit_transform(encoded_labels2)
print(df_T_2)
46/11:
# Dataset without outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITHOUT OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset with outliers

print("Dataset with outliers")

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITH OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
46/12:
# Dataset without outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITHOUT OUTLIERS: TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Dataset with outliers

print("Dataset with outliers")

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITH OUTLIERS:TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
46/13:
# Dataset without outliers

print("Dataset without outliers")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset with outliers

print("Dataset with outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
46/14:
# Dataset without outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset without outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset with outliers

print("Dataset with outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
46/15:
# Dataset without outliers

from sklearn.preprocessing import StandardScaler

print("Dataset without outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display("X train1 and X test1 scaling:", X_train_data1, "\n", X_test_data1)
display("Y train1 and Y test1 scaling:",Y_train_data1, "\n", Y_test_data1)

# Dataset with outliers

print("Dataset with outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display("X train2 and X test2 scaling:", X_train_data2, "\n", X_test_data2)
display("Y train2 and Y test2 scaling:",Y_train_data2, "\n", Y_test_data2)
46/16:
# Dataset without outliers

print("Dataset without outliers")

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)

# Make predictions on the test data
dt_pred1 = dt_regressor1.predict(X_test_data1)

# Calculate evaluation metrics
r2_1 = r2_score(y_test1, dt_pred1)
rmse_1 = mean_squared_error(y_test1, dt_pred1, squared=False)
mae_1 = mean_absolute_error(y_test1, dt_pred1)
mse_1 = mean_squared_error(y_test1, dt_pred1)

# Display results
results_dt_model1 = pd.DataFrame({
    "Model": ["Decision Tree (without outliers)"],
    'R2': [r2_1],
    'RMSE': [rmse_1],
    'MAE': [mae_1],
    'MSE': [mse_1]
})

print(results_dt_model1)

# Dataset with outliers

print("Dataset with outliers")

# Initialize and train the Decision Tree Regressor
dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)

# Make predictions on the test data
dt_pred2 = dt_regressor2.predict(X_test_data2)

# Calculate evaluation metrics
r2_2 = r2_score(y_test2, dt_pred2)
rmse_2 = mean_squared_error(y_test2, dt_pred2, squared=False)
mae_2 = mean_absolute_error(y_test2, dt_pred2)
mse_2 = mean_squared_error(y_test2, dt_pred2)

# Display results
results_dt_model2 = pd.DataFrame({
    "Model": ["Decision Tree (with outliers)"],
    'R2': [r2_2],
    'RMSE': [rmse_2],
    'MAE': [mae_2],
    'MSE': [mse_2]
})

print(results_dt_model2)
46/17:
# Dataset without outliers

from sklearn.model_selection import cross_val_score

print("Dataset without outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)
print(' ')

# Dataset with outliers

print("Dataset with outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
46/18:
# Dataset without outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset without outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset with outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
46/19:
results_combined1 = pd.concat([results_dt_model1,  
                              results_rf_model1, results_gb_model1, results_svr_regressor1], ignore_index=True)

results_combined2 = pd.concat([results_dt_model2, results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_sorted1 = results_combined1.sort_values(by='RMSE')
results_sorted2 = results_combined2.sort_values(by='RMSE')

print(results_sorted1)
print(' ')
print(results_sorted2)
46/20:
# Dataset without outliers

print("Dataset without outliers")

from graphviz import Digraph

tree_data1 = {}

for index, row in dataset_NO.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data1:
        tree_data1[target_class] = {}

    if target not in tree_data1[target_class]:
        tree_data1[target_class][target] = []

    tree_data1[target_class][target].append(probability)

# Accessing information
display(tree_data1)

# Dataset with outliers

print("Dataset with outliers")

from graphviz import Digraph

tree_data2 = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data2:
        tree_data2[target_class] = {}

    if target not in tree_data2[target_class]:
        tree_data2[target_class][target] = []

    tree_data2[target_class][target].append(probability)

# Accessing information
display(tree_data2)
46/21:
# Dataset without outliers

print("Dataset without outliers")


import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data1)
plt.show()
print(' ')

# Dataset with outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data2)
plt.show()
46/22:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
46/23:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
46/24:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
46/25:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
46/26:
# Read the CSV file

import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
46/27:
# Check for missing values

missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
46/28:
# Check for duplicate rows

duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
46/29:
#Inspect Data Types

data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
46/30:
# Checking for outliers

import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
46/31:
# Dataset without outliers

print("Dataset without outliers")

from sklearn.preprocessing import LabelEncoder

encoded_labels_1 = dataset_NO['Target Class']  
label_encoder = LabelEncoder()
df_TC_1 = label_encoder.fit_transform(encoded_labels_1)
print(df_TC_1 )
encoded_labels_2 = dataset_NO['Target']  
label_encoder = LabelEncoder()
df_T_1 = label_encoder.fit_transform(encoded_labels_2)
print(df_T_1)

# Dataset with outliers

print("Dataset with outliers")

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df_TC_2 = label_encoder.fit_transform(encoded_labels1)
print(df_TC_2)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df_T_2 = label_encoder.fit_transform(encoded_labels2)
print(df_T_2)
46/32:
# Dataset without outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITHOUT OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset with outliers

print("Dataset with outliers")

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITH OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
46/33:
# Dataset without outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITHOUT OUTLIERS: TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Dataset with outliers

print("Dataset with outliers")

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITH OUTLIERS:TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
46/34:
# Dataset without outliers

print("Dataset without outliers")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset with outliers

print("Dataset with outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
46/35:
# Dataset without outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset without outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset with outliers

print("Dataset with outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
46/36:
# Dataset without outliers

from sklearn.preprocessing import StandardScaler

print("Dataset without outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display("X train1 and X test1 scaling:", X_train_data1, "\n", X_test_data1)
display("Y train1 and Y test1 scaling:",Y_train_data1, "\n", Y_test_data1)

# Dataset with outliers

print("Dataset with outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display("X train2 and X test2 scaling:", X_train_data2, "\n", X_test_data2)
display("Y train2 and Y test2 scaling:",Y_train_data2, "\n", Y_test_data2)
46/37:
# Dataset without outliers

print("Dataset without outliers")

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)

# Make predictions on the test data
dt_pred1 = dt_regressor1.predict(X_test_data1)

# Calculate evaluation metrics
r2_1 = r2_score(y_test1, dt_pred1)
rmse_1 = mean_squared_error(y_test1, dt_pred1, squared=False)
mae_1 = mean_absolute_error(y_test1, dt_pred1)
mse_1 = mean_squared_error(y_test1, dt_pred1)

# Display results
results_dt_model1 = pd.DataFrame({
    "Model": ["Decision Tree (without outliers)"],
    'R2': [r2_1],
    'RMSE': [rmse_1],
    'MAE': [mae_1],
    'MSE': [mse_1]
})

print(results_dt_model1)

# Dataset with outliers

print("Dataset with outliers")

# Initialize and train the Decision Tree Regressor
dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)

# Make predictions on the test data
dt_pred2 = dt_regressor2.predict(X_test_data2)

# Calculate evaluation metrics
r2_2 = r2_score(y_test2, dt_pred2)
rmse_2 = mean_squared_error(y_test2, dt_pred2, squared=False)
mae_2 = mean_absolute_error(y_test2, dt_pred2)
mse_2 = mean_squared_error(y_test2, dt_pred2)

# Display results
results_dt_model2 = pd.DataFrame({
    "Model": ["Decision Tree (with outliers)"],
    'R2': [r2_2],
    'RMSE': [rmse_2],
    'MAE': [mae_2],
    'MSE': [mse_2]
})

print(results_dt_model2)
46/38:
# Dataset without outliers

from sklearn.model_selection import cross_val_score

print("Dataset without outliers")

cv_scores1 = cross_val_score(dt_regressor1, X_train_data1, y_train1, cv=5, scoring='neg_mean_squared_error')
mse_cv1 = -cv_scores1.mean()
print(mse_cv1)
print(' ')

# Dataset with outliers

print("Dataset with outliers")

cv_scores2 = cross_val_score(dt_regressor2, X_train_data2, y_train2, cv=5, scoring='neg_mean_squared_error')
mse_cv2 = -cv_scores2.mean()
print(mse_cv2)
46/39:
# Dataset without outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset without outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset with outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
46/40:
results_combined1 = pd.concat([results_dt_model1,  
                              results_rf_model1, results_gb_model1, results_svr_regressor1], ignore_index=True)

results_combined2 = pd.concat([results_dt_model2, results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_sorted1 = results_combined1.sort_values(by='RMSE')
results_sorted2 = results_combined2.sort_values(by='RMSE')

print(results_sorted1)
print(' ')
print(results_sorted2)
46/41:
# Dataset without outliers

print("Dataset without outliers")

from graphviz import Digraph

tree_data1 = {}

for index, row in dataset_NO.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data1:
        tree_data1[target_class] = {}

    if target not in tree_data1[target_class]:
        tree_data1[target_class][target] = []

    tree_data1[target_class][target].append(probability)

# Accessing information
display(tree_data1)

# Dataset with outliers

print("Dataset with outliers")

from graphviz import Digraph

tree_data2 = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data2:
        tree_data2[target_class] = {}

    if target not in tree_data2[target_class]:
        tree_data2[target_class][target] = []

    tree_data2[target_class][target].append(probability)

# Accessing information
display(tree_data2)
46/42:
# Dataset without outliers

print("Dataset without outliers")


import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data1)
plt.show()
print(' ')

# Dataset with outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data2)
plt.show()
49/1:
# Dataset without outliers

print("Dataset without outliers")

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)

# Make predictions on the test data
dt_pred1 = dt_regressor1.predict(X_test_data1) 
print(dt_pred1)

# Calculate evaluation metrics
r2_1 = r2_score(y_test1, dt_pred1)
rmse_1 = mean_squared_error(y_test1, dt_pred1, squared=False)
mae_1 = mean_absolute_error(y_test1, dt_pred1)
mse_1 = mean_squared_error(y_test1, dt_pred1)

# Display results
results_dt_model1 = pd.DataFrame({
    "Model": ["Decision Tree (without outliers)"],
    'R2': [r2_1],
    'RMSE': [rmse_1],
    'MAE': [mae_1],
    'MSE': [mse_1]
})

print(results_dt_model1)

# Dataset with outliers

print("Dataset with outliers")

# Initialize and train the Decision Tree Regressor
dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)

# Make predictions on the test data
dt_pred2 = dt_regressor2.predict(X_test_data2)

# Calculate evaluation metrics
r2_2 = r2_score(y_test2, dt_pred2)
rmse_2 = mean_squared_error(y_test2, dt_pred2, squared=False)
mae_2 = mean_absolute_error(y_test2, dt_pred2)
mse_2 = mean_squared_error(y_test2, dt_pred2)

# Display results
results_dt_model2 = pd.DataFrame({
    "Model": ["Decision Tree (with outliers)"],
    'R2': [r2_2],
    'RMSE': [rmse_2],
    'MAE': [mae_2],
    'MSE': [mse_2]
})

print(results_dt_model2)
49/2:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
49/3:
import pandas as pd
from IPython.display import display

dataset = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv")
display(dataset)
49/4:
missing_values = dataset.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
49/5:
duplicates = dataset.duplicated()
duplicate_rows = dataset[duplicates]
print(duplicate_rows)
df_cleaned = dataset.drop_duplicates()
display(df_cleaned)
49/6:
data_types = dataset.dtypes
display(data_types)
dataset['Target Class'] = dataset['Target Class'].astype('category')
display(dataset['Target Class'])
dataset['Target'] = dataset['Target'].astype('category')
display(dataset['Target'])
49/7:
import matplotlib.pyplot as plt

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) & (dataset['Probability*'] <= upper_threshold)]

# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")

# Show the plots
plt.show()
49/8:
# Dataset without outliers

print("Dataset without outliers")

from sklearn.preprocessing import LabelEncoder

encoded_labels_1 = dataset_NO['Target Class']  
label_encoder = LabelEncoder()
df_TC_1 = label_encoder.fit_transform(encoded_labels_1)
print(df_TC_1 )
encoded_labels_2 = dataset_NO['Target']  
label_encoder = LabelEncoder()
df_T_1 = label_encoder.fit_transform(encoded_labels_2)
print(df_T_1)

# Dataset with outliers

print("Dataset with outliers")

encoded_labels1 = dataset['Target Class']  
label_encoder = LabelEncoder()
df_TC_2 = label_encoder.fit_transform(encoded_labels1)
print(df_TC_2)
encoded_labels2 = dataset['Target']  
label_encoder = LabelEncoder()
df_T_2 = label_encoder.fit_transform(encoded_labels2)
print(df_T_2)
49/9:
# Dataset without outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

encoded_labels_1 = [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 19, 20]


colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset_NO['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels_1.count(label) for label in set(encoded_labels_1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITHOUT OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels_1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()

# Dataset with outliers

print("Dataset with outliers")

encoded_labels1 = [0,  0,  0,  0,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,
                   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,
                   6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  9, 10, 11, 11, 12, 12,
                   13, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18,
                   19, 19, 19, 20]

colors = ['red', 'green', 'blue', 'yellow', 'purple', 'maroon', 'pink', 'violet', 'turquoise', 'thistle', 'teal', 'tan', 'skyblue', 'silver', 'sienna', 'saddlebrown', 'plum', 'orange', 'olive', 'gray', 'darkgreen']
Target_Class = dataset['Target Class'].unique()

# Calculate the frequency of each class
class_counts = {label: encoded_labels1.count(label) for label in set(encoded_labels1)}

# Get class frequencies
class_frequencies = list(class_counts.values())

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(class_frequencies, labels=None, autopct='', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("WITH OUTLIERS: TARGET CLASS")

# Add percentages next to legend labels
plt.setp(autotexts, size=12, weight="bold")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({(class_frequencies[i]/len(encoded_labels1))*100:.1f}%)' for i in range(len(Target_Class))]

plt.legend(wedges, legend_labels, title='Target class', loc='upper right')

# Show the plot
plt.show()
49/10:
# Dataset without outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITHOUT OUTLIERS: TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()

# Dataset with outliers

print("Dataset with outliers")

# Given data
sum_value = 11.25233
percentage_values = [0.1061658 * n for n in [4, 2, 16, 12, 1, 1, 22, 1, 1, 1, 2, 2, 2, 1, 2, 15, 1, 1, 3, 3, 1]]
percentages = [sum(percentage_values[i:i+1]) / sum_value * 100 for i in range(len(percentage_values))]

# Create a pie chart
plt.figure(figsize=(15, 6))
wedges, texts, autotexts = plt.pie(percentages, labels=None, autopct=' ', startangle=140, wedgeprops={'edgecolor': 'white'})
plt.axis('equal')  
# Add percentages next to legend labels
plt.setp(autotexts, size=8, weight="bold")
plt.title("WITH OUTLIERS:TARGET CLASS AND ITS PROBABILITY")

# Create a separate table of legends
legend_labels = [f'{Target_Class[i]} ({percentages[i]:.1f}%)' for i in range(len(Target_Class))]

# Add a legend
plt.legend(wedges, legend_labels, title='Legend', loc='upper right')

# Show the plot
plt.show()
49/11:
# Dataset without outliers

print("Dataset without outliers")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

print("Dataset with outliers")

X = dataset_NO[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset_NO['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train1 : ')
display(X_train1.head())
display('X_test1 : ')
display(X_test1.head())
display('y_train1 : ')
display(y_train1.head())
display('y_test1 : ')
display(y_test1.head())

# Dataset with outliers

print("Dataset with outliers")

X = dataset[['Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = dataset['Probability*']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
49/12:
# Dataset without outliers

from sklearn.preprocessing import LabelEncoder

print("Dataset without outliers")

X_train_LE1 = X_train1.copy()

label_encodings_train1 = {}

for col in X_train_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE1[col])
    label_encodings_train1[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_1 in label_encodings_train1.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_1)

# Perform label encoding on the testing set
X_test_LE1 = X_test1.copy()

label_encodings_test = {}

for col in X_test_LE1:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE1[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_1 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_1)

# Dataset with outliers

print("Dataset with outliers")

X_train_LE2 = X_train2.copy()

label_encodings_train = {}

for col in X_train_LE2:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_train_LE2[col])
    label_encodings_train[col] = df_encoded

# Print label encodings for the training set
for col, X_train_set_2 in label_encodings_train.items():
    print(f"Label Encoding for {col}:")
    print(X_train_set_2)

# Perform label encoding on the testing set
X_test_LE = X_test2.copy()

label_encodings_test = {}

for col in X_test_LE:
    label_encoder = LabelEncoder()
    df_encoded = label_encoder.fit_transform(X_test_LE[col])
    label_encodings_test[col] = df_encoded

# Print label encodings for the testing set
for col, X_test_set_2 in label_encodings_test.items():
    print(f"Label Encoding for {col}:")
    print(X_test_set_2)
49/13:
# Dataset without outliers

from sklearn.preprocessing import StandardScaler

print("Dataset without outliers")

train_X1 = X_train_set_1
test_X1 = X_train_set_1
train_Y1 = y_train1
test_Y1 = y_test1

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
X_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))
Y_train_data1 = scaler.fit_transform(train_Y1.values.reshape(-1, 1))
Y_test_data1 = scaler.transform(test_Y1.values.reshape(-1, 1))

display("X train1 and X test1 scaling:", X_train_data1, "\n", X_test_data1)
display("Y train1 and Y test1 scaling:",Y_train_data1, "\n", Y_test_data1)

# Dataset with outliers

print("Dataset with outliers")

train_X2 = X_train_set_2
test_X2 = X_train_set_2
train_Y2 = y_train2
test_Y2 = y_test2

# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
X_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
X_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))
Y_train_data2 = scaler.fit_transform(train_Y2.values.reshape(-1, 1))
Y_test_data2 = scaler.transform(test_Y2.values.reshape(-1, 1))

display("X train2 and X test2 scaling:", X_train_data2, "\n", X_test_data2)
display("Y train2 and Y test2 scaling:",Y_train_data2, "\n", Y_test_data2)
49/14:
# Dataset without outliers

print("Dataset without outliers")

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

dt_regressor1 = DecisionTreeRegressor(random_state=42)
dt_regressor1.fit(X_train_data1, y_train1)

# Make predictions on the test data
dt_pred1 = dt_regressor1.predict(X_test_data1) 
print(dt_pred1)

# Calculate evaluation metrics
r2_1 = r2_score(y_test1, dt_pred1)
rmse_1 = mean_squared_error(y_test1, dt_pred1, squared=False)
mae_1 = mean_absolute_error(y_test1, dt_pred1)
mse_1 = mean_squared_error(y_test1, dt_pred1)

# Display results
results_dt_model1 = pd.DataFrame({
    "Model": ["Decision Tree (without outliers)"],
    'R2': [r2_1],
    'RMSE': [rmse_1],
    'MAE': [mae_1],
    'MSE': [mse_1]
})

print(results_dt_model1)

# Dataset with outliers

print("Dataset with outliers")

# Initialize and train the Decision Tree Regressor
dt_regressor2 = DecisionTreeRegressor(random_state=42)
dt_regressor2.fit(X_train_data2, y_train2)

# Make predictions on the test data
dt_pred2 = dt_regressor2.predict(X_test_data2)

# Calculate evaluation metrics
r2_2 = r2_score(y_test2, dt_pred2)
rmse_2 = mean_squared_error(y_test2, dt_pred2, squared=False)
mae_2 = mean_absolute_error(y_test2, dt_pred2)
mse_2 = mean_squared_error(y_test2, dt_pred2)

# Display results
results_dt_model2 = pd.DataFrame({
    "Model": ["Decision Tree (with outliers)"],
    'R2': [r2_2],
    'RMSE': [rmse_2],
    'MAE': [mae_2],
    'MSE': [mse_2]
})

print(results_dt_model2)
49/15:
# Dataset without outliers

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

print("Dataset without outliers")

# Random Forest Regressor

rf_model1 = RandomForestRegressor()
rf_model1.fit(X_train_data1, y_train1)
rf_predictions1 = rf_model1.predict(X_test_data1)
r2 = r2_score(y_test1, rf_predictions1)
rmse = mean_squared_error(y_test1, rf_predictions1, squared=False)
mae = mean_absolute_error(y_test1, rf_predictions1)
mse = mean_squared_error(y_test1, rf_predictions1)

results_rf_model1 = pd.DataFrame({
    "Model": "rf_model1",
    'R2': [r2_score(y_test1, rf_predictions1)],
    'RMSE': [mean_squared_error(y_test1, rf_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, rf_predictions1)],
    'MSE': [mean_squared_error(y_test1, rf_predictions1)]
})
print(results_rf_model1)
print(' ')

# Gradient Boosting Regressor

gb_model1 = GradientBoostingRegressor()
gb_model1.fit(X_train_data1, y_train1)
gb_predictions1 = gb_model1.predict(X_test_data1)
r2 = r2_score(y_test1, gb_predictions1)
rmse = mean_squared_error(y_test1, gb_predictions1, squared=False)
mae = mean_absolute_error(y_test1, gb_predictions1)
mse = mean_squared_error(y_test1, gb_predictions1)
results_gb_model1 = pd.DataFrame({
    "Model": "gb_model1",
    'R2': [r2_score(y_test1,gb_predictions1)],
    'RMSE': [mean_squared_error(y_test1, gb_predictions1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, gb_predictions1)],
    'MSE': [mean_squared_error(y_test1, gb_predictions1)]
})
print(results_gb_model1)
print(' ')

# Build a Support vector machines

svr_regressor1 = SVR()
svr_regressor1.fit(X_train_data1, y_train1.values.ravel())
svr_pred1 = svr_regressor1.predict(X_test_data1)
r2 = r2_score(y_test1, svr_pred1)
rmse = mean_squared_error(y_test1, svr_pred1, squared=False)
mae = mean_absolute_error(y_test1, svr_pred1)
mse = mean_squared_error(y_test1, svr_pred1)
results_svr_regressor1 = pd.DataFrame({
    "Model": "svr_regressor1",
    'R2': [r2_score(y_test1, svr_pred1)],
    'RMSE': [mean_squared_error(y_test1, svr_pred1, squared=False)],
    'MAE': [mean_absolute_error(y_test1, svr_pred1)],
    'MSE': [mean_squared_error(y_test1, svr_pred1)]
})
print(results_svr_regressor1)
print(' ')

# Dataset with outliers

print("Dataset with outliers")

# Random Forest Regressor

rf_model2 = RandomForestRegressor()
rf_model2.fit(X_train_data2, y_train2)
rf_predictions2 = rf_model2.predict(X_test_data2)
r2 = r2_score(y_test2, rf_predictions2)
rmse = mean_squared_error(y_test2, rf_predictions2, squared=False)
mae = mean_absolute_error(y_test2, rf_predictions2)
mse = mean_squared_error(y_test2, rf_predictions2)

results_rf_model2 = pd.DataFrame({
    "Model": "rf_model2",
    'R2': [r2_score(y_test2, rf_predictions2)],
    'RMSE': [mean_squared_error(y_test2, rf_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, rf_predictions2)],
    'MSE': [mean_squared_error(y_test2, rf_predictions2)]
})
print(results_rf_model2)
print(' ')

# Gradient Boosting Regressor

gb_model2 = GradientBoostingRegressor()
gb_model2.fit(X_train_data2, y_train2)
gb_predictions2 = gb_model2.predict(X_test_data2)
r2 = r2_score(y_test2, gb_predictions2)
rmse = mean_squared_error(y_test2, gb_predictions2, squared=False)
mae = mean_absolute_error(y_test2, gb_predictions2)
mse = mean_squared_error(y_test2, gb_predictions2)
results_gb_model2 = pd.DataFrame({
    "Model": "gb_model2",
    'R2': [r2_score(y_test2, gb_predictions2)],
    'RMSE': [mean_squared_error(y_test2, gb_predictions2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, gb_predictions2)],
    'MSE': [mean_squared_error(y_test2, gb_predictions2)]
})
print(results_gb_model2)
print(' ')

# Build a Support vector machines

svr_regressor2 = SVR()
svr_regressor2.fit(X_train_data2, y_train2.values.ravel())
svr_pred2 = svr_regressor2.predict(X_test_data2)
r2 = r2_score(y_test2, svr_pred2)
rmse = mean_squared_error(y_test2, svr_pred2, squared=False)
mae = np.sqrt(mean_absolute_error(y_test2, svr_pred2))
mse = mean_squared_error(y_test2, svr_pred2)
results_svr_regressor2 = pd.DataFrame({
    "Model": "svr_regressor2",
    'R2': [r2_score(y_test2, svr_pred2)],
    'RMSE': [mean_squared_error(y_test2, svr_pred2, squared=False)],
    'MAE': [mean_absolute_error(y_test2, svr_pred2)],
    'MSE': [mean_squared_error(y_test2, svr_pred2)]
})
print(results_svr_regressor2)
print(' ')
49/16:
results_combined1 = pd.concat([results_dt_model1,  
                              results_rf_model1, results_gb_model1, results_svr_regressor1], ignore_index=True)

results_combined2 = pd.concat([results_dt_model2, results_rf_model2, results_gb_model2, results_svr_regressor2], ignore_index=True)

results_sorted1 = results_combined1.sort_values(by='RMSE')
results_sorted2 = results_combined2.sort_values(by='RMSE')

print(results_sorted1)
print(' ')
print(results_sorted2)
49/17:
# Dataset without outliers

print("Dataset without outliers")

from graphviz import Digraph

tree_data1 = {}

for index, row in dataset_NO.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data1:
        tree_data1[target_class] = {}

    if target not in tree_data1[target_class]:
        tree_data1[target_class][target] = []

    tree_data1[target_class][target].append(probability)

# Accessing information
display(tree_data1)

# Dataset with outliers

print("Dataset with outliers")

from graphviz import Digraph

tree_data2 = {}

for index, row in dataset.iterrows():
    target_class = row['Target Class']
    target = row['Target']
    probability = row['Probability*']

    if target_class not in tree_data2:
        tree_data2[target_class] = {}

    if target not in tree_data2[target_class]:
        tree_data2[target_class][target] = []

    tree_data2[target_class][target].append(probability)

# Accessing information
display(tree_data2)
49/18:
# Dataset without outliers

print("Dataset without outliers")


import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data1)
plt.show()
print(' ')

# Dataset with outliers

print("Dataset without outliers")

import matplotlib.pyplot as plt

def plot_tree(data, parent=None, level=0):
    for key, value in data.items():
        if isinstance(value, dict):
            print("  " * level + f"- {key}")
            plot_tree(value, parent=key, level=level+1)
        else:
            print("  " * level + f"- {key}: {value[0]}")

# Assuming `tree_data` contains your hierarchical data
plot_tree(tree_data2)
plt.show()
50/1:
# Read the CSV file

customer_data = pd.read_excel(r"C:\Users\sound\OneDrive\Desktop\Internshala work\customer_churn_large_dataset.xlsx")
display(customer_data)
50/2:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
50/3:
# Read the CSV file

customer_data = pd.read_excel(r"C:\Users\sound\OneDrive\Desktop\Internshala work\customer_churn_large_dataset.xlsx")
display(customer_data)
50/4:
# Check for missing values

missing_values = customer_data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
50/5:
#  Check for duplicate rows

duplicates = customer_data.duplicated()
duplicate_rows = customer_data[duplicates]
print(duplicate_rows)
df_cleaned = customer_data.drop_duplicates()
display(df_cleaned)
51/1:
# Drop irrelevant columns
columns_to_drop = ['Total_Usage_GB']
display(columns_to_drop)
51/2:
# Drop irrelevant columns
columns_to_drop = ['Total_Usage_GB']
dataset = customer_data.drop(columns_to_drop, axis=1)
display(columns_to_drop)
51/3:
# Drop irrelevant columns
columns_to_drop = ['Total_Usage_GB']
dataset = customer_data.drop(columns_to_drop, axis=1)
display(dataset)
51/4:
# Drop irrelevant columns
columns_to_drop = ['Total_Usage_GB']
dataset = customer_data.drop(columns_to_drop, axis=1)
display(dataset)
51/5:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
51/6:
# Read the CSV file

customer_data = pd.read_excel(r"C:\Users\sound\OneDrive\Desktop\Internshala work\customer_churn_large_dataset.xlsx")
display(customer_data)
51/7:
# Check for missing values

missing_values = customer_data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
51/8:
#  Check for duplicate rows

duplicates = customer_data.duplicated()
duplicate_rows = customer_data[duplicates]
print(duplicate_rows)
df_cleaned = customer_data.drop_duplicates()
display(df_cleaned)
51/9:
# Drop irrelevant columns
columns_to_drop = ['Total_Usage_GB']
dataset = customer_data.drop(columns_to_drop, axis=1)
display(dataset)
51/10:
# Checking for outliers

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Churn', ax=ax)
plt.title("Boxplot with Outliers")
51/11:
# Checking for outliers

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset.boxplot(column='Subscription_Length_Months', ax=ax)
plt.title("Boxplot with Outliers")
51/12:
# Checking for outliers

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(30,10))
dataset.boxplot(column='Subscription_Length_Months', ax=ax)
plt.title("Boxplot with Outliers")
51/13:
# Checking for outliers

dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(30,20))
dataset.boxplot(column='Subscription_Length_Months', ax=ax)
plt.title("Boxplot with Outliers")
51/14:
# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(30, 20))
dataset.boxplot(columns=['Subscription_Length_Months', 'Monthly_Bill'], ax=ax)

# Calculate the Interquartile Range (IQR)
Q1 = dataset.quantile(0.25)
Q3 = dataset.quantile(0.75)
iqr = Q3 - Q1

# Identify outliers
outliers = dataset[(dataset < (Q1 - 1.5 * iqr)) | (dataset > (Q3 + 1.5 * iqr))]

# Count outliers
outlier_count = outliers.count()

plt.title("Boxplot with Outliers")
51/15:
# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(30, 20))
dataset.boxplot(column=['Subscription_Length_Months', 'Monthly_Bill'], ax=ax)

# Calculate the Interquartile Range (IQR)
Q1 = dataset.quantile(0.25)
Q3 = dataset.quantile(0.75)
iqr = Q3 - Q1

# Identify outliers
outliers = dataset[(dataset < (Q1 - 1.5 * iqr)) | (dataset > (Q3 + 1.5 * iqr))]

# Count outliers
outlier_count = outliers.count()

plt.title("Boxplot with Outliers")
51/16:
# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(30, 20))
dataset.boxplot(column=['Subscription_Length_Months', 'Monthly_Bill'], ax=ax)
outlier_count = dataset.count()
plt.title("Boxplot with Outliers")
51/17:
# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20, 10))
dataset.boxplot(column=['Subscription_Length_Months', 'Monthly_Bill'], ax=ax)
outlier_count = dataset.count()
plt.title("Boxplot with Outliers")
51/18:
# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20, 10))
dataset.boxplot(column=['Subscription_Length_Months', 'Monthly_Bill'], ax=ax)
outlier_count = dataset.count()
plt.title("Boxplot with Outliers")
51/19:
# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20, 10))
dataset.boxplot(column=['Monthly_Bill'], ax=ax)
outlier_count = dataset.count()
plt.title("Boxplot with Outliers")
51/20:
# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20, 10))
dataset.boxplot(column=['Monthly_Bill'], ax=ax)
outlier_count = dataset.count()
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Probability*'].quantile(0.25)
q3 = dataset['Probability*'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Probability*'] >= lower_threshold) ]
                     
# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")
51/21:
# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20, 10))
dataset.boxplot(column=['Monthly_Bill'], ax=ax)
outlier_count = dataset.count()
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Monthly_Bill'].quantile(0.25)
q3 = dataset['Monthly_Bill'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Monthly_Bill'] >= lower_threshold)]
                     
# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot without Outliers")
51/22:
# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20, 10))
dataset.boxplot(column=['Monthly_Bill'], ax=ax)
outlier_count = dataset.count()
plt.title("Boxplot with Outliers")

# Detecting outliers using IQR method
q1 = dataset['Monthly_Bill'].quantile(0.25)
q3 = dataset['Monthly_Bill'].quantile(0.75)
iqr = q3 - q1
lower_threshold = q1 - 1.5 * iqr
upper_threshold = q3 + 1.5 * iqr
dataset_NO = dataset[(dataset['Monthly_Bill'] >= lower_threshold)]
                     
# Create a boxplot after removing outliers
fig, ax = plt.subplots(figsize=(20,10))
dataset_NO.boxplot(column='Monthly_Bill', ax=ax)
plt.title("Boxplot without Outliers")
51/23:
# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20, 10))
dataset.boxplot(column=['Monthly_Bill'], ax=ax)
outlier_count = dataset.count()
plt.title("Boxplot with Outliers")
51/24:
# Visualize the data

import matplotlib.pyplot as plt
import numpy as np

# Generate some example data (replace this with your own dataset)
x = dataset['Gender']
y = dataset['Churn']

# Create histogram
plt.hist(data, bins=50, color='skyblue', edgecolor='black')

# Add labels and title
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.title('Histogram Plot')

# Show the plot
plt.show()
51/25:
# Visualize the data

import matplotlib.pyplot as plt
import numpy as np

# Generate some example data (replace this with your own dataset)
x = dataset['Gender']
y = dataset['Churn']

# Create histogram
plt.hist(x,y, bins=50, color='skyblue', edgecolor='black')

# Add labels and title
plt.xlabel('Gender')
plt.ylabel('Churn')
plt.title('Histogram Plot')

# Show the plot
plt.show()
51/26:
# Visualize the data

import matplotlib.pyplot as plt
import numpy as np

# Generate some example data (replace this with your own dataset)
x = dataset['Gender']
y = dataset['Churn']

# Create histogram
plt.hist(x,y, bins=2, color='skyblue', edgecolor='black')

# Add labels and title
plt.xlabel('Gender')
plt.ylabel('Churn')
plt.title('Histogram Plot')

# Show the plot
plt.show()
51/27:
import matplotlib.pyplot as plt

# Generate some example data (replace this with your own dataset)
x = dataset['Gender']

# Create histogram
plt.hist(x, bins=2, color='skyblue', edgecolor='black')

# Add labels and title
plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Histogram Plot')

plt.show()
51/28:
import matplotlib.pyplot as plt

# Generate some example data (replace this with your own dataset)
x = dataset['Gender']
y = dataset['Churn']
# Create histogram
plt.hist(x, y, bins=2, color='skyblue', edgecolor='black')

# Add labels and title
plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Histogram Plot')

plt.show()
51/29:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = dataset['Gender'].unique()
churn_labels = dataset['Churn'].unique()

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[True], bar_width, label='Churn=True')
plt.bar(index + bar_width, counts[False], bar_width, label='Churn=False')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/30:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = dataset['Gender'].unique()
churn_labels = dataset['Churn'].unique()

# Convert 'Churn' to boolean
dataset['Churn'] = dataset['Churn'].map({'True': True, 'False': False})

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[True], bar_width, label='Churn=True')
plt.bar(index + bar_width, counts[False], bar_width, label='Churn=False')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/31:
# Label encoding

from sklearn.preprocessing import LabelEncoder

# Assuming 'dataset' is your DataFrame
columns_to_encode = ['Name', 'Age', 'Gender', 'Location']

label_encoder = LabelEncoder()

for column in columns_to_encode:
    Label_encoded = label_encoder.fit_transform(dataset[column])
    print(Label_encoded)
51/32:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = Label_encoded['Gender'].unique()
churn_labels = dataset['Churn'].unique()

# Convert 'Churn' to boolean
dataset['Churn'] = dataset['Churn'].map({'True': True, 'False': False})

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[True], bar_width, label='Churn=True')
plt.bar(index + bar_width, counts[False], bar_width, label='Churn=False')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/33:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = dataset['Gender'].unique()
churn_labels = dataset['Churn'].unique()

# Convert 'Churn' to boolean
dataset['Churn'] = dataset['Churn'].map({'True': True, 'False': False})

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[True], bar_width, label='Churn=True')
plt.bar(index + bar_width, counts[False], bar_width, label='Churn=False')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/34:
# Label encoding

from sklearn.preprocessing import LabelEncoder

data_name = dataset['Name']  
label_encoder = LabelEncoder()
df_Name = label_encoder.fit_transform(data_name)
print(df_Name)

data_Gender = dataset['Gender']  
label_encoder = LabelEncoder()
df_Gender = label_encoder.fit_transform(data_Gender)
print(df_Gender)

data_Location = dataset['Location']  
label_encoder = LabelEncoder()
df_Location = label_encoder.fit_transform(data_Location)
print(df_Location)
51/35:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = df_Gender
churn_labels = dataset['Churn']

# Convert 'Churn' to boolean
dataset['Churn'] = dataset['Churn'].map({'True': True, 'False': False})

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[True], bar_width, label='Churn=True')
plt.bar(index + bar_width, counts[False], bar_width, label='Churn=False')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/36:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = df_Gender.unique
churn_labels = dataset['Churn'].unique

# Convert 'Churn' to boolean
dataset['Churn'] = dataset['Churn'].map({'True': True, 'False': False})

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[True], bar_width, label='Churn=True')
plt.bar(index + bar_width, counts[False], bar_width, label='Churn=False')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/37:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = df_Gender.unique()
churn_labels = dataset['Churn'].unique()

# Convert 'Churn' to boolean
dataset['Churn'] = dataset['Churn'].map({'True': True, 'False': False})

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[True], bar_width, label='Churn=True')
plt.bar(index + bar_width, counts[False], bar_width, label='Churn=False')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/38:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = df_Gender.unique()
churn_labels = dataset['Churn'].unique()

# Convert 'Churn' to boolean
dataset['Churn'] = dataset['Churn'].map({'True': True, 'False': False})

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[True], bar_width, label='Churn=True')
plt.bar(index + bar_width, counts[False], bar_width, label='Churn=False')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/39:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = df_Gender.unique()
churn_labels = dataset['Churn'].unique()

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[1], bar_width, label='Churn=1')
plt.bar(index + bar_width, counts[0], bar_width, label='Churn=0')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/40:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = np.unique(df_Gender)
churn_labels = np.unique(dataset['Churn'])

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[1], bar_width, label='Churn=1')
plt.bar(index + bar_width, counts[0], bar_width, label='Churn=0')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/41:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = np.unique(df_Gender)
churn_labels = np.unique(dataset['Churn'])

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[1], bar_width, label='Churn=1')
plt.bar(index + bar_width, counts[0], bar_width, label='Churn=0')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/42:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = np.unique(df_Gender)
churn_labels = np.unique(dataset['Churn'])

# Convert 'Churn' to boolean (if needed)
# Skip this step since 'Churn' is already binary

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts['1'], bar_width, label='Churn=1')
plt.bar(index + bar_width, counts['0'], bar_width, label='Churn=0')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/43:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = np.unique(df_Gender)
churn_labels = np.unique(dataset['Churn'])

# Convert 'Churn' to boolean (if needed)
# Skip this step since 'Churn' is already binary

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts['1'], bar_width, label='Churn=1')
plt.bar(index + bar_width, counts['0'], bar_width, label='Churn=0')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/44:
fig, ax = plt.subplots(1, figsize=(5,5))
plt.suptitle('Gender Split', fontsize=15)
ax.bar(dataset['Gender'].value_counts().index, 
       dataset['Gender'].value_counts().values,
       color = ['darkblue', 'darkorange'])
ax.set_xticks(range(0, 2))
ax.set_xticklabels(['Male','Female'], fontsize = 14);
51/45:
fig, ax = plt.subplots(1, figsize=(5,5))
plt.suptitle('Gender Split', fontsize=15)
ax.bar(dataset['Gender'].value_counts().index, 
       dataset['Churn'].value_counts().values,
       color = ['darkblue', 'darkorange'])
ax.set_xticks(range(0, 2))
ax.set_xticklabels(['Male','Female'], fontsize = 14);
51/46:
fig, ax = plt.subplots(1, figsize=(5,5))
plt.suptitle('Gender Split', fontsize=15)
ax.bar(dataset['Churn'].value_counts().index, 
       dataset['Gender'].value_counts().values,
       color = ['darkblue', 'darkorange'])
ax.set_xticks(range(0, 2))
ax.set_xticklabels(['Male','Female'], fontsize = 14);
51/47:
plt.figure()
dataset.groupby('Churn')['Gender'].hist()
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Histogram of Gender vs. Churn')
plt.legend()
plt.show()
51/48:
plt.figure()
dataset.groupby('Churn')[df_Gender].hist()
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Histogram of Gender vs. Churn')
plt.legend()
plt.show()
51/49:
plt.figure()
dataset.groupby('Churn'),[df_Gender].hist()
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Histogram of Gender vs. Churn')
plt.legend()
plt.show()
51/50:
plt.figure()
dataset.groupby('Churn')['Gender'].apply(plt.hist)
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Histogram of Gender vs. Churn')
plt.legend()
plt.show()
51/51:
gender = df_Gender.value_counts()
plt.figure(figsize=(7, 6))
ax = gender.plot(kind='bar', rot=0, color="c")
ax.set_title("Bar Graph of Gender", y = 1)
ax.set_xlabel('Gender')
ax.set_ylabel('Number of People')
ax.set_xticklabels(('Male', 'Female'))
51/52:
import pandas as pd

# Assuming df_Gender is a numpy array
df_Gender = pd.Series(df_Gender)

# Count the occurrences of each gender
gender_counts = df_Gender.value_counts()

# Plotting
plt.figure(figsize=(7, 6))
ax = gender_counts.plot(kind='bar', rot=0, color="c")
ax.set_title("Bar Graph of Gender", y=1)
ax.set_xlabel('Gender')
ax.set_ylabel('Number of People')
ax.set_xticklabels(('Male', 'Female'))

# Adding y label for Churn
plt.text(-0.5, max(gender_counts) + 1000, 'Churn: 0', fontsize=12, color='blue')
plt.text(0.5, max(gender_counts) + 1000, 'Churn: 1', fontsize=12, color='blue')

plt.show()
51/53:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = df_Gender.unique()
churn_labels = dataset['Churn'].unique()

# Convert 'Churn' to boolean
dataset['Churn'] = dataset['Churn'].astype(bool)

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[False], bar_width, label='Churn=0')
plt.bar(index + bar_width, counts[True], bar_width, label='Churn=1')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn Prediction by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/54:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = df_Gender.unique()
churn_labels = dataset['Churn'].unique()

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[False], bar_width, label='Churn=0')
plt.bar(index + bar_width, counts[True], bar_width, label='Churn=1')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn Prediction by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/55:
import matplotlib.pyplot as plt

# Assuming 'Gender' and 'Churn' are categorical variables
gender_labels = df_Gender
churn_labels = dataset['Churn']

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(gender_labels))

plt.bar(index, counts[False], bar_width, label='Churn=0')
plt.bar(index + bar_width, counts[True], bar_width, label='Churn=1')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn Prediction by Gender')
plt.xticks(index + bar_width / 2, gender_labels)
plt.legend()

plt.show()
51/56:
import matplotlib.pyplot as plt

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(counts))

plt.bar(index, counts[0], bar_width, label='Churn=0')
plt.bar(index + bar_width, counts[1], bar_width, label='Churn=1')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn Prediction by Gender')
plt.xticks(index + bar_width / 2, counts.index)
plt.legend()

plt.show()
51/57:
import matplotlib.pyplot as plt

# Count occurrences of combinations of 'Gender' and 'Churn'
counts = dataset.groupby(['Gender', 'Churn']).size().unstack(fill_value=0)

# Create a grouped bar chart
bar_width = 0.35
index = np.arange(len(counts))

plt.bar(index, counts[0], bar_width, label='Churn=0')
plt.bar(index + bar_width, counts[1], bar_width, label='Churn=1')

plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.title('Churn Prediction by Gender')
plt.xticks(index + bar_width / 2, counts.index)
plt.legend()

plt.show()
51/58:
name = dataset['Gender'].head(12)
price = dataset['Churn'].head(12)
 
# Figure Size
fig = plt.figure(figsize =(10, 7))
 
# Horizontal Bar Plot
plt.bar(name[0:10], price[0:10])
 
# Show Plot
plt.show()
51/59:
plt.figure()
dataset.groupby('Churn')['Gender'].hist()
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Histogram of Gender vs. Churn')
plt.legend()
plt.show()
51/60:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.figure()
dataset.groupby('Gender')['Churn'].value_counts().unstack().plot(kind='bar')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Bar Plot of Churn vs. Gender')
plt.legend()
plt.show()
51/61:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.figure()
churn_count_df = dataset.groupby(['Gender', 'Churn']).size().unstack()
plt.figure()
churn_count_df.plot(kind='bar')
plt.xlabel('Gender')
plt.ylabel('Churn Count')
plt.title('Bar Plot of Churn vs. Gender for Males and Females')
plt.legend()
plt.show()
51/62:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
churn_count_df = dataset.groupby(['Gender', 'Churn']).size().unstack()
plt.figure()
churn_count_df.plot(kind='bar')
plt.xlabel('Gender and Churn')
plt.ylabel('Churn Count')
plt.title('Bar Plot of Churn vs. Gender for Males and Females')
plt.xticks(['Females Churn 0', 'Females Churn 1', 'Males Churn 0', 'Males Churn 1'])
plt.legend()
plt.show()
51/63:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

tick_labels = ['Females Churn 0', 'Females Churn 1', 'Males Churn 0', 'Males Churn 1']
tick_values = np.arange(len(tick_labels))

plt.xticks(tick_values, tick_labels)
plt.bar(['Females Churn 0', 'Females Churn 1', 'Males Churn 0', 'Males Churn 1'], churn_count, use_category_order=True)
51/64:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

tick_labels = ['Females Churn 0', 'Females Churn 1', 'Males Churn 0', 'Males Churn 1']
tick_values = np.arange(len(tick_labels))

plt.xticks(tick_values, tick_labels)
churn_count = dataset.groupby(['Gender', 'Churn']).size().unstack()
churn_count.plot(kind='bar')
plt.bar(['Females Churn 0', 'Females Churn 1', 'Males Churn 0', 'Males Churn 1'], churn_count, use_category_order=True)

plt.xlabel('Gender and Churn')
plt.ylabel('Churn Count')
plt.title('Bar Plot of Churn vs. Gender for Males and Females')
plt.xticks(['Females Churn 0', 'Females Churn 1', 'Males Churn 0', 'Males Churn 1'])
plt.legend()
plt.show()
51/65:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

tick_labels = ['Females Churn 0', 'Females Churn 1', 'Males Churn 0', 'Males Churn 1']
tick_values = np.arange(len(tick_labels))

plt.xticks(tick_values, tick_labels)
churn_count = dataset.groupby(['Gender', 'Churn']).size().unstack()
churn_count.plot(kind='bar')
plt.bar(['Females Churn 0', 'Females Churn 1', 'Males Churn 0', 'Males Churn 1'], churn_count, use_category_order=True)
plt.show()
51/66:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
churn_count_df = dataset.groupby(['Gender', 'Churn']).size().unstack()

churn_rate_df = churn_count_dataset.div(churn_count_dataset.sum(axis=1), axis=0).mul(100).round(2)
plt.figure()
churn_rate_df.plot(kind='pie', autopct="%1.1f%%")
plt.title('Churn Rate for Males and Females')
plt.show()
51/67:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
churn_count_df = dataset.groupby(['Gender', 'Churn']).size().unstack()
plt.figure()
churn_count_df.plot(kind='pie', autopct="%1.1f%%")
plt.title('Churn Rate for Males and Females')
plt.show()
51/68:
import pandas as pd
import matplotlib.pyplot as plt

# Create a DataFrame with the churn count for males and females
churn_count_df = pd.DataFrame({'Gender': ['Females', 'Males'], 'Churn Status': ['Churned', 'Not Churned'], 'Churn Count': [100, 50, 150, 75]})

# Create a pie chart of the churn count for males and females
plt.figure()
churn_count_df.plot(kind='pie', x='Churn Status', y='Churn Count', by='Gender', autopct="%1.1f%%", subplots=True)
plt.title('Churn Rate for Males and Females by Churn Status')
plt.show()
51/69:
# Plot histogram of predicted probabilities
x = df_Gender
y = dataset['Churn']
plt.hist(x, y, bins=4) 
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
# Display the plot
plt.show()
51/70:
import numpy as np
import matplotlib.pyplot as plt

# Create a list of evenly spaced bins
bins = np.arange(0, 1.1, 0.25)

# Create the histogram
hist, bin_edges = np.histogram(x, bins=bins)

# Plot the histogram
plt.bar(bin_edges[:-1], hist)
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
plt.show()
51/71:
import numpy as np
import matplotlib.pyplot as plt

x = df_Gender
y = dataset['Churn']

# Create a list of evenly spaced bins
bins = np.arange(0, 1.1, 0.25)

# Plot the histogram
plt.hist(x, y, bins=bins)
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
plt.show()
51/72:
import numpy as np
import matplotlib.pyplot as plt

x = df_Gender
y = dataset['Churn']

# Create a list of evenly spaced bins
bins = np.arange(0, 1)

# Plot the histogram
plt.hist(x, y, bins=bins)
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
plt.show()
51/73:
import numpy as np
import matplotlib.pyplot as plt

x = df_Gender
y = dataset['Churn']

# Create a list of evenly spaced bins
bins = np.arange(0, 1, 1, 1)

# Plot the histogram
plt.hist(x, y, bins=bins)
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
plt.show()
52/1:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
52/2:
# Read the CSV file

customer_data = pd.read_excel(r"C:\Users\sound\OneDrive\Desktop\Internshala work\customer_churn_large_dataset.xlsx")
display(customer_data)
52/3:
# Check for missing values

missing_values = customer_data.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
52/4:
#  Check for duplicate rows

duplicates = customer_data.duplicated()
duplicate_rows = customer_data[duplicates]
print(duplicate_rows)
df_cleaned = customer_data.drop_duplicates()
display(df_cleaned)
52/5:
# Drop irrelevant columns
columns_to_drop = ['Total_Usage_GB']
dataset = customer_data.drop(columns_to_drop, axis=1)
display(dataset)
52/6:
# Checking for outliers
dataset.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20, 10))
dataset.boxplot(column=['Monthly_Bill'], ax=ax)
outlier_count = dataset.count()
plt.title("Boxplot with Outliers")
52/7:
# Label encoding

from sklearn.preprocessing import LabelEncoder

data_name = dataset['Name']  
label_encoder = LabelEncoder()
df_Name = label_encoder.fit_transform(data_name)
print(df_Name)

data_Gender = dataset['Gender']  
label_encoder = LabelEncoder()
df_Gender = label_encoder.fit_transform(data_Gender)
print(df_Gender)

data_Location = dataset['Location']  
label_encoder = LabelEncoder()
df_Location = label_encoder.fit_transform(data_Location)
print(df_Location)
52/8:
import numpy as np
import matplotlib.pyplot as plt

x = dataset['']
y = dataset['Churn']

# Create a list of evenly spaced bins
bins = np.arange(0, 1, 1, 1)

# Plot the histogram
plt.hist(x, y, bins=bins)
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
plt.show()
52/9:
import numpy as np
import matplotlib.pyplot as plt

x = dataset['Gender']
y = dataset['Churn']

# Create a list of evenly spaced bins
bins = np.arange(0, 1, 1, 1)

# Plot the histogram
plt.hist(x, y, bins=bins)
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
plt.show()
52/10:
import numpy as np
import matplotlib.pyplot as plt

x = dataset['Gender']
y = dataset['Churn']

# Create a list of evenly spaced bins
bins = np.arange(0, 1.1, 0.25)

# Plot the histogram
plt.hist(x, y, bins=bins)
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
plt.show()
52/11:
import numpy as np
import matplotlib.pyplot as plt

x = dataset['Gender']
y = dataset['Churn']

import numpy as np
import matplotlib.pyplot as plt

# Create a list of evenly spaced bins
bins = np.arange(0, 1.1, 0.25)

# Create the histogram
hist, bin_edges = np.histogram(x, bins=bins)

# Plot the histogram
plt.bar(bin_edges[:-1], hist)
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
plt.show()
52/12:
import numpy as np
import matplotlib.pyplot as plt

x = df_Gender
y = dataset['Churn']

import numpy as np
import matplotlib.pyplot as plt

# Create a list of evenly spaced bins
bins = np.arange(0, 1.1, 0.25)

# Create the histogram
hist, bin_edges = np.histogram(x, bins=bins)

# Plot the histogram
plt.bar(bin_edges[:-1], hist)
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
plt.show()
52/13:
import numpy as np
import matplotlib.pyplot as plt

x = df_Gender
y = dataset['Churn']

import numpy as np
import matplotlib.pyplot as plt

# Create a list of evenly spaced bins
bins = np.arange(0, 1.1, 0.25)

# Create the histogram
hist, bin_edges = np.histogram(x, bins=bins)

# Plot the histogram
plt.bar(x, y, bin_edges[:-1], hist)
plt.xlabel('Predicted Probabilities')
plt.ylabel('Frequency')
plt.title('Customer Churn Prediction')
plt.show()
53/1:
import pandas as pd

drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

source_column_name = 'Column Name'
drug_df1['Drug Name'] = 'Eplerenone'
drug_df1['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics']
destination_df = pd.read_excel('destination.xlsx')
df = pd.concat([drug_df1, destination_df], axis=1)
df.to_excel('combined.csv', index=False)
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=[source_column_name])
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/2:
import pandas as pd

drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

source_column_name = 'Column Name'
drug_df1['Drug Name'] = 'Eplerenone'
drug_df1['Drug classes'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'
destination_df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
df = pd.concat([drug_df1, destination_df], axis=1)
df.to_excel('combined.csv', index=False)
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=[source_column_name])
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/3:
import pandas as pd

drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

source_column_name = 'Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID'
drug_df1['Drug Name'] = 'Eplerenone'
drug_df1['Drug classes'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'
destination_df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
df = pd.concat([drug_df1, destination_df], axis=1)
df.to_excel('combined.csv', index=False)
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=[source_column_name])
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/4:
import pandas as pd
drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')
source_column_name = 'Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID'
drug_df1['Drug Name'] = 'Eplerenone'
drug_df1['Drug classes'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'
destination_df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
df = pd.concat([drug_df1, destination_df], axis=1)
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=[source_column_name])
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=[source_column_name])
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/5:
import pandas as pd
drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')
source_column_name = 'Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID'
drug_df1['Drug Name'] = 'Eplerenone'
drug_df1['Drug classes'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'
destination_df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
df = pd.concat([drug_df1, destination_df], axis=1)
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=[source_column_name])
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=[source_column_name])
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/6:
import pandas as pd
drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')
source_column_name = 'Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID'
drug_df1['Drug Name'] = 'Eplerenone'
drug_df1['Drug classes'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'
destination_df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
df = pd.concat([drug_df1, destination_df], axis=1)
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=[source_column_name])
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=[source_column_name])
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/7:
import pandas as pd
drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')
source_column_name = 'Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID'
drug_df1['Drug Name'] = 'Eplerenone'
drug_df1['Drug classes'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'
destination_df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')
df = pd.concat([drug_df1, destination_df], axis=1)
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=[source_column_name])
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=[source_column_name])
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/8:
import pandas as pd

# Read the source CSV file into a Pandas DataFrame
drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')
source_column_name = 'Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID'

# Read the destination CSV file into a Pandas DataFrame
destination_df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)

# Combine the two DataFrames
combined_df = pd.concat(drug_df1, destination_df], axis=1)

# Add the new columns
combined_df['Drug Name'] =  'Eplerenone'
combined_df['Drug Class'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'

# Write the combined DataFrame to a new CSV file
combined_df.to_csv('combined.csv', index=False)
53/9:
import pandas as pd

# Read the source CSV file into a Pandas DataFrame
drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')
source_column_name = 'Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID'

# Read the destination CSV file into a Pandas DataFrame
destination_df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)

# Combine the two DataFrames
combined_df = pd.concat([drug_df1, destination_df], axis=1)

# Add the new columns
combined_df['Drug Name'] =  'Eplerenone'
combined_df['Drug Class'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'

# Write the combined DataFrame to a new CSV file
combined_df.to_csv('combined.csv', index=False)
53/10:
import pandas as pd

# Read the source CSV file into a Pandas DataFrame
drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')
source_column_name = 'Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID'

# Read the destination CSV file into a Pandas DataFrame
destination_df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Combine the two DataFrames
combined_df = pd.concat([drug_df1, destination_df], axis=1)

# Add the new columns
combined_df['Drug Name'] =  'Eplerenone'
combined_df['Drug Class'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'

# Write the combined DataFrame to a new CSV file
combined_df.to_csv('combined.csv', index=False)
53/11:
import os

destination_file_path = r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv'

if os.path.exists(destination_file_path) and os.stat(destination_file_path).st_size > 0:
    destination_df = pd.read_csv(destination_file_path)
else:
    print("Destination CSV file is empty or doesn't exist.")
53/12:
import pandas as pd

# Read the CSV file
drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

# Add new columns
drug_df1['Drug Name'] = 'Eplerenone'
drug_df1['Drug classes'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'

# Concatenate the DataFrame with itself
df = pd.concat([drug_df1, drug_df1], axis=1)

# Write to an Excel file
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.xlsx', index=False)

# Read specific columns from the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID'])

# Write to another Excel file
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\selected_columns.xlsx', index=False)
53/13:
import pandas as pd

# Read the CSV file
drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

# Add new columns
drug_df1['Drug Name'] = 'Eplerenone'
drug_df1['Drug classes'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'

# Concatenate the DataFrame with itself
df = pd.concat([drug_df1, drug_df1], axis=1)

# Write to an Excel file
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.xlsx', index=False)

# Read specific columns from the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID'])

# Write to another Excel file
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\selected_columns.xlsx', index=False)
53/14:
import pandas as pd

# Read the CSV file
drug_df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

# Add new columns
drug_df1['Drug Name'] = 'Drug Name'
drug_df1['Drug classes'] = 'Drug classes'

# Add new columns with names
drug_df1['Name_1'] = 'Eplerenone'
drug_df1['Name_2'] = 'Aldosterone receptor antagonists, Potassium-sparing diuretics'

# Duplicate each row 100 times
df = pd.concat([drug_df1]*100, ignore_index=True)

# Write to an Excel file
df.to_excel(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.xlsx', index=False)

# Read specific columns from the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv', usecols=['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID'])

# Duplicate each row 100 times
df = pd.concat([df]*100, ignore_index=True)
53/15:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

# Add new columns with specified values repeated 100 times
df['Drug Name'] = ['Eplerenone'] * 100
df['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 100

# Reorder the columns to have 'Drug Name' and 'Drug classes' as the first two columns
df = df[['Drug Name', 'Drug classes'] + [col for col in df if col not in ['Drug Name', 'Drug classes']]]

# Write to an Excel file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/16:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

# Add new columns with specified values repeated 100 times
df['Drug Name'] = ['Eplerenone'] * 99
df['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 99

# Reorder the columns to have 'Drug Name' and 'Drug classes' as the first two columns
df = df[['Drug Name', 'Drug classes'] + [col for col in df if col not in ['Drug Name', 'Drug classes']]]

# Write to an Excel file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/17:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

# Add new columns with specified values repeated 100 times
df['Drug Name'] = ['Eplerenone'] * 99
df['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 99

# Reorder the columns to have 'Drug Name' and 'Drug classes' as the first two columns
df = df[['Drug Name', 'Drug classes'] + [col for col in df if col not in ['Drug Name', 'Drug classes']]]

# Write to an Excel file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
53/18:
import pandas as pd

# Read the CSV file
df2 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Spironolactone (drug 2 aldostrone for bp).csv')

# Select specific columns
selected_columns = df2[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 99 times (not 100 times as it was mentioned in the comment)
df2['Drug Name'] = ['Spironolactone'] * 99
df2['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 99

# Reorder the columns to have 'Drug Name' and 'Drug classes' as the first two columns
df2 = df2[['Drug Name', 'Drug classes'] + [col for col in df2 if col not in ['Drug Name', 'Drug classes']]]

# Write to a CSV file (not Excel)
df2.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/19:
import pandas as pd

# Read the CSV file
df2 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Spironolactone (drug 2 aldostrone for bp).csv')

# Select specific columns
selected_columns = df2[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 99 times (not 100 times as it was mentioned in the comment)
df2['Drug Name'] = ['Spironolactone'] * 100
df2['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 100

# Reorder the columns to have 'Drug Name' and 'Drug classes' as the first two columns
df2 = df2[['Drug Name', 'Drug classes'] + [col for col in df2 if col not in ['Drug Name', 'Drug classes']]]

# Write to a CSV file (not Excel)
df2.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/20:
import pandas as pd

# Read the CSV file
df2 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Spironolactone (drug 2 aldostrone for bp).csv')

# Select specific columns
selected_columns = df2[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 99 times (not 100 times as it was mentioned in the comment)
df2['Drug Name'] = ['Spironolactone'] * 100
df2['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 100

# Reorder the columns to have 'Drug Name' and 'Drug classes' as the first two columns
df2 = df2[['Drug Name', 'Drug classes'] + [col for col in df2 if col not in ['Drug Name', 'Drug classes']]]

# Write to a CSV file (not Excel)
df2.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/21:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

# Add new columns with specified values repeated 100 times
df['Drug Name'] = ['Eplerenone'] * 99
df['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 99

# Reorder the columns to have 'Drug Name' and 'Drug classes' as the first two columns
df = df[['Drug Name', 'Drug classes'] + [col for col in df if col not in ['Drug Name', 'Drug classes']]]

# Write to an Excel file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
53/22:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

# Add new columns with specified values repeated 100 times
df['Drug Name'] = ['Eplerenone'] * 99
df['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 99

# Reorder the columns to have 'Drug Name' and 'Drug classes' as the first two columns
df = df[['Drug Name', 'Drug classes'] + [col for col in df if col not in ['Drug Name', 'Drug classes']]]

# Write to an Excel file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
53/23:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Spironolactone (drug 2 aldostrone for bp).csv')

# Select specific columns
selected_columns = df[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df['Drug Name'] = ['Spironolactone'] * 100
df['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 100

# Reorder the columns
df_source = df['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID', 'Drug Name', 'Drug classes']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
53/24:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Spironolactone (drug 2 aldostrone for bp).csv')

# Select specific columns
selected_columns = df[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df['Drug Name'] = ['Spironolactone'] * 100
df['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 100

# Reorder the columns
df_source = df[['Sl.No', 'Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID',]]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
53/25:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Spironolactone (drug 2 aldostrone for bp).csv')

# Select specific columns
selected_columns = df[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df['Drug Name'] = ['Spironolactone'] * 100
df['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 100

# Reorder the columns
df_source = df[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID',]]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
53/26:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Spironolactone (drug 2 aldostrone for bp).csv')

# Select specific columns
selected_columns = df[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df['Drug Name'] = ['Spironolactone'] * 100
df['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 100

# Reorder the columns
df_source = df[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID',]]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
53/27:
import pandas as pd

# Read the first CSV file
df_source = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Spironolactone (drug 2 aldostrone for bp).csv')

# Select specific columns
selected_columns = df_source[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df_source['Drug Name'] = ['Spironolactone'] * 100
df_source['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 100

# Reorder the columns
df_source = df_source[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
53/28:
import pandas as pd

# Read the first CSV file
df_source = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Spironolactone (drug 2 aldostrone for bp).csv')

# Select specific columns
selected_columns = df_source[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df_source['Drug Name'] = ['Spironolactone'] * 100
df_source['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 100

# Reorder the columns
df_source = df_source[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')


# Merge the two DataFrames
combined_df = pd.concat([df_destination, df_source], ignore_index=True)
53/29:
import pandas as pd

# Read the first CSV file
df_source = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Spironolactone (drug 2 aldostrone for bp).csv')

# Select specific columns
selected_columns = df_source[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df_source['Drug Name'] = ['Spironolactone'] * 100
df_source['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 100

# Reorder the columns
df_source = df_source[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df_source], ignore_index=True)


# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/30:
import pandas as pd

# Read the first CSV file
df3 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\lisinopril (drug 3 angiotensin for bp).csv')

# Select specific columns
selected_columns = df3[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df3['Drug Name'] = ['Lisinopril'] * 100
df3['Drug classes'] = ['Angiotensin Converting Enzyme Inhibitors'] * 100

# Reorder the columns
df3 = df3[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df3], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/31:
import pandas as pd

# Read the first CSV file
df4 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Enalapril (drug 4 angiotensin for bp).csv')

# Select specific columns
selected_columns = df3[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df4['Drug Name'] = ['Enalapril '] * 100
df4['Drug classes'] = ['Angiotensin Converting Enzyme Inhibitors'] * 100

# Reorder the columns
df4 = df4[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df4], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/32:
import pandas as pd

# Read the first CSV file
df4 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Enalapril (drug 4 angiotensin for bp).csv')

# Select specific columns
selected_columns = df3[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df4['Drug Name'] = ['Enalapril '] * 100
df4['Drug classes'] = ['Angiotensin Converting Enzyme Inhibitors'] * 100

# Reorder the columns
df4 = df4[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df4], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/33:
import pandas as pd

# Read the first CSV file
df4 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Benazepril (drug 5 angiotensin for bp).csv')

# Select specific columns
selected_columns = df3[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df4['Drug Name'] = ['Benazepril'] * 100
df4['Drug classes'] = ['Angiotensin Converting Enzyme Inhibitors'] * 100

# Reorder the columns
df4 = df4[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df4], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/34:
import pandas as pd

# Read the first CSV file
df6 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\doxazosin (drug 6 peripherally acting antiadrenerigic agent).csv')

# Select specific columns
selected_columns = df3[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df6['Drug Name'] = ['Doxazosin'] * 100
df6['Drug classes'] = ['Alpha-adrenoreceptor antagonists, Antiadrenergic agents, peripherally acting'] * 100

# Reorder the columns
df6 = df6[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df6], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/35:
import pandas as pd

# Read the first CSV file
df7 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Prazosin (drug 7 antiadrenergic agent peripheral).csv')

# Select specific columns
selected_columns = df3[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df7['Drug Name'] = ['Prazosin'] * 100
df7['Drug classes'] = ['Antiadrenergic agents, peripherally acting'] * 100

# Reorder the columns
df7 = df7[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df7], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/36:
import pandas as pd

# Read the first CSV file
df8 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Terazosin (drug 8 Antiadrenergic agent peripheral).csv')

# Select specific columns
selected_columns = df3[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df8['Drug Name'] = ['Terazosin'] * 100
df8['Drug classes'] = ['Alpha-adrenoreceptor antagonists, Antiadrenergic agents, peripherally acting'] * 100

# Reorder the columns
df8 = df8[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df8], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/37:
import pandas as pd

# Read the first CSV file
df9 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Clonidine (drug 9, Antiadrenergic agent centrally).csv')

# Select specific columns
selected_columns = df3[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df9['Drug Name'] = ['Terazosin'] * 100
df9['Drug classes'] = ['Alpha-adrenoreceptor antagonists, Antiadrenergic agents, peripherally acting'] * 100

# Reorder the columns
df9 = df9[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df9], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/38:
import pandas as pd

# Read the first CSV file
df9 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Clonidine (drug 9, Antiadrenergic agent centrally).csv')

# Select specific columns
selected_columns = df3[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df9['Drug Name'] = ['Terazosin'] * 68
df9['Drug classes'] = ['Alpha-adrenoreceptor antagonists, Antiadrenergic agents, peripherally acting'] * 68

# Reorder the columns
df9 = df9[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df9], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/39:
import pandas as pd

# Read the first CSV file
df10 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Nitroglycerine (drug 10, Antianginal agent).csv')

# Select specific columns
selected_columns = df10[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df10['Drug Name'] = ['Nitroglycerine'] * 68
df10['Drug classes'] = ['Antianginal agents, Vasodilators'] * 68

# Reorder the columns
df10 = df10[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df10], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/40:
import pandas as pd

# Read the first CSV file
df10 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Nitroglycerine (drug 10, Antianginal agent).csv')

# Select specific columns
selected_columns = df10[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df10['Drug Name'] = ['Nitroglycerine'] * 2
df10['Drug classes'] = ['Antianginal agents, Vasodilators'] * 2

# Reorder the columns
df10 = df10[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df10], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv', index=False)
53/41:
import pandas as pd
import matplotlib.pyplot as plt

large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Split Data by Drug
for drug_name, drug_df in large_dataset.groupby('Drug Name'):

    # Step 3: Create Pie Charts
    labels = drug_df['Target Class']
    sizes = drug_df['Probability*']

    plt.figure(figsize=(8, 8))
    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(labels))))
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Probabilities')

    # Step 4: Display or Save Charts
    plt.show()
53/42:
import pandas as pd
import matplotlib.pyplot as plt

large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Split Data by Drug
for drug_name, drug_df in large_dataset.groupby('Drug Name'):

    # Step 3: Create Pie Charts
    labels = drug_df['Target Class']
    sizes = drug_df['Probability*']

    plt.figure(figsize=(8, 8))
    plt.pie(sizes, labels=None, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(labels))))
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Probabilities')

    # Step 4: Display or Save Charts
    plt.show()
53/43:
import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'large_dataset' with columns 'Drug Name', 'Target Class', 'Probability*'
# You need to replace 'large_dataset' with your actual DataFrame name.

# Step 1: Load the Data
# Load your large dataset (replace 'large_dataset.csv' with your actual file path)
large_dataset = pd.read_csv('large_dataset.csv')

# Step 2: Split Data by Drug
for drug_name, drug_df in large_dataset.groupby('Drug Name'):

    # Step 3: Create Pie Charts
    labels = drug_df['Target Class']
    sizes = drug_df['Probability*'].fillna(0.0)  # Fill missing values with 0.0

    plt.figure(figsize=(8, 8))
    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(labels))))
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Probabilities')

    # Step 4: Display or Save Charts
    plt.show()  # To display the chart
    # plt.savefig(f'{drug_name}_pie_chart.png')  # To save the chart as an image
53/44:
import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'large_dataset' with columns 'Drug Name', 'Target Class', 'Probability*'
# You need to replace 'large_dataset' with your actual DataFrame name.

# Step 1: Load the Data
# Load your large dataset (replace 'large_dataset.csv' with your actual file path)
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Split Data by Drug
for drug_name, drug_df in large_dataset.groupby('Drug Name'):

    # Step 3: Create Pie Charts
    labels = drug_df['Target Class']
    sizes = drug_df['Probability*'].fillna(0.0)  # Fill missing values with 0.0

    plt.figure(figsize=(8, 8))
    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(labels))))
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Probabilities')

    # Step 4: Display or Save Charts
    plt.show()  # To display the chart
    # plt.savefig(f'{drug_name}_pie_chart.png')  # To save the chart as an image
53/45:
import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'large_dataset' with columns 'Drug Name', 'Target Class', 'Probability*'
# You need to replace 'large_dataset' with your actual DataFrame name.

# Step 1: Load the Data
# Load your large dataset (replace 'large_dataset.csv' with your actual file path)
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Split Data by Drug
for drug_name, drug_df in large_dataset.groupby('Drug Name'):

    # Step 3: Create Small Pie Charts
    labels = drug_df['Target Class']
    sizes = drug_df['Probability*'].fillna(0.0)  # Fill missing values with 0.0

    plt.figure(figsize=(6, 6))  # Adjust the size of the figure (smaller)
    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(labels))))
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Probabilities')

    # Step 4: Display or Save Charts
    plt.show()  # To display the chart
    # plt.savefig(f'{drug_name}_pie_chart.png')  # To save the chart as an image
53/46:
import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'large_dataset' with columns 'Drug Name', 'Target Class', 'Probability*'
# You need to replace 'large_dataset' with your actual DataFrame name.

# Step 1: Load the Data
# Load your large dataset (replace 'large_dataset.csv' with your actual file path)
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Step 3: Create a Pie Chart for Each Drug
for drug_name in unique_drugs:
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Create Pie Chart
    labels = drug_df['Target Class']
    sizes = drug_df['Probability*'].fillna(0.0)  # Fill missing values with 0.0

    plt.figure(figsize=(6, 6))  # Adjust the size of the figure (smaller)
    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(labels))))
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Probabilities')

    # Step 4: Display or Save Charts
    plt.show()  # To display the chart
    # plt.savefig(f'{drug_name}_pie_chart.png')  # To save the chart as an image
53/47:
import pandas as pd

# Read the CSV file
df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Eplereonone drug target prediction csv file.csv')

# Add new columns with specified values repeated 100 times
df1['Drug Name'] = ['Eplerenone'] * 99
df1['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 99

# Reorder the columns to have 'Drug Name' and 'Drug classes' as the first two columns
df1 = df1[['Drug Name', 'Drug classes'] + [col for col in df1 if col not in ['Drug Name', 'Drug classes']]]

# Write to an Excel file
df1.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
53/48:
import pandas as pd

# Read the first CSV file
df2 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Spironolactone (drug 2 aldostrone for bp).csv')

# Select specific columns
selected_columns = df2[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df2['Drug Name'] = ['Spironolactone'] * 100
df2['Drug classes'] = ['Aldosterone receptor antagonists, Potassium-sparing diuretics'] * 100

# Reorder the columns
df2 = df2[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df2], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
53/49:
import pandas as pd

# Read the first CSV file
df3 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\lisinopril (drug 3 angiotensin for bp).csv')

# Select specific columns
selected_columns = df3[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df3['Drug Name'] = ['Lisinopril'] * 100
df3['Drug classes'] = ['Angiotensin Converting Enzyme Inhibitors'] * 100

# Reorder the columns
df3 = df3[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df3], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
53/50:
import pandas as pd

# Read the first CSV file
df4 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Enalapril (drug 4 angiotensin for bp).csv')

# Select specific columns
selected_columns = df4[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df4['Drug Name'] = ['Enalapril '] * 100
df4['Drug classes'] = ['Angiotensin Converting Enzyme Inhibitors'] * 100

# Reorder the columns
df4 = df4[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df4], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
53/51:
import pandas as pd

# Read the first CSV file
df5 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Benazepril (drug 5 angiotensin for bp).csv')

# Select specific columns
selected_columns = df5[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df5['Drug Name'] = ['Benazepril'] * 100
df5['Drug classes'] = ['Angiotensin Converting Enzyme Inhibitors'] * 100

# Reorder the columns
df5 = df5[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df4], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
53/52:
import pandas as pd

# Read the first CSV file
df5 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Benazepril (drug 5 angiotensin for bp).csv')

# Select specific columns
selected_columns = df5[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df5['Drug Name'] = ['Benazepril'] * 100
df5['Drug classes'] = ['Angiotensin Converting Enzyme Inhibitors'] * 100

# Reorder the columns
df5 = df5[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df5], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
53/53:
import pandas as pd

# Read the first CSV file
df6 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\doxazosin (drug 6 peripherally acting antiadrenerigic agent).csv')

# Select specific columns
selected_columns = df6[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df6['Drug Name'] = ['Doxazosin'] * 100
df6['Drug classes'] = ['Alpha-adrenoreceptor antagonists, Antiadrenergic agents, peripherally acting'] * 100

# Reorder the columns
df6 = df6[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df6], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
53/54:
import pandas as pd

# Read the first CSV file
df7 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Prazosin (drug 7 antiadrenergic agent peripheral).csv')

# Select specific columns
selected_columns = df7[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df7['Drug Name'] = ['Prazosin'] * 100
df7['Drug classes'] = ['Antiadrenergic agents, peripherally acting'] * 100

# Reorder the columns
df7 = df7[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df7], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
53/55:
import pandas as pd

# Read the first CSV file
df8 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Terazosin (drug 8 Antiadrenergic agent peripheral).csv')

# Select specific columns
selected_columns = df8[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df8['Drug Name'] = ['Terazosin'] * 100
df8['Drug classes'] = ['Alpha-adrenoreceptor antagonists, Antiadrenergic agents, peripherally acting'] * 100

# Reorder the columns
df8 = df8[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df8], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
53/56:
import pandas as pd

# Read the first CSV file
df9 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Clonidine (drug 9, Antiadrenergic agent centrally).csv')

# Select specific columns
selected_columns = df9[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df9['Drug Name'] = ['Terazosin'] * 68
df9['Drug classes'] = ['Alpha-adrenoreceptor antagonists, Antiadrenergic agents, peripherally acting'] * 68

# Reorder the columns
df9 = df9[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df9], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
53/57:
import pandas as pd

# Read the first CSV file
df10 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Nitroglycerine (drug 10, Antianginal agent).csv')

# Select specific columns
selected_columns = df10[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df10['Drug Name'] = ['Nitroglycerine'] * 2
df10['Drug classes'] = ['Antianginal agents, Vasodilators'] * 2

# Reorder the columns
df10 = df10[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df10], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
54/1:
import pandas as pd

# Read the first CSV file
df10 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Amaldopine (drug 11, calcium channel blocking agents).csv')

# Select specific columns
selected_columns = df10[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df10['Drug Name'] = ['Amlodipine'] * 68
df10['Drug classes'] = ['Calcium channel blocking agents'] * 68
df10['Chemical structure'] = ['CCOC(=O)C1=C(COCCN)NC(C)=C(C1C1=CC=CC=C1Cl)C(=O)OC'] * 68
 
# Reorder the columns
df10 = df10[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df10], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
54/2:
import pandas as pd

# Read the first CSV file
df10 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Amaldopine (drug 11, calcium channel blocking agents).csv')

# Select specific columns
selected_columns = df10[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df10['Drug Name'] = ['Amlodipine'] * 100
df10['Drug classes'] = ['Calcium channel blocking agents'] * 100
df10['Chemical structure'] = ['CCOC(=O)C1=C(COCCN)NC(C)=C(C1C1=CC=CC=C1Cl)C(=O)OC'] * 100
 
# Reorder the columns
df10 = df10[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df10], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
54/3:
import pandas as pd

# Read the first CSV file
df11 = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\Nifedipine (drug 12, Calcium channel blocking agents).csv")

# Select specific columns
selected_columns = df11[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df11['Drug Name'] = ['Nifedipine'] * 100
df11['Drug classes'] = ['Calcium channel blocking agents'] * 100
df11['Chemical structure'] = ['COC(=O)C1=C(C)NC(C)=C(C1C1=CC=CC=C1[N+]([O-])=O)C(=O)OC'] * 100
 
# Reorder the columns
df10 = df11[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df11], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
54/4:
import pandas as pd

# Read the first CSV file
df12 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Hydralazine(drug 14, vasodilators).csv')

# Select specific columns
selected_columns = df12[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df12['Drug Name'] = ['Hydralazine'] * 100
df12['Drug classes'] = ['Vasodilators'] * 100
df12['Chemical structure'] = ['NNC1=NN=CC2=CC=CC=C12'] * 100
 
# Reorder the columns
df10 = df12[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df12], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
54/5:
import pandas as pd

# Read the first CSV file
df13 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Metoprolol (DRUG 17, cardiovascular beta blockers).csv')

# Select specific columns
selected_columns = df13[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df13['Drug Name'] = ['Metoprolol'] * 100
df13['Drug classes'] = ['Cardioselective beta blockers'] * 100
df13['Chemical structure'] = ['COCCC1=CC=C(OCC(O)CNC(C)C)C=C1'] * 100
 
# Reorder the columns
df13 = df13[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df13], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
54/6:
import pandas as pd

# Read the first CSV file
df14 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Verapamil ( drug 19, Antiarrythmics).csv')

# Select specific columns
selected_columns = df14[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df14['Drug Name'] = ['Verapamil'] * 100
df14['Drug classes'] = ['Calcium channel blocking agents, Group IV antiarrhythmics'] * 100
df14['Chemical structure'] = ['COC1=C(OC)C=C(CCN(C)CCCC(C#N)(C(C)C)C2=CC(OC)=C(OC)C=C2)C=C1'] * 100
 
# Reorder the columns
df14 = df14[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df14], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
54/7:
import pandas as pd

# Read the first CSV file
df15 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\Atenolol (Cardiovascular disease. drug 18).csv')

# Select specific columns
selected_columns = df15[['Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID']]

# Add new columns with specified values repeated 100 times
df15['Drug Name'] = ['Atenolol'] * 100
df15['Drug classes'] = ['Cardioselective beta blockers'] * 100
df15['Chemical structure'] = ['CC(C)NCC(O)COC1=CC=C(CC(N)=O)C=C1'] * 100
 
# Reorder the columns
df15 = df15[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df15], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
54/8:
import pandas as pd

# Read the first CSV file
df1 = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Add new columns with specified values repeated 100 times
df1['Chemical structure'] = ['[H][C@@]12CC[C@@]3(CCC(=O)O3)[C@@]1(C)C[C@H]1O[C@@]11[C@@]2([H])[C@@H](CC2=CC(=O)CC[C@]12C)C(=O)OC'] * 100
 
# Reorder the columns
df1 = df1[['Drug Name', 'Drug classes','Target Class', 'Target', 'Common name', 'Probability*', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']]

# Read the destination CSV file
df_destination = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Merge the two DataFrames
combined_df = pd.concat([df_destination, df1], ignore_index=True)

# Write the combined DataFrame to a CSV file
combined_df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
54/9:
import pandas as pd

# Read the first CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

for index, row in df.iterrows():
    drug_name = row['Eplerenone']
    chemical_structure = row['[H][C@@]12CC[C@@]3(CCC(=O)O3)[C@@]1(C)C[C@H]1O[C@@]11[C@@]2([H])[C@@H](CC2=CC(=O)CC[C@]12C)C(=O)OC'] * 100
    
    # Assuming you want to add the chemical structure to the respective drug column
    df.loc[df['Drug_Name'] == drug_name, drug_name] = chemical_structure

df.drop(columns=['Chemical_Structure'], inplace=True)

# Write the combined DataFrame to a CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')
54/10:
import pandas as pd

# Read the first CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Print out the column names
print(df.columns)
54/11:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Eplerenone'  
chemical_structure_to_add = ['[H][C@@]12CC[C@@]3(CCC(=O)O3)[C@@]1(C)C[C@H]1O[C@@]11[C@@]2([H])[C@@H](CC2=CC(=O)CC[C@]12C)C(=O)OC'] * 100
# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/12:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Eplerenone'  
chemical_structure_to_add = '[H][C@@]12CC[C@@]3(CCC(=O)O3)[C@@]1(C)C[C@H]1O[C@@]11[C@@]2([H])[C@@H](CC2=CC(=O)CC[C@]12C)C(=O)OC' * 100

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/13:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Eplerenone'  
chemical_structure_to_add = '[H][C@@]12CC[C@@]3(CCC(=O)O3)[C@@]1(C)C[C@H]1O[C@@]11[C@@]2([H])[C@@H](CC2=CC(=O)CC[C@]12C)C(=O)OC'

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/14:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Spironolactone'  
chemical_structure_to_add = '[H][C@@]12CC[C@@]3(CCC(=O)O3)[C@@]1(C)CC[C@@]1([H])[C@@]2([H])[C@@]([H])(CC2=CC(=O)CC[C@]12C)SC(C)=O'

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/15:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Lisinopril'  
chemical_structure_to_add = 'NCCCC[C@H](N[C@@H](CCC1=CC=CC=C1)C(O)=O)C(=O)N1CCC[C@H]1C(O)=O'

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/16:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Enalapril'  
chemical_structure_to_add = 'CCOC(=O)[C@H](CCC1=CC=CC=C1)N[C@@H](C)C(=O)N1CCC[C@H]1C(O)=O'

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/17:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Enalapril'  
chemical_structure_to_add = 'CCOC(=O)[C@H](CCC1=CC=CC=C1)N[C@@H](C)C(=O)N1CCC[C@H]1C(O)=O'

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/18:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Enalapril'  
chemical_structure_to_add = 'CCOC(=O)[C@H](CCC1=CC=CC=C1)N[C@@H](C)C(=O)N1CCC[C@H]1C(O)=O'

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/19:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Enalapril '  
chemical_structure_to_add = 'CCOC(=O)[C@H](CCC1=CC=CC=C1)N[C@@H](C)C(=O)N1CCC[C@H]1C(O)=O'

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/20:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Benazepril'  
chemical_structure_to_add = '[H][C@@]1(CCC2=CC=CC=C2N(CC(O)=O)C1=O)N[C@@H](CCC1=CC=CC=C1)C(=O)OCC'

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/21:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Doxazosin'  
chemical_structure_to_add = 'COC1=C(OC)C=C2C(N)=NC(=NC2=C1)N1CCN(CC1)C(=O)C1COC2=CC=CC=C2O1'

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/22:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Prazosin'  
chemical_structure_to_add = 'COC1=C(OC)C=C2C(N)=NC(=NC2=C1)N1CCN(CC1)C(=O)C1=CC=CO1'

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
54/23:
import pandas as pd

# Read the CSV file
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv')

# Specify the drug name and its corresponding chemical structure
drug_name_to_add = 'Terazosin'  
chemical_structure_to_add = 'COC1=C(OC)C=C2C(N)=NC(=NC2=C1)N1CCN(CC1)C(=O)C1CCCO1'

# Add the chemical structure for the specified drug
df.loc[df['Drug Name'] == drug_name_to_add, 'Chemical structure'] = chemical_structure_to_add

# Write the modified DataFrame back to the CSV file
df.to_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\High BP drug target.csv', index=False)
55/1:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
57/1:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
57/2:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
57/3:
data_types = df.dtypes
display(data_types)
df['Drug Name'] = df['Drug Name'].unique()
display(df['Drug Name'])
df['Drug classes'] = df['Drug classes'].unique()
display(df['Drug classes'])
57/4:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()

unique_drug_classes = df['Drug classes'].drop_duplicates()
57/5:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
57/6:
import matplotlib.pyplot as plt

df.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
df.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")
57/7:
import matplotlib.pyplot as plt

df.describe().T

# Create a boxplot before removing outliers
fig, ax = plt.subplots(figsize=(20,10))
df.boxplot(column='Probability*', ax=ax)
plt.title("Boxplot with Outliers")
57/8:
import pandas as pd
import matplotlib.pyplot as plt

large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Generate Pie Chart for Target Class Distribution
target_class_counts = large_dataset['Target Class'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(target_class_counts, labels=None, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(target_class_counts))))
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Target Class Distribution')



# Display Pie Chart
plt.show()
57/9:
import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'large_dataset' with columns 'Drug Name', 'Target Class', and 'Probability*'
# You need to replace 'large_dataset' with your actual DataFrame name.

# Step 1: Load the Data
# Load your large dataset (replace 'large_dataset.csv' with your actual file path)
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Step 3: Generate a Pie Chart for Each Drug
for drug_name in unique_drugs:
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Create Pie Chart
    plt.figure(figsize=(8, 8))
    plt.pie(target_class_counts, labels=target_class_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(target_class_counts))))
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Distribution')

    # Display Pie Chart
    plt.show()
61/1:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
61/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
61/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
61/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
print(Name_df)
print(Age_df)
print(Churn_df)
61/5:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
61/6:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
61/7:
data_types = df.dtypes
display(data_types)
df['Target Class'] = df['Target Class'].astype('category')
display(df['Target Class'])
df['Target'] = df['Target'].astype('category')
display(df['Target'])
61/8:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
61/9: df.describe(include="all")
61/10: df.corr()
61/11:
df_chrun = df['Churn']
display(df_chrun)
61/12:
df_chrun = df['Churn']
display(df_chrun)
df_chrun.corr()
61/13:
df_chrun = df['Churn']
display(df_chrun)
correlation = df_chrun.corr(Age_df)
correlation_matrix = df.corr()
61/14:
df_drop = df['Total_Usage_GB']
df1 = df_drop.drop(df_drop, axis=1)
display(df)
61/15:
df_drop = df['Total_Usage_GB']
df1 = df_drop.drop(df_drop)
display(df)
61/16:
df_drop = df['Total_Usage_GB']
df1 = df_drop.drop(df_drop)
display(df1)
61/17: df.drop(['Total_Usage_GB'], axis=1)
61/18: df.describe(include="all")
61/19:
df.describe(include="all")
df.drop(['Total_Usage_GB'], axis=1)
61/20:
df.describe(include="all")
df.drop(['Total_Usage_GB'], axis=1)
61/21: df.describe(include="all")
61/22:
# Splitting train and test data

X = df[['CustomerID', 'Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train2 : ')
display(X_train2.head())
display('X_test2 : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
61/23: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
61/24:
# Splitting train and test data

X = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train : ')
display(X_train2.head())
display('X_test : ')
display(X_test2.head())
display('y_train2 : ')
display(y_train2.head())
display('y_test2 : ')
display(y_test2.head())
61/25:
# Splitting train and test data

X = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train : ')
display(X_train2.head())
display('X_test : ')
display(X_test2.head())
display('y_train : ')
display(y_train2.head())
display('y_test : ')
display(y_test2.head())
61/26: X_train.shape
61/27:
# Splitting train and test data

X = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train2.head())
display('X_test: ')
display(X_test2.head())
display('y_train: ')
display(y_train2.head())
display('y_test: ')
display(y_test2.head())
61/28: X_train.shape
61/29: X_train.shape
61/30:
# Splitting train and test data

X = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
61/31: X_train.shape
61/32: print('Shape of X-train:', X_train.shape)
61/33:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
61/34:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
61/35:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:' y_train.shape)
print('Shape of y-test:' y_test.shape)
61/36:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
62/1:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
61/37:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
61/38:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
61/39:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
61/40:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
61/41:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
61/42:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
61/43:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
61/44:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
61/45:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
61/46:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
61/47: df.describe(include="all")
61/48: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
61/49:
# Label Encoding


label_encoder = preprocessing.LabelEncoder() 
df['species']= label_encoder.fit_transform(df['species']) 
df['species'].unique()
61/50:
# Label Encoding

from sklearn import preprocessing


label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique()
61/51:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
61/52:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
61/53:
# Splitting train and test data

X = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
61/54:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
61/55: df.describe(include="all")
61/56:
# Creation of base KNN model.

from sklearn.neighbors import KNeighborsClassifier

NNH_A = KNeighborsClassifier(n_neighbors= 11 , weights = 'distance' )
NNH_A.fit(X_train, y_train)
61/57:
# calcuating overall model accuracy 

predicted_labels_NNH_A = NNH_A.predict(X_test)
NNH_A.score(X_test, y_test)
61/58:
# calcuating overall model accuracy 

predicted_labels_NNH_A = NNH_A.predict(X_test)
NNH_A.score(X_test, y_test)

# Evaluation on training set 

predicted_labels_train_A = NNH_A.predict(X_train)
NNH_A.score(X_train, y_train)
61/59:
# calcuating overall model accuracy 

predicted_labels_NNH_A = NNH_A.predict(X_test)
NNH_A.score(X_test, y_test)

# Evaluation on training set 

predicted_labels_train_A = NNH_A.predict(X_train)
61/60:
# calcuating overall model accuracy 

predicted_labels_NNH_A = NNH_A.predict(X_test)
NNH_A.score(X_test, y_test)

# Evaluation on training set 

predicted_labels_train_A = NNH_A.predict(X_train)
print(predicted_labels_train_A)
61/61:
import pandas as pd

# Assuming you have a DataFrame called df with a 'Churn' column
# You may load your data using: df = pd.read_csv('your_data.csv')

# Create the three groups
churned = df[df['Churn'] == 1]
potential_churn = df[df['Churn'] == 0]
non_churned = df[df['Churn'] == 0]

# Example: Display the first few rows of each group
print("Churned Customers:")
print(churned.head())

print("\nPotential Churn Customers:")
print(potential_churn.head())

print("\nNon-Churned Customers:")
print(non_churned.head())
61/62: sns.pairplot(df,diag_kind='kde', hue = 'Class')
61/63:
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
sns.pairplot(df, diag_kind='kde')  # 'kde' stands for Kernel Density Estimate
plt.show()
61/64:
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]
sns.pairplot(df1, diag_kind='kde') 
plt.show()
61/65:
# Creation of base KNN model.

from sklearn.neighbors import KNeighborsClassifier

NNH_A = KNeighborsClassifier(n_neighbors= 11 , weights = 'distance' )
NNH_A.fit(X_train, y_train)

# calcuating overall model accuracy 

predicted_labels_NNH_A = NNH_A.predict(X_test)
NNH_A.score(X_test, y_test)

# Evaluation on training set 

predicted_labels_train_A = NNH_A.predict(X_train)
print(predicted_labels_train_A)
61/66:
# Visualization of churn on various features

# Churn by location

X = df['Location'] 
Y = df['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 
plt.title("Churn by location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 
  
# Show the plot 
plt.show()
63/1:
# Visualization of churn on various features

# Churn by location

import matplotlib.pyplot as plt 
import pandas as pd 

X = df['Location'] 
Y = df['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 
plt.title("Churn by location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 
  
# Show the plot 
plt.show()
63/2:
# Visualization of churn on various features

# Churn by location

import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

X = df['Location'] 
Y = df['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 
plt.title("Churn by location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 
  
# Show the plot 
plt.show()
63/3:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
63/4:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
63/5:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
63/6:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
63/7:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
63/8:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
63/9: df.describe(include="all")
63/10: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
63/11:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
63/12:
# Visualization of churn on various features

# Churn by location

import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

X = df['Location'] 
Y = df['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 
plt.title("Churn by location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 
  
# Show the plot 
plt.show()
64/1:
# Visualization of churn on various features

# Churn by location

import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]
64/2:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
64/3:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
64/4:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
64/5:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
64/6:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
64/7:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
64/8:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
64/9: df.describe(include="all")
64/10: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
64/11:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
64/12:
# Visualization of churn on various features

# Churn by location

import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]
64/13:
# Visualization of churn on various features

# Churn by location

import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

X = df['Location'] 
Y = df['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 
plt.title("Churn by location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 
  
# Show the plot 
plt.show()
65/1:
# Import necessary libraries
import matplotlib.pyplot as plt 
import pandas as pd 

# Assuming you have a DataFrame df with columns: 'Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn'

# Select relevant columns from the DataFrame
df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

# Define X and Y for plotting
X = df['Location'] 
Y = df['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 

# Set the title and labels for the plot
plt.title("Churn by Location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 
  
# Show the plot 
plt.show()
65/2:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
65/3:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
65/4:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
65/5:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
65/6:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
65/7:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
65/8: df.describe(include="all")
65/9: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
65/10:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
65/11:
# Import necessary libraries
import matplotlib.pyplot as plt 
import pandas as pd 

# Assuming you have a DataFrame df with columns: 'Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn'

# Select relevant columns from the DataFrame
df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

# Define X and Y for plotting
X = df['Location'] 
Y = df['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 

# Set the title and labels for the plot
plt.title("Churn by Location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 
  
# Show the plot 
plt.show()
68/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
68/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
68/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
68/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
68/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
68/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
68/7: df.describe(include="all")
68/8: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
68/9:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
68/10:
# Import necessary libraries
import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

# Define X and Y for plotting
X = df['Location'] 
Y = df['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 

# Set the title and labels for the plot
plt.title("Churn by Location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 
  
# Show the plot 
plt.show()
68/11:
X = list(df.iloc[:, 0]) 
Y = list(df.iloc[:, 1]) 
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 
plt.title("Students over 11 Years") 
plt.xlabel("Years") 
plt.ylabel("Number of Students") 
  
# Show the plot 
plt.show()
68/12:
# Splitting train and test data

X = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
68/13:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
68/14:
# Import necessary libraries
import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

# Define X and Y for plotting
X = df['Location'] 
Y = df['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 

# Set the title and labels for the plot
plt.title("Churn by Location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 

# Show the plot 
plt.show()
68/15:
# Import necessary libraries
import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

# Define X and Y for plotting
X = df1['Location'] 
Y = df1['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 

# Set the title and labels for the plot
plt.title("Churn by Location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 

# Show the plot 
plt.show()
68/16:
# Import necessary libraries
import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

# Define X and Y for plotting
X = df['Location'].unique() 
Y = df['Churn'].unique() 
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 

# Set the title and labels for the plot
plt.title("Churn by Location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 

# Show the plot 
plt.show()
68/17:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
68/18:
# Import necessary libraries
import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

# Define X and Y for plotting
X = df['Location']
Y = df['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 

# Set the title and labels for the plot
plt.title("Churn by Location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 

# Show the plot 
plt.show()
68/19:
# Import necessary libraries
import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

# Define X and Y for plotting
X = df['Location'].unique()
Y = df['Churn']
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 

# Set the title and labels for the plot
plt.title("Churn by Location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 

# Show the plot 
plt.show()
68/20:
# Import necessary libraries
import matplotlib.pyplot as plt 
import pandas as pd 

df1 = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill', 'Churn']]

# Define X and Y for plotting
X = df['Location'].unique()
Y = df['Churn'].unique()
  
# Plot the data using bar() method 
plt.bar(X, Y, color='g') 

# Set the title and labels for the plot
plt.title("Churn by Location") 
plt.xlabel("Location") 
plt.ylabel("Churn") 

# Show the plot 
plt.show()
68/21:
import matplotlib.pyplot as plt
import numpy as np

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Count the number of churns for each location
churn_counts = df1.groupby(['Location', 'Churn']).size().unstack(fill_value=0)

# Plot the grouped bar chart
churn_counts.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Churn")
plt.show()
68/22:
import matplotlib.pyplot as plt
import numpy as np

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Count the number of churns for each location
churn_counts = df1.groupby(['Subscription_Length_Months', 'Churn']).size().unstack(fill_value=0)

# Plot the grouped bar chart
churn_counts.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Subscription_Length_Months")
plt.ylabel("Churn")
plt.show()
68/23:
import matplotlib.pyplot as plt
import numpy as np

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Plot the grouped bar chart
pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")
plt.legend(['Non-Churn', 'Churn'])
plt.show()
68/24:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")
plt.legend(['Non-Churn', 'Churn'])

# Add annotations to the bars
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')

plt.show()
68/25:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")
plt.legend(['Non-Churn', 'Churn'])

# Add annotations to the bars
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 3., p.get_height()), ha='top', va='bottom')

plt.show()
68/26:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")
plt.legend(['Non-Churn', 'Churn'])

# Add annotations to the bars
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 3., p.get_height()), ha='right', va='bottom')

plt.show()
68/27:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")
plt.legend(['Non-Churn', 'Churn'])

# Add annotations to the bars
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 3., p.get_height()), ha='right', va='top')

plt.show()
68/28:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")
plt.legend(['Non-Churn', 'Churn'])

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

plt.show()
68/29:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Subscription_Length_Months")
plt.ylabel("Count")
plt.legend(['Non-Churn', 'Churn'])

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

plt.show()
68/30:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")
plt.legend(['Non-Churn', 'Churn'])

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

plt.show()
68/31:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")
plt.legend(['Non-Churn', 'Churn'])

# Add annotations to the bars
Annotation = 
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

plt.show()
68/32:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")
plt.legend(['Non-Churn', 'Churn'])

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'Non-Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

plt.show()
68/33:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'Non-Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = ['Non-Churn', 'Churn']
legend_counts = [pivot_table[0].sum(), pivot_table[1].sum()]
legend_labels_with_counts = [f'{label} ({count})' for label, count in zip(legend_labels, legend_counts)]
ax.legend(legend_labels_with_counts)

plt.show()
68/34:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Get unique locations
locations = df1['Location'].unique()

# Iterate through each location
for location in locations:
    location_df = df1[df1['Location'] == location]
    
    # Create a pivot table to count churn and non-churn for the location
    pivot_table = location_df.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

    # Create the grouped bar chart
    ax = pivot_table.plot(kind='bar', stacked=True)
    plt.title(f"Churn in {location}")
    plt.xlabel("Location")
    plt.ylabel("Count")

    # Add annotations to the bars
    for p in ax.patches:
        if p.get_height() > 0:  
            if p.get_y() == 0: 
                ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
            else:  # Churned
                ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

    # Create a custom legend with counts
    non_churn_count = pivot_table[0].sum()
    churn_count = pivot_table[1].sum()
    legend_labels = [f'Non-Churn ({non_churn_count})', f'Churn ({churn_count})']
    ax.legend(legend_labels)

    plt.show()
68/35:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
68/36:
import matplotlib.pyplot as plt
import pandas as pd

# Example data (replace with your actual DataFrame)
df1 = df[['Location', 'Churn']]

# Set the figure size
plt.figure(figsize=(10, 6))

# Create the bar chart
plt.bar(df['Location'], df['Churn'], color='skyblue')

# Add titles and labels
plt.title('Churn by Location')
plt.xlabel('Location')
plt.ylabel('Values')

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')
    

# Show the plot
plt.show()
68/37:
import matplotlib.pyplot as plt
import pandas as pd

# Example data (replace with your actual DataFrame)
df1 = df[['Location', 'Churn']]

# Set the figure size
plt.figure(figsize=(10, 6))

# Create the bar chart
plt.bar(df['Location'], df['Churn'], color='skyblue')

# Add titles and labels
plt.title('Churn by Location')
plt.xlabel('Location')
plt.ylabel('Values')

# Show the plot
plt.show()
70/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
70/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
70/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
70/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
70/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
70/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
70/7: df.describe(include="all")
70/8: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
70/9:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
70/10:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
70/11:
import matplotlib.pyplot as plt
import pandas as pd

# Example data (replace with your actual DataFrame)
df1 = df[['Location', 'Churn']]

# Set the figure size
plt.figure(figsize=(10, 6))

# Create the bar chart
plt.bar(df['Location'], df['Churn'], color='skyblue')

# Add titles and labels
plt.title('Churn by Location')
plt.xlabel('Location')
plt.ylabel('Values')

# Show the plot
plt.show()
71/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
71/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
71/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
71/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
71/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
71/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
71/7: df.describe(include="all")
71/8: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
71/9:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
71/10:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
71/11:
import matplotlib.pyplot as plt
import pandas as pd

# Example data (replace with your actual DataFrame)
df1 = df[['Location', 'Churn']]

# Set the figure size
plt.figure(figsize=(10, 6))

# Create the bar chart
plt.bar(df['Location'], df['Churn'], color='skyblue')

# Add titles and labels
plt.title('Churn by Location')
plt.xlabel('Location')
plt.ylabel('Values')

# Show the plot
plt.show()
72/1:

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Set the figure size
plt.figure(figsize=(10, 6))

# Create the bar chart with customizations
plt.bar(df['Location'], df['Churn'], color=['red', 'green', 'blue', 'yellow', 'purple'], alpha=0.7)

# Add titles and labels
plt.title('Comparison of Categories')
plt.xlabel('Category')
plt.ylabel('Values')

# Add data labels
for i, value in enumerate(df['Churn']):
    plt.text(i, value + 1, str(value), ha='center', va='bottom')

# Show the plot
plt.show()
72/2:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
72/3:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
72/4:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
72/5:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
72/6:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
72/7:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
72/8: df.describe(include="all")
72/9: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
72/10:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
72/11:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
72/12:

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Set the figure size
plt.figure(figsize=(10, 6))

# Create the bar chart with customizations
plt.bar(df['Location'], df['Churn'], color=['red', 'green', 'blue', 'yellow', 'purple'], alpha=0.7)

# Add titles and labels
plt.title('Comparison of Categories')
plt.xlabel('Category')
plt.ylabel('Values')

# Add data labels
for i, value in enumerate(df['Churn']):
    plt.text(i, value + 1, str(value), ha='center', va='bottom')

# Show the plot
plt.show()
72/13:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame with columns 'Location' and 'Churn'
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend
legend_labels = ['Non-Churn', 'Churn']
ax.legend(legend_labels)

plt.show()
72/14:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Set the positions and width for the bars
positions = range(len(df['Location']))
width = 0.4

# Create the grouped bar chart
plt.figure(figsize=(10, 6))
plt.bar(positions, df['Churned'], width, label='Churned', color='b')
plt.bar([p + width for p in positions], df['Non-Churned'], width, label='Non-Churned', color='r')

# Add titles and labels
plt.title('Churn Comparison by Location')
plt.xlabel('Location')
plt.ylabel('Count')
plt.xticks([p + width/2 for p in positions], df['Location'])

# Add legend
plt.legend()

# Show the plot
plt.show()
72/15:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Set the positions and width for the bars
positions = range(len(df['Location']))
width = 0.4

# Create the grouped bar chart
plt.figure(figsize=(10, 6))
plt.bar(positions, df['Churn'], width, label='Churned', color='b')
plt.bar([p + width for p in positions], df['Non-Churned'], width, label='Non-Churned', color='r')

# Add titles and labelsa
plt.title('Churn Comparison by Location')
plt.xlabel('Location')
plt.ylabel('Count')
plt.xticks([p + width/2 for p in positions], df['Location'])

# Add legend
plt.legend()

# Show the plot
plt.show()
72/16:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Set the positions and width for the bars
positions = range(len(df['Location']))
width = 0.4

# Create the grouped bar chart
plt.figure(figsize=(10, 6))
plt.bar(positions, df['Churn'], width, label='Churn', color='b')
plt.bar([p + width for p in positions], df['Non-Churn'], width, label='Non-Churn', color='r')

# Add titles and labels
plt.title('Churn Comparison by Location')
plt.xlabel('Location')
plt.ylabel('Count')
plt.xticks([p + width/2 for p in positions], df['Location'])

# Add legend
plt.legend()

# Show the plot
plt.show()
73/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
73/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
73/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
73/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
73/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
73/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
73/7: df.describe(include="all")
73/8: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
73/9:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
73/10:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
73/11:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Set the positions and width for the bars
positions = range(len(df['Location']))
width = 0.4

# Create the grouped bar chart
plt.figure(figsize=(10, 6))
plt.bar(positions, df['Churn'], width, label='Churn', color='b')
plt.bar([p + width for p in positions], df['Non-Churn'], width, label='Non-Churn', color='r')

# Add titles and labels
plt.title('Churn Comparison by Location')
plt.xlabel('Location')
plt.ylabel('Count')
plt.xticks([p + width/2 for p in positions], df['Location'])

# Add legend
plt.legend()

# Show the plot
plt.show()
73/12:
k = 5
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)
73/13:
from sklearn.neighbors import KNeighborsClassifier

k = 5
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)
73/14:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
73/15:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
73/16:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
73/17:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
73/18:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
73/19:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
73/20: df.describe(include="all")
73/21: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
73/22:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
73/23:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
73/24:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Set the positions and width for the bars
positions = range(len(df['Location']))
width = 0.4

# Create the grouped bar chart
plt.figure(figsize=(10, 6))
plt.bar(positions, df['Churn'], width, label='Churn', color='b')
plt.bar([p + width for p in positions], df['Non-Churn'], width, label='Non-Churn', color='r')

# Add titles and labels
plt.title('Churn Comparison by Location')
plt.xlabel('Location')
plt.ylabel('Count')
plt.xticks([p + width/2 for p in positions], df['Location'])

# Add legend
plt.legend()

# Show the plot
plt.show()
73/25:
from sklearn.neighbors import KNeighborsClassifier

k = 5
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)
73/26:
# Splitting train and test data

X = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
73/27:
from sklearn.neighbors import KNeighborsClassifier

k = 5
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)
y_pred = knn_model.predict(X_test)
73/28:
from sklearn.neighbors import KNeighborsClassifier

k = 5
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)
y_pred = knn_model.predict(X_test)
print(y_pred)
73/29:
from sklearn.base import accuracy_score
from sklearn.neighbors import KNeighborsClassifier

k = 5
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)
y_pred = knn_model.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
73/30:
from sklearn.base import accuracy_score
from sklearn.neighbors import KNeighborsClassifier

k = 5
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)
y_pred = knn_model.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
73/31:
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

k = 5
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)
y_pred = knn_model.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
73/32:
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

k = 5
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)
y_pred = knn_model.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
cm=metrics.confusion_matrix(y_test, predicted_labels_NNH_A, labels=["Normal", "Type_H"])

df_cm = pd.DataFrame(cm, index = [i for i in ["Normal","Type_H"]],
                  columns = [i for i in ["Predicted Normal","Predicted Type_H"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True)
74/1:
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

k = 5
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)
y_pred = knn_model.predict(X_test)
print(y_pred)
74/2:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
74/3:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
74/4:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
74/5:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
74/6:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
74/7:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
74/8: df.describe(include="all")
74/9: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
74/10:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
74/11:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
74/12:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Set the positions and width for the bars
positions = range(len(df['Location']))
width = 0.4

# Create the grouped bar chart
plt.figure(figsize=(10, 6))
plt.bar(positions, df['Churn'], width, label='Churn', color='b')
plt.bar([p + width for p in positions], df['Non-Churn'], width, label='Non-Churn', color='r')

# Add titles and labels
plt.title('Churn Comparison by Location')
plt.xlabel('Location')
plt.ylabel('Count')
plt.xticks([p + width/2 for p in positions], df['Location'])

# Add legend
plt.legend()

# Show the plot
plt.show()
74/13:
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

k = 5
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X_train, y_train)
y_pred = knn_model.predict(X_test)
print(y_pred)
74/14:
import pandas as pd

# Assuming you have a DataFrame called df with a 'Churn' column
# You may load your data using: df = pd.read_csv('your_data.csv')

# Create the three groups
churned = df[df['Churn'] == 1]
potential_churn = df[df['Churn'] == 0]
non_churned = df[df['Churn'] == 0]

# Example: Display the first few rows of each group
print("Churned Customers:")
print(churned.head())

print("\nPotential Churn Customers:")
print(potential_churn.head())

print("\nNon-Churned Customers:")
print(non_churned.head())
74/15:
# Creation of base KNN model.

from sklearn.neighbors import KNeighborsClassifier

NNH_A = KNeighborsClassifier(n_neighbors= 11 , weights = 'distance' )
NNH_A.fit(X_train, y_train)

# calcuating overall model accuracy 

predicted_labels_NNH_A = NNH_A.predict(X_test)
NNH_A.score(X_test, y_test)

# Evaluation on training set 

predicted_labels_train_A = NNH_A.predict(X_train)
print(predicted_labels_train_A)
74/16:
# Splitting train and test data

X = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
74/17:
# Splitting train and test data

X = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
74/18:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
74/19:
# Creation of base KNN model.

from sklearn.neighbors import KNeighborsClassifier

NNH_A = KNeighborsClassifier(n_neighbors= 11 , weights = 'distance' )
NNH_A.fit(X_train, y_train)

# calcuating overall model accuracy 

predicted_labels_NNH_A = NNH_A.predict(X_test)
NNH_A.score(X_test, y_test)

# Evaluation on training set 

predicted_labels_train_A = NNH_A.predict(X_train)
print(predicted_labels_train_A)
74/20:
# Assuming you've already split your data as X_train, X_test, y_train, y_test

# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Now 'y_pred' contains the predicted values for 'Churn' based on the features in 'X_test'
74/21:
# Assuming you've already split your data as X_train, X_test, y_train, y_test

# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Now 'y_pred' contains the predicted values for 'Churn' based on the features in 'X_test'
74/22:
# Assuming you've already split your data as X_train, X_test, y_train, y_test

# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(df['Monthly_Bill:50'])
display(y_pred)

# Now 'y_pred' contains the predicted values for 'Churn' based on the features in 'X_test'
74/23:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(df['Monthly_Bill:50'])
display(y_pred)

new_data = pd.DataFrame({
    'Name': ['John'],  
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

y_pred_new = model.predict(new_data)

df['Predicted_Churn'] = y_pred_new
74/24:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(df['Monthly_Bill:50'])
display(y_pred)

new_data = pd.DataFrame({
    'Name': ['John'],  
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

y_pred_new = model.predict(new_data)

df['Predicted_Churn'] = y_pred_new
74/25:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(df['Monthly_Bill:50'])
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['John'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/26:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(df['Monthly_Bill:50'])
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['John'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [40.0]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/27:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(df['Monthly_Bill:50'])
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['John'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [88.48]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/28:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['John'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [88.48]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/29:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [88.48]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/30:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/31:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [10]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/32:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [100]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/33:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [1],  
    'Monthly_Bill': [100]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/34:
# Creation of base KNN model.

from sklearn.neighbors import KNeighborsClassifier

NNH_A = KNeighborsClassifier(n_neighbors= 11 , weights = 'distance' )
NNH_A.fit(X_train, y_train)

# calcuating overall model accuracy 

predicted_labels_NNH_A = NNH_A.predict(X_test)
NNH_A.score(X_test, y_test)

# Evaluation on training set 

predicted_labels_train_A = NNH_A.predict(X_train)
print(predicted_labels_train_A)
74/35:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [3],  
    'Monthly_Bill': [100]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/36:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [2],  
    'Monthly_Bill': [100]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/37:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [2],  
    'Monthly_Bill': [10]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/38:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [2],  
    'Monthly_Bill': [10]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/39:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['72471'],  
    'Location': [4],    
    'Subscription_Length_Months': [2],  
    'Monthly_Bill': [10]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/40:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['72471'],  
    'Location': [4],    
    'Subscription_Length_Months': [5],  
    'Monthly_Bill': [84.50]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/41:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
74/42:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)

accuracy = accuracy_score(y_test, y_pred)
74/43:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
74/44:
# Creation of base KNN model.

from sklearn.neighbors import KNeighborsClassifier

NNH_A = KNeighborsClassifier(n_neighbors= 11 , weights = 'distance' )
NNH_A.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
74/45:
# Creation of base KNN model.

from sklearn.neighbors import KNeighborsClassifier

NNH_A = KNeighborsClassifier(n_neighbors= 11 , weights = 'distance' )
NNH_A.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data1 = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
y_pred_new1 = model.predict(new_data1)
print("Predicted Churn:", y_pred_new1)

accuracy1 = accuracy_score(y_test, y_pred)
print(accuracy1)
74/46:
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# creating a RF classifier 
clf = RandomForestClassifier(n_estimators = 100)   
  
# Training the model on the training dataset 
# fit function is used to train the model using the training sets as parameters 
clf.fit(X_train, y_train) 
  
# performing predictions on the test dataset 
y_pred = clf.predict(X_test) 

# Assuming df is your DataFrame containing the original data
new_data2 = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
y_pred_new2 = model.predict(new_data2)
print("Predicted Churn:", y_pred_new2)

accuracy2 = accuracy_score(y_test, y_pred)
print(accuracy2)
74/47:
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Initialize the model
model = LogisticRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
74/48:
# Creation of base KNN model.

from sklearn.neighbors import KNeighborsClassifier

NNH_A = KNeighborsClassifier(n_neighbors= 11 , weights = 'distance' )
NNH_A.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
display(y_pred)

# Assuming df is your DataFrame containing the original data
new_data1 = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
y_pred_new1 = model.predict(new_data1)
print("Predicted Churn:", y_pred_new1)

accuracy1 = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy1)
74/49:
from pyexpat import model
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# creating a RF classifier 
clf = RandomForestClassifier(n_estimators = 100)   

clf.fit(X_train, y_train) 
  
# performing predictions on the test dataset 
y_pred = clf.predict(X_test) 

# Assuming df is your DataFrame containing the original data
new_data2 = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
y_pred_new2 = model.predict(new_data2)
print("Predicted Churn:", y_pred_new2)

accuracy2 = accuracy_score(y_test, y_pred)
print(accuracy2)
74/50:
from pyexpat import model
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# creating a RF classifier 
clf = RandomForestClassifier(n_estimators = 100)   

clf.fit(X_train, y_train) 
  
# performing predictions on the test dataset 
y_pred = clf.predict(X_test) 

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
74/51:
from pyexpat import model
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# creating a RF classifier 
clf = RandomForestClassifier(n_estimators = 100)   

clf.fit(X_train, y_train) 
  
# performing predictions on the test dataset 
y_pred = clf.predict(X_test) 

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
y_pred_new = clf.predict(new_data)
print("Predicted Churn:", y_pred_new)

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
74/52:
from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to your training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
74/53:
from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to your training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
74/54:
from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE(random_state=42)

X_r = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y_r = df['Churn']

# Apply SMOTE to your training data
Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_r, y_r, test_size=0.2, random_state=42)
X_resampled, y_resampled = smote.fit_resample(Xr_train, yr_train)
74/55:
from imblearn.over_sampling import SMOTE
from pyexpat import model
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Initialize SMOTE
smote = SMOTE(random_state=42)

X_r = df[['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y_r = df['Churn']

# Apply SMOTE to your training data
Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_r, y_r, test_size=0.2, random_state=42)
X_resampled, y_resampled = smote.fit_resample(Xr_train, yr_train)

# creating a RF classifier 
clf = RandomForestClassifier(n_estimators = 100)   

clf.fit(Xr_train, yr_train) 
  
# performing predictions on the test dataset 
yr_pred = clf.predict(Xr_test) 

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [4],    
    'Subscription_Length_Months': [4],  
    'Monthly_Bill': [84.00]  
})

# Predict using the model
yr_pred_new = clf.predict(new_data)
print("Predicted Churn:", yr_pred_new)

accuracy = accuracy_score(yr_test, yr_pred)
print(accuracy)
74/56:
from sklearn.tree import DecisionTreeClassifier  # Import DecisionTreeClassifier

# Create a Decision Tree classifier
clf = DecisionTreeClassifier()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],
    'Location': [4],
    'Subscription_Length_Months': [4],
    'Monthly_Bill': [84.00]
})

# Predict using the model
y_pred_new = clf.predict(new_data)
print("Predicted Churn:", y_pred_new)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
74/57:
from sklearn.tree import DecisionTreeClassifier  # Import DecisionTreeClassifier

# Create a Decision Tree classifier
clf = DecisionTreeClassifier()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],
    'Location': [4],
    'Subscription_Length_Months': [4],
    'Monthly_Bill': [84.00]
})

# Predict using the model
y_pred_new = clf.predict(new_data)
print("Predicted Churn:", y_pred_new)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
74/58:
from sklearn.linear_model import LogisticRegression

# Create a Logistic Regression classifier
clf = LogisticRegression()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],
    'Location': [4],
    'Subscription_Length_Months': [4],
    'Monthly_Bill': [84.00]
})

# Predict using the model
y_pred_new = clf.predict(new_data)
print("Predicted Churn:", y_pred_new)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
74/59:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their search space
param_grid = {
    'n_estimators': [50, 100, 200],  # List of values to try for n_estimators
    'max_depth': [None, 10, 20],    # List of values to try for max_depth
    # Add more hyperparameters and their search space if needed
}

# Initialize the Random Forest Classifier
rf_classifier = RandomForestClassifier()

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
74/60:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their search space
param_grid = {
    'max_depth': [None, 5, 10, 20],  # List of values to try for max_depth
    'min_samples_split': [2, 5, 10],  # List of values to try for min_samples_split
    # Add more hyperparameters and their search space if needed
}

# Initialize the Decision Tree Classifier
dt_classifier = DecisionTreeClassifier()

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
74/61:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their search space
param_grid = {
    'max_depth': [None, 5, 10, 20],  # List of values to try for max_depth
    'min_samples_split': [2, 5, 10],  # List of values to try for min_samples_split
    # Add more hyperparameters and their search space if needed
}

# Initialize the Decision Tree Classifier
dt_classifier = DecisionTreeClassifier()

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xr_train, yr_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yr_pred = best_model.predict(Xr_test)
accuracyr = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracyr)
74/62:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)

# Assuming df is your DataFrame containing the original data
new_data = pd.DataFrame({
    'Name': ['100000'],
    'Location': [4],
    'Subscription_Length_Months': [4],
    'Monthly_Bill': [84.00]
})

# Predict using the model
y_pred_new = clf.predict(new_data)
print("Predicted Churn:", y_pred_new)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
74/63:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their search space
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100]  # List of values to try for C
}

# Initialize the Logistic Regression classifier
logistic_reg = LogisticRegression()

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(logistic_reg, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
74/64:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
74/65:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)
74/66:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
76/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
76/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
76/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
76/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
76/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
76/7: df.describe(include="all")
76/8: df.drop(['CustomerID', 'Age', 'Gender', 'Total_Usage_GB'], axis=1)
76/9:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
76/10:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
76/11:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Set the positions and width for the bars
positions = range(len(df['Location']))
width = 0.4

# Create the grouped bar chart
plt.figure(figsize=(10, 6))
plt.bar(positions, df['Churn'], width, label='Churn', color='b')
plt.bar([p + width for p in positions], df['Non-Churn'], width, label='Non-Churn', color='r')

# Add titles and labels
plt.title('Churn Comparison by Location')
plt.xlabel('Location')
plt.ylabel('Count')
plt.xticks([p + width/2 for p in positions], df['Location'])

# Add legend
plt.legend()

# Show the plot
plt.show()
76/12: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
76/13:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
76/14:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
76/15:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
76/16:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/17:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their search space
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100]  # List of values to try for C
}

# Initialize the Logistic Regression classifier
logistic_reg = LogisticRegression()

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(logistic_reg, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/18:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their search space
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100]  # List of values to try for C
}

# Initialize the Logistic Regression classifier
logistic_reg = LogisticRegression()

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(logistic_reg, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/19:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their search space
param_grid = {
    'C': [10, 20, 30, 50, 100]  # List of values to try for C
}

# Initialize the Logistic Regression classifier
logistic_reg = LogisticRegression()

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(logistic_reg, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/20:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their search space
param_grid = {
    'C': [100, 20, 30, 50, 1000]  # List of values to try for C
}

# Initialize the Logistic Regression classifier
logistic_reg = LogisticRegression()

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(logistic_reg, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/21:
from sklearn.model_selection import RandomizedSearchCV

# Define the hyperparameters and their search space
param_dist = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # List of values to sample for C
}

# Initialize the Logistic Regression classifier
logistic_reg = LogisticRegression()

# Initialize Random Search with cross-validation (e.g., 5-fold cross-validation)
random_search = RandomizedSearchCV(logistic_reg, param_dist, cv=5, n_iter=5, scoring='accuracy', random_state=42)

# Fit the Random Search to your training data
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = random_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/22:
from skopt import BayesSearchCV

# Define the hyperparameters and their search space
param_dist = {
    'C': (0.001, 1000.0, 'log-uniform'),  # Tuple format (low, high, distribution)
}

# Initialize the Logistic Regression classifier
logistic_reg = LogisticRegression()

# Initialize Bayesian Optimization with cross-validation (e.g., 5-fold cross-validation)
bayes_search = BayesSearchCV(logistic_reg, param_dist, cv=5, n_iter=5, scoring='accuracy', random_state=42)

# Fit the Bayesian Optimization to your training data
bayes_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = bayes_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = bayes_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/23:
from skopt import BayesSearchCV

# Define the hyperparameters and their search space
param_dist = {
    'C': (0.001, 1000.0, 'log-uniform'),  # Tuple format (low, high, distribution)
}

# Initialize the Logistic Regression classifier
logistic_reg = LogisticRegression()

# Initialize Bayesian Optimization with cross-validation (e.g., 5-fold cross-validation)
bayes_search = BayesSearchCV(logistic_reg, param_dist, cv=5, n_iter=5, scoring='accuracy', random_state=42)

# Fit the Bayesian Optimization to your training data
bayes_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = bayes_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = bayes_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/24:
from deap import base, creator, tools, gp
import random

# Define the hyperparameters and their search space
param_space = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'penalty': ['l1', 'l2']
}

# Define the evaluation function
def evaluate_hyperparameters(hyperparameters):
    # hyperparameters is a dictionary like {'C': value, 'penalty': value}
    # Create and train the model with the given hyperparameters
    clf = LogisticRegression(**hyperparameters)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy,

# Create a toolbox for the Genetic Algorithm
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", dict, fitness=creator.FitnessMax)

toolbox = base.Toolbox()

# Define the attributes (hyperparameters) and their possible values
toolbox.register("attr_C", random.choice, param_space['C'])
toolbox.register("attr_penalty", random.choice, param_space['penalty'])

# Define the individuals (combination of hyperparameters)
toolbox.register("individual", tools.initCycle, creator.Individual, (toolbox.attr_C, toolbox.attr_penalty), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# Define the genetic operators (crossover and mutation)
toolbox.register("mate", tools.cxOnePoint)
toolbox.register("mutate", tools.mutUniformInt, low=0, up=len(param_space['C'])-1, indpb=0.05)

# Define the evaluation function and the type of optimization (maximize or minimize)
toolbox.register("evaluate", evaluate_hyperparameters)
toolbox.decorate("evaluate", tools.DeltaPenalty(evaluate_hyperparameters, (-1.0,)))

# Create the population and evolve it
population = toolbox.population(n=10)
hof = tools.HallOfFame(1)
stats = tools.Statistics(lambda ind: ind.fitness.values)
stats.register("avg", np.mean)
stats.register("min", np.min)
stats.register("max", np.max)

algorithms.eaMuPlusLambda(population, toolbox, mu=5, lambda_=5, cxpb=0.6, mutpb=0.3, ngen=10, stats=stats, halloffame=hof)

# Get the best individual (hyperparameters)
best_hyperparameters = hof[0]
print("Best Hyperparameters:", best_hyperparameters)

# Evaluate the model with the best hyperparameters
best_model = LogisticRegression(C=best_hyperparameters['C'], penalty=best_hyperparameters['penalty'])
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/25:
from deap import base, creator, tools, gp
import random

# Define the hyperparameters and their search space
param_space = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'penalty': ['l1', 'l2']
}

# Define the evaluation function
def evaluate_hyperparameters(hyperparameters):
    # hyperparameters is a dictionary like {'C': value, 'penalty': value}
    # Create and train the model with the given hyperparameters
    clf = LogisticRegression(**hyperparameters)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy,

# Create a toolbox for the Genetic Algorithm
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", dict, fitness=creator.FitnessMax)

toolbox = base.Toolbox()

# Define the attributes (hyperparameters) and their possible values
toolbox.register("attr_C", random.choice, param_space['C'])
toolbox.register("attr_penalty", random.choice, param_space['penalty'])

# Define the individuals (combination of hyperparameters)
toolbox.register("individual", tools.initCycle, creator.Individual, (toolbox.attr_C, toolbox.attr_penalty), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# Define the genetic operators (crossover and mutation)
toolbox.register("mate", tools.cxOnePoint)
toolbox.register("mutate", tools.mutUniformInt, low=0, up=len(param_space['C'])-1, indpb=0.05)

# Define the evaluation function and the type of optimization (maximize or minimize)
toolbox.register("evaluate", evaluate_hyperparameters)
toolbox.decorate("evaluate", tools.DeltaPenalty(evaluate_hyperparameters, (-1.0,)))

# Create the population and evolve it
population = toolbox.population(n=10)
hof = tools.HallOfFame(1)
stats = tools.Statistics(lambda ind: ind.fitness.values)
stats.register("avg", np.mean)
stats.register("min", np.min)
stats.register("max", np.max)

algorithms.eaMuPlusLambda(population, toolbox, mu=5, lambda_=5, cxpb=0.6, mutpb=0.3, ngen=10, stats=stats, halloffame=hof)

# Get the best individual (hyperparameters)
best_hyperparameters = hof[0]
print("Best Hyperparameters:", best_hyperparameters)

# Evaluate the model with the best hyperparameters
best_model = LogisticRegression(C=best_hyperparameters['C'], penalty=best_hyperparameters['penalty'])
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/26:
from deap import base, creator, tools, gp
import random

# Define the hyperparameters and their search space
param_space = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'penalty': ['l1', 'l2']
}

# Define the evaluation function
def evaluate_hyperparameters(individual):
    hyperparameters = {
        'C': individual[0],
        'penalty': individual[1]
    }
    # Create and train the model with the given hyperparameters
    clf = LogisticRegression(**hyperparameters)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy,

# Create a toolbox for the Genetic Algorithm
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()

# Define the attributes (hyperparameters) and their possible values
toolbox.register("attr_C", random.choice, param_space['C'])
toolbox.register("attr_penalty", random.choice, param_space['penalty'])

# Define the individuals (combination of hyperparameters)
toolbox.register("individual", tools.initCycle, creator.Individual, (toolbox.attr_C, toolbox.attr_penalty), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# Define the genetic operators (crossover and mutation)
toolbox.register("mate", tools.cxOnePoint)
toolbox.register("mutate", tools.mutUniformInt, low=0, up=len(param_space['C'])-1, indpb=0.05)

# Define the evaluation function and the type of optimization (maximize or minimize)
toolbox.register("evaluate", evaluate_hyperparameters)
toolbox.decorate("evaluate", tools.DeltaPenalty(evaluate_hyperparameters, (-1.0,)))

# Create the population and evolve it
population = toolbox.population(n=10)
hof = tools.HallOfFame(1)
stats = tools.Statistics(lambda ind: ind.fitness.values)
stats.register("avg", np.mean)
stats.register("min", np.min)
stats.register("max", np.max)

algorithms.eaMuPlusLambda(population, toolbox, mu=5, lambda_=5, cxpb=0.6, mutpb=0.3, ngen=10, stats=stats, halloffame=hof)

# Get the best individual (hyperparameters)
best_hyperparameters = hof[0]
print("Best Hyperparameters:", best_hyperparameters)

# Evaluate the model with the best hyperparameters
best_model = LogisticRegression(C=best_hyperparameters[0], penalty=best_hyperparameters[1])
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/27:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their search space
param_grid = {
    'n_estimators': [50, 100, 200],  # Number of trees in the forest
    'max_depth': [None, 5, 10, 20],  # Maximum depth of the trees
    'min_samples_split': [2, 5, 10]  # Minimum number of samples required to split an internal node
}

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier()

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/28:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their search space
param_grid = {
    'n_estimators': [50, 100, 200],  
    'max_depth': [None, 5, 10, 20], 
    'min_samples_split': [2, 5, 10]
}

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier()

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/29:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their search space
param_grid = {
    'n_estimators': [50, 100, 200],  
    'max_depth': [None, 5, 10, 20], 
    'min_samples_split': [2, 5, 10]
}

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier()

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/30:
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define the hyperparameters and their search space
param_dist = {
    'n_estimators': [50, 100, 200, 300],        # Number of trees in the forest
    'max_depth': [None, 5, 10, 20],            # Maximum depth of the trees
    'min_samples_split': [2, 5, 10],          # Minimum samples required to split an internal node
    'min_samples_leaf': [1, 2, 4],            # Minimum samples required to be at a leaf node
    'bootstrap': [True, False],                # Whether bootstrap samples are used when building trees
    'criterion': ['gini', 'entropy']          # Function to measure the quality of a split
}

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Initialize Randomized Search with cross-validation (e.g., 5-fold cross-validation)
random_search = RandomizedSearchCV(rf_classifier, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)

# Fit the Random Search to your training data
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = random_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/31:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
X, y = data.data, data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
76/32:
from skopt import BayesSearchCV
param_dist = {'param_name': (low, high, 'distribution')}
bayes_search = BayesSearchCV(estimator, param_dist, n_iter=5, scoring='accuracy', cv=5)
bayes_search.fit(X, y)
best_params = bayes_search.best_params_
76/33:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
X, y = data.data, data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
77/1:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
X, y = data.data, data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
77/2:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
77/3:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
77/4:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
77/5:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
77/6:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
77/7:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
77/8: df.describe(include="all")
77/9: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
77/10:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
77/11:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
77/12:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'Non-Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = ['Non-Churn', 'Churn']
legend_counts = [pivot_table[0].sum(), pivot_table[1].sum()]
legend_labels_with_counts = [f'{label} ({count})' for label, count in zip(legend_labels, legend_counts)]
ax.legend(legend_labels_with_counts)

plt.show()
77/13:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
77/14:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
77/15:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
77/16:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
X, y = data.data, data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
77/17:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xr_train, yr_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yr_pred = best_model.predict(Xr_test)
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
77/18:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
77/19:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression(max_iter=1000) 


# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
77/20:
from pyexpat import model
import pandas as pd

new_data = pd.DataFrame({
    'Name': ['John'],  # Example name
    'Location': [1],    # Example location
    'Subscription_Length_Months': [6],  # Example subscription length
    'Monthly_Bill': [50.0]  # Example monthly bill
})
y_pred_new = model.predict(new_data)
df['Predicted_Churn'] = y_pred_new
77/21:
from pyexpat import model
import pandas as pd

new_data = pd.DataFrame({
    'Name': ['John'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
77/22:
from pyexpat import model
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression(max_iter=1000) 


# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

import pandas as pd

new_data = pd.DataFrame({
    'Name': ['John'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
77/23:
from sklearn.linear_model import LogisticRegression

# Create a Logistic Regression classifier
clf = LogisticRegression(max_iter=1000) 

# Train the model
clf.fit(X_train, y_train)

import pandas as pd

new_data = pd.DataFrame({
    'Name': ['John'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

# Predict using the trained model
y_pred_new = clf.predict(new_data)
print("Predicted Churn:", y_pred_new)
77/24:
from sklearn.linear_model import LogisticRegression

# Create a Logistic Regression classifier
clf = LogisticRegression(max_iter=1000) 

# Train the model
clf.fit(X_train, y_train)

import pandas as pd

new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

# Predict using the trained model
y_pred_new = clf.predict(new_data)
print("Predicted Churn:", y_pred_new)
77/25:
import pandas as pd
from pyexpat import model
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression(max_iter=1000) 


# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)


new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

# Predict using the model
y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
77/26:
import pandas as pd
from pyexpat import model
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression(max_iter=1000) 


# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)


new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

y_pred_new = model.predict(new_data)
print("Predicted Churn:", y_pred_new)
77/27:
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression(max_iter=1000) 


# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)


new_data = pd.DataFrame({
    'Name': ['100000'],  
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
77/28:
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression(max_iter=1000) 


# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)


new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Age':  ['23']
    'Gender': [0] 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
77/29:
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression(max_iter=1000) 


# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)


new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Age':  ['23'],
    'Gender': [0], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
77/30:
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression(max_iter=1000) 


# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)


new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
77/31:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xl, yl_pred = data.data, data.target

# Split the data into training and testing sets
Xl_train, Xl_test, yl_train, yl_test = train_test_split(Xl, yl_pred, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xl_train, yl_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yl_pred = best_model.predict(Xl_test)
accuracy = accuracy_score(yl_test, yl_pred)
print("Accuracy:", accuracy)
77/32:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
77/33:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Create a Logistic Regression classifier
clf = LogisticRegression()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
77/34:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
77/35:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
77/36:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
77/37:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
77/38:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
77/39:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
77/40:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
77/41:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
77/42:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
77/43: df.describe(include="all")
77/44: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
77/45:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
77/46:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
77/47:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'Non-Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = ['Non-Churn', 'Churn']
legend_counts = [pivot_table[0].sum(), pivot_table[1].sum()]
legend_labels_with_counts = [f'{label} ({count})' for label, count in zip(legend_labels, legend_counts)]
ax.legend(legend_labels_with_counts)

plt.show()
77/48:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
77/49:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
77/50:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
77/51:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': 100
    'max_depth': None
     'min_samples_split': 2
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xr_train, yr_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yr_pred = best_model.predict(Xr_test)
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
77/52:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [100],  # List of values to try for n_estimators
    'max_depth': [None],    # List of values to try for max_depth
    'min_samples_split': [2]  # List of values to try for min_samples_split
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xr_train, yr_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yr_pred = best_model.predict(Xr_test)
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
77/53:
new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
77/54:

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
77/55:

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
77/56:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [100],  # List of values to try for n_estimators
    'max_depth': [None],    # List of values to try for max_depth
    'min_samples_split': [1]  # List of values to try for min_samples_split
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xr_train, yr_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yr_pred = best_model.predict(Xr_test)
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
77/57:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [100],  # List of values to try for n_estimators
    'max_depth': [None],    # List of values to try for max_depth
    'min_samples_split': [2]  # List of values to try for min_samples_split
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xr_train, yr_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yr_pred = best_model.predict(Xr_test)
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
77/58:
# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
77/59:
# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
77/60:
# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
77/61:
# Create a Random Forest classifier
# Assuming Xr_train is a DataFrame with explicit column names
clf = RandomForestClassifier(n_estimators=100, random_state=42, feature_names=['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill'])

# Train the model
clf.fit(Xr_train, yr_train)


# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
77/62:
# Assuming Xr_train and Xr_test are DataFrames with explicit column names
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
77/63:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
79/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
79/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
79/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
79/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
79/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
79/7: df.describe(include="all")
79/8: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
79/9:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
79/10:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
79/11:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'Non-Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = ['Non-Churn', 'Churn']
legend_counts = [pivot_table[0].sum(), pivot_table[1].sum()]
legend_labels_with_counts = [f'{label} ({count})' for label, count in zip(legend_labels, legend_counts)]
ax.legend(legend_labels_with_counts)

plt.show()
79/12:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
79/13:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
79/14:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
79/15:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [100],  # List of values to try for n_estimators
    'max_depth': [None],    # List of values to try for max_depth
    'min_samples_split': [2]  # List of values to try for min_samples_split
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xr_train, yr_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yr_pred = best_model.predict(Xr_test)
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
79/16:
# Create a Random Forest classifier
# Assuming Xr_train is a DataFrame with explicit column names
clf = RandomForestClassifier(n_estimators=100, random_state=42, feature_names=['Name', 'Location', 'Subscription_Length_Months', 'Monthly_Bill'])

# Train the model
clf.fit(Xr_train, yr_train)


# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
79/17:
# Assuming Xr_train and Xr_test are DataFrames with explicit column names
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
79/18:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
79/19:
# Create Decision Tree classifier object
clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
79/20:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/21:
# Create Decision Tree classifier object
clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
79/22:
# Checking for new data


new_data = pd.DataFrame({
    'Name': ['100000'], 
    'Location': [1],    
    'Subscription_Length_Months': [6],  
    'Monthly_Bill': [50.0]  
})

yn_pred = clf.predict(new_data)
print(yn_pred)
80/1:
import csv

# Define the new data
new_data = [
    ['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill'],
    ['Customer_100001', 33, 'Male', 'Houston', 23, 55.13],
    ['Customer_100002', 62, 'Female', 'New York', 19, 61.65],
    ['Customer_100003', 64, 'Male', 'Chicago', 17, 96.11],
    ['Customer_100004', 51, 'Female', 'New York', 20, 49.25],
    ['Customer_100005', 27, 'Female', 'Los Angeles', 19, 76.57],
    ['Customer_100006', 45, 'Male', 'San Francisco', 10, 65.28],
    ['Customer_100007', 32, 'Female', 'Los Angeles', 7, 79.15],
    ['Customer_100008', 50, 'Male', 'New York', 15, 92.73],
    ['Customer_100009', 28, 'Female', 'Chicago', 12, 60.34],
    ['Customer_100010', 39, 'Male', 'Miami', 6, 88.21],
]

# Define the CSV file path
csv_file_path = [r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset']

# Write the data to the CSV file
with open(csv_file_path, mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerows(new_data)

print(f'New data has been written to:', [r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset'])
80/2:
import csv

# Define the new data
new_data = [
    ['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill'],
    ['Customer_100001', 33, 'Male', 'Houston', 23, 55.13],
    ['Customer_100002', 62, 'Female', 'New York', 19, 61.65],
    ['Customer_100003', 64, 'Male', 'Chicago', 17, 96.11],
    ['Customer_100004', 51, 'Female', 'New York', 20, 49.25],
    ['Customer_100005', 27, 'Female', 'Los Angeles', 19, 76.57],
    ['Customer_100006', 45, 'Male', 'San Francisco', 10, 65.28],
    ['Customer_100007', 32, 'Female', 'Los Angeles', 7, 79.15],
    ['Customer_100008', 50, 'Male', 'New York', 15, 92.73],
    ['Customer_100009', 28, 'Female', 'Chicago', 12, 60.34],
    ['Customer_100010', 39, 'Male', 'Miami', 6, 88.21],
]

# Define the CSV file path
csv_file_path = [r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset']

for path in csv_file_path:
    with open(path, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerows(new_data)

print(f'New data has been written to:', [r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset'])
80/3:
import csv

csv_file_path = [r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv',
# Define the new data
new_data = [
    ['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill'],
    ['Customer_100001', 33, 'Male', 'Houston', 23, 55.13],
    ['Customer_100002', 62, 'Female', 'New York', 19, 61.65],
    ['Customer_100003', 64, 'Male', 'Chicago', 17, 96.11],
    ['Customer_100004', 51, 'Female', 'New York', 20, 49.25],
    ['Customer_100005', 27, 'Female', 'Los Angeles', 19, 76.57],
    ['Customer_100006', 45, 'Male', 'San Francisco', 10, 65.28],
    ['Customer_100007', 32, 'Female', 'Los Angeles', 7, 79.15],
    ['Customer_100008', 50, 'Male', 'New York', 15, 92.73],
    ['Customer_100009', 28, 'Female', 'Chicago', 12, 60.34],
    ['Customer_100010', 39, 'Male', 'Miami', 6, 88.21],
]

# Define the CSV file path


for path in csv_file_path:
    with open(path, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerows(new_data)

print(f'New data has been written to:', [r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset'])
80/4:
import csv

csv_file_path = [r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv']
# Define the new data
new_data = [
    ['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill'],
    ['Customer_100001', 33, 'Male', 'Houston', 23, 55.13],
    ['Customer_100002', 62, 'Female', 'New York', 19, 61.65],
    ['Customer_100003', 64, 'Male', 'Chicago', 17, 96.11],
    ['Customer_100004', 51, 'Female', 'New York', 20, 49.25],
    ['Customer_100005', 27, 'Female', 'Los Angeles', 19, 76.57],
    ['Customer_100006', 45, 'Male', 'San Francisco', 10, 65.28],
    ['Customer_100007', 32, 'Female', 'Los Angeles', 7, 79.15],
    ['Customer_100008', 50, 'Male', 'New York', 15, 92.73],
    ['Customer_100009', 28, 'Female', 'Chicago', 12, 60.34],
    ['Customer_100010', 39, 'Male', 'Miami', 6, 88.21],
]

# Define the CSV file path


for path in csv_file_path:
    with open(path, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerows(new_data)

print(f'New data has been written to:', [r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset'])
79/23:
# Checking for new data

New_Data = [r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv']

yn_pred = clf.predict(New)
print(yn_pred)
79/24:
# Checking for new data

New_Data = [r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv']

yn_pred = clf.predict(New_Data)
print(yn_pred)
79/25:
# Checking for new data
import pandas as pd
from sklearn.tree import DecisionTreeClassifier

New_Data = [r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv']

yn_pred = clf.predict(New_Data)
print(yn_pred)
79/26:
import joblib
import pandas as pd

# Load the trained classifier
clf = joblib.load(r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv')  # Replace with the actual path

# Load the new data
new_data = pd.read_csv(r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv')  # Replace with the actual path

# Assuming 'new_data' is in the same format as the data the classifier was trained on
yn_pred = clf.predict(new_data)
print(yn_pred)
79/27:
import joblib
import pandas as pd

# Load the trained classifier
clf = joblib.load[r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv']

# Load the new data
new_data = pd.read_csv[r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv']

# Assuming 'new_data' is in the same format as the data the classifier was trained on
yn_pred = clf.predict(new_data)
print(yn_pred)
79/28:
import joblib
import pandas as pd

clf = joblib.load(r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.joblib')
new_data = pd.read_csv(r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv')
yn_pred = clf.predict(new_data)
print(yn_pred)
79/29:
import joblib
import pandas as pd

clf = pd.read_csv(r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv')
new_data = pd.read_csv(r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv')
yn_pred = clf.predict(new_data)
print(yn_pred)
79/30:
import joblib
import pandas as pd

clf = joblib.load(r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.joblib')
new_data = pd.read_csv(r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\file1.csv')
yn_pred = clf.predict(new_data)
print(yn_pred)
79/31:
new_data = [
    ['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill'],
    ['Customer_100001', 33, 'Male', 'Houston', 23, 55.13],
    ['Customer_100002', 62, 'Female', 'New York', 19, 61.65],
    ['Customer_100003', 64, 'Male', 'Chicago', 17, 96.11],
    ['Customer_100004', 51, 'Female', 'New York', 20, 49.25],
    ['Customer_100005', 27, 'Female', 'Los Angeles', 19, 76.57],
    ['Customer_100006', 45, 'Male', 'San Francisco', 10, 65.28],
    ['Customer_100007', 32, 'Female', 'Los Angeles', 7, 79.15],
    ['Customer_100008', 50, 'Male', 'New York', 15, 92.73],
    ['Customer_100009', 28, 'Female', 'Chicago', 12, 60.34],
    ['Customer_100010', 39, 'Male', 'Miami', 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/32:
# Create Decision Tree classifier object
clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill'],
    ['Customer_100001', 33, 'Male', 'Houston', 23, 55.13],
    ['Customer_100002', 62, 'Female', 'New York', 19, 61.65],
    ['Customer_100003', 64, 'Male', 'Chicago', 17, 96.11],
    ['Customer_100004', 51, 'Female', 'New York', 20, 49.25],
    ['Customer_100005', 27, 'Female', 'Los Angeles', 19, 76.57],
    ['Customer_100006', 45, 'Male', 'San Francisco', 10, 65.28],
    ['Customer_100007', 32, 'Female', 'Los Angeles', 7, 79.15],
    ['Customer_100008', 50, 'Male', 'New York', 15, 92.73],
    ['Customer_100009', 28, 'Female', 'Chicago', 12, 60.34],
    ['Customer_100010', 39, 'Male', 'Miami', 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/33:
# Create Decision Tree classifier object
clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['Customer_100001', 33, 'Male', 'Houston', 23, 55.13],
    ['Customer_100002', 62, 'Female', 'New York', 19, 61.65],
    ['Customer_100003', 64, 'Male', 'Chicago', 17, 96.11],
    ['Customer_100004', 51, 'Female', 'New York', 20, 49.25],
    ['Customer_100005', 27, 'Female', 'Los Angeles', 19, 76.57],
    ['Customer_100006', 45, 'Male', 'San Francisco', 10, 65.28],
    ['Customer_100007', 32, 'Female', 'Los Angeles', 7, 79.15],
    ['Customer_100008', 50, 'Male', 'New York', 15, 92.73],
    ['Customer_100009', 28, 'Female', 'Chicago', 12, 60.34],
    ['Customer_100010', 39, 'Male', 'Miami', 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/34:
# Create Decision Tree classifier object
clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', 33, '0', '3', 23, 55.13],
    ['100002', 62, '1', '1', 19, 61.65],
    ['100003', 64, '0', '4', 17, 96.11],
    ['100004', 51, '1', '1', 20, 49.25],
    ['100005', 27, '1', '0', 19, 76.57],
    ['100006', 45, '0', '5', 10, 65.28],
    ['100007', 32, '1', '1', 7, 79.15],
    ['100008', 50, '0', '2', 15, 92.73],
    ['100009', 28, '1', '4', 12, 60.34],
    ['100010', 39, '0', '2', 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/35:
# Create Decision Tree classifier object
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', 33, '0', '3', 23, 55.13],
    ['100002', 62, '1', '1', 19, 61.65],
    ['100003', 64, '0', '4', 17, 96.11],
    ['100004', 51, '1', '1', 20, 49.25],
    ['100005', 27, '1', '0', 19, 76.57],
    ['100006', 45, '0', '5', 10, 65.28],
    ['100007', 32, '1', '1', 7, 79.15],
    ['100008', 50, '0', '2', 15, 92.73],
    ['100009', 28, '1', '4', 12, 60.34],
    ['100010', 39, '0', '2', 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/36:
# Create Decision Tree classifier object
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 23, 55.13],
    ['100002', '1', 19, 61.65],
    ['100003', '4', 17, 96.11],
    ['100004', '1', 20, 49.25],
    ['100005', '0', 19, 76.57],
    ['100006', '5', 10, 65.28],
    ['100007', '1', 7, 79.15],
    ['100008', '2', 15, 92.73],
    ['100009', '4', 12, 60.34],
    ['100010', '2', 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/37:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

clf = DecisionTreeClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [100],  # List of values to try for n_estimators
    'max_depth': [None],    # List of values to try for max_depth
    'min_samples_split': [2]  # List of values to try for min_samples_split
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xr_train, yr_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yr_pred = best_model.predict(Xr_test)
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
79/38:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 23, 55.13],
    ['100002', '1', 19, 61.65],
    ['100003', '4', 17, 96.11],
    ['100004', '1', 20, 49.25],
    ['100005', '0', 19, 76.57],
    ['100006', '5', 10, 65.28],
    ['100007', '1', 7, 79.15],
    ['100008', '2', 15, 92.73],
    ['100009', '4', 12, 60.34],
    ['100010', '2', 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/39:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

clf = DecisionTreeClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [100],  # List of values to try for n_estimators
    'max_depth': [None],    # List of values to try for max_depth
    'min_samples_split': [2]  # List of values to try for min_samples_split
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xr_train, yr_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yr_pred = best_model.predict(Xr_test)
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
79/40:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [100],  # List of values to try for n_estimators
    'max_depth': [None],    # List of values to try for max_depth
    'min_samples_split': [2]  # List of values to try for min_samples_split
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xr_train, yr_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yr_pred = best_model.predict(Xr_test)
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
79/41:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 23, 55.13],
    ['100002', '1', 19, 61.65],
    ['100003', '4', 17, 96.11],
    ['100004', '1', 20, 49.25],
    ['100005', '0', 19, 76.57],
    ['100006', '5', 10, 65.28],
    ['100007', '1', 7, 79.15],
    ['100008', '2', 15, 92.73],
    ['100009', '4', 12, 60.34],
    ['100010', '2', 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/42:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 23, 55.13],
    ['100002', '1', 19, 61.65],
    ['100003', '4', 17, 96.11],
    ['100004', '1', 20, 49.25],
    ['100005', '0', 19, 76.57],
    ['100006', '0', 10, 65.28],
    ['100007', '1', 7, 79.15],
    ['100008', '0', 15, 92.73],
    ['100009', '4', 12, 60.34],
    ['100010', '2', 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/43:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 23, 55.13],
    ['100002', '1', 19, 61.65],
    ['100003', '4', 17, 96.11],
    ['100004', '1', 20, 49.25],
    ['100005', '0', 19, 76.57],
    ['100006', '0', 10, 65.28],
    ['100007', '1', 7, 79.15],
    ['100008', '0', 15, 92.73],
    ['100009', '4', 12, 49.34],
    ['100010', '2', 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/44:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    [100001, 3, 23, 55.13],
    [100002, 1, 19, 61.65],
    [100003, 4, 17, 96.11],
    [100004, 1, 20, 49.25],
    [100005, 0, 19, 76.57],
    [100006, 0, 10, 65.28],
    [100007, 1, 7, 79.15],
    [100008, 0, 15, 92.73],
    [100009, 4, 12, 49.34],
    [100010, 2, 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/45:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    [100011, 3, 25, 58.39],
    [100012, 2, 18, 63.72],
    [100013, 0, 22, 45.81],
    [100014, 4, 16, 82.04],
    [100015, 1, 21, 71.98],
    [100016, 0, 13, 55.71],
    [100017, 2, 8, 67.23],
    [100018, 4, 11, 40.15],
    [100019, 1, 14, 78.36],
    [100020, 3, 9, 60.93]
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/46:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    [100001, 3, 23, 55.13],
    [100002, 1, 19, 61.65],
    [100003, 4, 17, 96.11],
    [100004, 1, 20, 49.25],
    [100005, 0, 19, 76.57],
    [100006, 0, 10, 65.28],
    [100007, 1, 7, 79.15],
    [100008, 0, 15, 92.73],
    [100009, 4, 12, 49.34],
    [100010, 2, 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/47:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    [100001, 2, 6, 55.13],
    [100002, 3, 19, 61.65],
    [100003, 1, 10, 96.11],
    [100004, 3, 20, 49.25],
    [100005, 3, 15, 76.57],
    [100006, 4, 10, 65.28],
    [100007, 3, 7, 79.15],
    [100008, 2, 20, 92.73],
    [100009, 5, 12, 49.34],
    [100010, 2, 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/48:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    [100001, 2, 6, 40],
    [100002, 3, 19, 50,50],
    [100003, 1, 10, 45.00],
    [100004, 3, 20, 49.25],
    [100005, 3, 15, 76.57],
    [100006, 4, 10, 65.28],
    [100007, 3, 7, 79.15],
    [100008, 2, 20, 92.73],
    [100009, 5, 12, 49.34],
    [100010, 2, 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/49:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    [100001, 2, 6, 40.00],
    [100002, 3, 19, 50,50],
    [100003, 1, 10, 45.00],
    [100004, 3, 20, 49.25],
    [100005, 3, 15, 76.57],
    [100006, 4, 10, 65.28],
    [100007, 3, 7, 79.15],
    [100008, 2, 20, 92.73],
    [100009, 5, 12, 49.34],
    [100010, 2, 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/50:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    [100001, 2, 6, 40.00],
    [100002, 3, 19, 50.50],
    [100003, 1, 10, 45.00],
    [100004, 3, 20, 49.25],
    [100005, 3, 15, 76.57],
    [100006, 4, 10, 65.28],
    [100007, 3, 7, 79.15],
    [100008, 2, 20, 92.73],
    [100009, 5, 12, 49.34],
    [100010, 2, 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/51:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    [100001, 2, 6, 40.00],
    [100002, 3, 19, 50.50],
    [100003, 1, 10, 45.00],
    [100004, 3, 20, 49.25],
    [100005, 3, 15, 76.57],
    [100006, 4, 10, 65.28],
    [100007, 3, 7, 79.15],
    [100008, 2, 20, 92.73],
    [100009, 5, 12, 49.34],
    [100010, 2, 6, 88.21],
]

# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print("Predicted Churn for new data:", yn_pred)
79/52:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

import random

new_data = [
    [100011, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100012, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100013, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100014, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100015, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100016, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100017, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100018, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100019, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100020, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)]
]

yn_pred = clf.predict(new_data)
print("Predicted Churn for new data:", yn_pred)
79/53:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

import random

new_data = [
    [100011, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100012, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100013, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100014, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100015, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100016, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100017, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100018, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100019, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100020, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)]
]

yn_pred = clf.predict(new_data)
print("Predicted Churn for new data:", yn_pred)
79/54:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

import random

new_data = [
    [100011, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100012, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100013, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100014, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100015, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100016, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100017, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100018, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100019, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)],
    [100020, random.randint(0, 5), random.randint(0, 20), round(random.uniform(40, 100), 2)]
]

yn_pred = clf.predict(new_data)
print("Predicted Churn for new data:", yn_pred)
79/55:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

import random

new_data = [[100001, 2, 6, 40.00], [100002, 3, 19, 50.50], [100003, 1, 10, 45.00], [100004, 3, 20, 49.25], [100005, 3, 15, 76.57], [100006, 4, 10, 65.28], [100007, 3, 7, 79.15], [100008, 2, 20, 92.73], [100009, 5, 12, 49.34], [100010, 2, 6, 88.21]]
y_pred = model.predict(new_data)

# Print the predicted churn for new data
print('Predicted churn for new data:', y_pred)
79/56:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

import random

new_data = [[100001, 2, 6, 40.00], [100002, 3, 19, 50.50], [100003, 1, 10, 45.00], [100004, 3, 20, 49.25], [100005, 3, 15, 76.57], [100006, 4, 10, 65.28], [100007, 3, 7, 79.15], [100008, 2, 20, 92.73], [100009, 5, 12, 49.34], [100010, 2, 6, 88.21]]
y_pred = clf.predict(new_data)

# Print the predicted churn for new data
print('Predicted churn for new data:', y_pred)
79/57:
new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '5', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '4', 4, 200.00],
    ['100010', '2', 5, 250.00],
]


# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/58:
new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]


# Make predictions on the test dataset
yn_pred = clf.predict(new_data)
print(yn_pred)
79/59:
import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE

X = df[['Name', 'Location', 'Subscription_Length_Months']]
y = df['Monthly_Bill']

# Oversample the data using SMOTE
oversampler = SMOTE()
X_resampled, y_resampled = oversampler.fit_resample(X, y)
79/60:
import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE

X = df[['Name', 'Location', 'Subscription_Length_Months']]
y = df['Monthly_Bill']

# Oversample the data using SMOTE
oversampler = SMOTE()
X_resampled, y_resampled = oversampler.fit_resample(X, y)

print('Number of samples in each class before oversampling:', df['target'].value_counts())
print('Number of samples in each class after oversampling:', y_resampled.value_counts())
79/61:
import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE

X = df[['Name', 'Location', 'Monthly_Bill']]
y = df['Subscription_Length_Months']

# Oversample the data using SMOTE
oversampler = SMOTE()
X_resampled, y_resampled = oversampler.fit_resample(X, y)

print('Number of samples in each class before oversampling:', df['target'].value_counts())
print('Number of samples in each class after oversampling:', y_resampled.value_counts())
79/62:
import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE

X = df[['Name', 'Location', 'Monthly_Bill']]
y = df['Subscription_Length_Months']

# Oversample the data using SMOTE
oversampler = SMOTE()
X_resampled, y_resampled = oversampler.fit_resample(X, y)

print('Number of samples in each class before oversampling:', df['Subscription_Length_Months'].value_counts())
print('Number of samples in each class after oversampling:', y_resampled.value_counts())
79/63:
new_data = ['100001', '4', '1', '50.00']

yn_pred = clf.predict(new_data)
print(yn_pred)
79/64:
new_data = [
    'Name': '100001', 
    'Location': '4', 
    'Subscription_Length_Months': '1', 
    'Monthly_Bill': '50.00'
]

yn_pred = clf.predict(new_data)
print(yn_pred)
79/65:
new_data_list = [[new_data['Name'], new_data['Location'], new_data['Subscription_Length_Months'], new_data['Monthly_Bill']]]

yn_pred = clf.predict(new_data_list)
print(yn_pred)
79/66:
new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = clf.predict(new_data_list)
print(yn_pred)
79/67:
new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = clf.predict(new_data)
print(yn_pred)
79/68:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

import random

new_data = [[100001, 2, 6, 40.00], [100002, 3, 19, 50.50], [100003, 1, 10, 45.00], [100004, 3, 20, 49.25], [100005, 3, 15, 76.57], [100006, 4, 10, 65.28], [100007, 3, 7, 79.15], [100008, 2, 20, 92.73], [100009, 5, 12, 49.34], [100010, 2, 6, 88.21]]
y_pred = clf.predict(new_data)

# Print the predicted churn for new data
print('Predicted churn for new data:', y_pred)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = clf.predict(new_data)
print(yn_pred)
79/69:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
79/70:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
79/71:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

clf = RandomForestClassifier()

# Define a grid of hyperparameters to search
param_grid = {
    'n_estimators': [100],  # List of values to try for n_estimators
    'max_depth': [None],    # List of values to try for max_depth
    'min_samples_split': [2]  # List of values to try for min_samples_split
}

# Initialize Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

# Fit the Grid Search to your training data
grid_search.fit(Xr_train, yr_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model on the test set
yr_pred = best_model.predict(Xr_test)
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
79/72:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/73:
XScaled_train_A  = X_train.apply(zscore) 
XScaled_test_A  = X_test.apply(zscore) 
XScaled_test_A.describe()
79/74:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV
 
# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}
 
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
 
# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)
 
tree_cv.fit(X, y)
 
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/75:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV
 
# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}
 
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
 
# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)
 
tree_cv.fit(Xr, yr)
 
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/76:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV
 
# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}
 
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
 
# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)
 
tree_cv.fit(X, y)
 
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/77:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV
 
 # Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}
 
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
 
# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)
 
tree_cv.fit(Xr, yr)
 
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/78:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV
 
 # Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}
 
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
 
# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)
 
tree_cv.fit(Xr, yr)
 
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/79:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV
 
 # Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target

# Split the data into training and testing sets
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}
 
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
 
# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)
 
tree_cv.fit(Xr, yr)
 
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/80:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}
79/81:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(2),
              "criterion": ["gini", "entropy"]}
79/82:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(2),
              "criterion": ["gini", "entropy"]}
79/83:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(4),
              "min_samples_leaf": randint(2),
              "criterion": ["gini", "entropy"]}
79/84:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(4),
              "min_samples_leaf": randint(2),
              "criterion": ["gini", "entropy"]}
79/85:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(4),
              "min_samples_leaf": randint(2),
              "criterion": ["gini", "entropy"]}
79/86:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Creating the hyperparameter grid 
param_dist = {"max_depth": [3, None],
              "max_features": randint(4),
              "min_samples_leaf": randint(2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
tree_cv.fit(Xr, yr)
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/87:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(4),
              "min_samples_leaf": randint(2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
tree_cv.fit(Xr, yr)
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/88:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
tree_cv.fit(Xr, yr)
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/89:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
tree_cv.fit(Xr, yr)
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/90:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
tree_cv.fit(Xr, yr)
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/91:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()
tree_cv.fit(Xr, yr)
# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/92:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/93:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/94:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
79/95:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
79/96:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/97:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/98:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/99:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/100:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/101:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/102:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/103:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
79/104:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/105:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/106:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
79/107:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
79/108:
from sklearn.metrics import confusion_matrix, classification_report

# Assuming yr_test and yr_pred are your true and predicted labels respectively
conf_matrix = confusion_matrix(yr_test, yr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision, recall, and F1-score
report = classification_report(yr_test, yr_pred)
print("\nClassification Report:")
print(report)
79/109:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
79/110:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
79/111:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
79/112:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
79/113:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
79/114:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
79/115:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00]
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
82/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
82/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
82/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
82/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
82/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
82/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
82/7: df.describe(include="all")
82/8: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
82/9:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
82/10:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
82/11:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'Non-Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = ['Non-Churn', 'Churn']
legend_counts = [pivot_table[0].sum(), pivot_table[1].sum()]
legend_labels_with_counts = [f'{label} ({count})' for label, count in zip(legend_labels, legend_counts)]
ax.legend(legend_labels_with_counts)

plt.show()
82/12:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
82/13:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
82/14:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
82/15:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
82/16:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
82/17:
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
82/18:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
82/19:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00]
]

yn_pred = clf.predict(new_data)
print('Predicted churn for new data:', y_pred)
82/20:
# Create Decision Tree classifier object
clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)
82/21:
# Create Decision Tree classifier object
from sklearn.metrics import confusion_matrix


clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)
 
# compute the confusion matrix
cm = confusion_matrix(y_test,y_pred)
82/22:
# Create Decision Tree classifier object
from sklearn.metrics import confusion_matrix


clf = clf = DecisionTreeClassifier()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)
 
# compute the confusion matrix
cm = confusion_matrix(y_test,y_pred)

#Plot the confusion matrix.
sns.heatmap(cm, 
            annot=True,
            fmt='g')
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)
plt.show()
 
 
# Finding precision and recall
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy   :", accuracy)
82/23:
# Create Decision Tree classifier object
from sklearn.metrics import confusion_matrix


clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)
 
# compute the confusion matrix
cm = confusion_matrix(yr_test,yr_pred)

#Plot the confusion matrix.
sns.heatmap(cm, 
            annot=True,
            fmt='g')
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)
plt.show()
 
 
# Finding precision and recall
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy   :", accuracy)
82/24:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
82/25:
# Create Decision Tree classifier object
from sklearn.metrics import confusion_matrix


clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)
 
# compute the confusion matrix
cm = confusion_matrix(yr_test,yr_pred)

#Plot the confusion matrix.
sns.heatmap(cm, 
            annot=True,
            fmt='g')
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)
plt.show()
 
 
# Finding precision and recall
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy   :", accuracy)
82/26:
# Create Decision Tree classifier object
from sklearn.metrics import confusion_matrix


clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

def prediction(X_test, clf_object): 
  
    # Predicton on test with giniIndex 
    y_pred = clf_object.predict(Xr_test) 
    print("Predicted values:") 
    print(yr_pred)
82/27:
# Create Decision Tree classifier object
from sklearn.metrics import confusion_matrix


clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

def prediction(X_test, clf_object): 
  
    # Predicton on test with giniIndex 
    y_pred = clf_object.predict(Xr_test) 
    print("Predicted values:") 
    print(yr_pred)
82/28:
# Create Decision Tree classifier object
from sklearn.metrics import confusion_matrix


clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

def prediction(X_test, clf_object): 
  
    # Predicton on test with giniIndex 
    y_pred = clf_object.predict(Xr_test) 
    print("Predicted values:") 
    print(yr_pred) 
    return y_pred
82/29:
# Create Decision Tree classifier object
from sklearn.metrics import classification_report, confusion_matrix


clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

def prediction(Xr_test, clf_object): 
  
    # Predicton on test with giniIndex 
    yr_pred = clf_object.predict(Xr_test) 
    print("Predicted values:") 
    print(yr_pred) 
    return yr_pred 

# Function to calculate accuracy 
def cal_accuracy(yr_test, yr_pred): 
      
    print("Confusion Matrix: ", 
        confusion_matrix(yr_test, yr_pred)) 
      
    print ("Accuracy : ", 
    accuracy_score(yr_test,yr_pred)*100) 
      
    print("Report : ", 
    classification_report(yr_test, yr_pred))
83/1:
# Create Decision Tree classifier object
from sklearn.metrics import classification_report, confusion_matrix


clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

def prediction(Xr_test, clf_object): 
  
    # Predicton on test with giniIndex 
    yr_pred = clf_object.predict(Xr_test) 
    print("Predicted values:") 
    print(yr_pred) 
    return yr_pred 

# Function to calculate accuracy 
def cal_accuracy(yr_test, yr_pred): 
      
    print("Confusion Matrix: ", 
        confusion_matrix(yr_test, yr_pred)) 
      
    print ("Accuracy : ", 
    accuracy_score(yr_test,yr_pred)*100) 
      
    print("Report : ", 
    classification_report(yr_test, yr_pred))
83/2:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
83/3:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
83/4:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
83/5:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
83/6:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
83/7:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
83/8: df.describe(include="all")
83/9: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
83/10:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
83/11:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
83/12:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'Non-Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = ['Non-Churn', 'Churn']
legend_counts = [pivot_table[0].sum(), pivot_table[1].sum()]
legend_labels_with_counts = [f'{label} ({count})' for label, count in zip(legend_labels, legend_counts)]
ax.legend(legend_labels_with_counts)

plt.show()
83/13:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
83/14:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
83/15:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
83/16:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
83/17:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
83/18:
# Create Decision Tree classifier object
from sklearn.metrics import classification_report, confusion_matrix


clf = DecisionTreeClassifier()

# Train the model
clf.fit(Xr_train, yr_train)

def prediction(Xr_test, clf_object): 
  
    # Predicton on test with giniIndex 
    yr_pred = clf_object.predict(Xr_test) 
    print("Predicted values:") 
    print(yr_pred) 
    return yr_pred 

# Function to calculate accuracy 
def cal_accuracy(yr_test, yr_pred): 
      
    print("Confusion Matrix: ", 
        confusion_matrix(yr_test, yr_pred)) 
      
    print ("Accuracy : ", 
    accuracy_score(yr_test,yr_pred)*100) 
      
    print("Report : ", 
    classification_report(yr_test, yr_pred))
83/19:
# Train Decision Tree Classifer
from sklearn import metrics


clf = clf.fit(Xr_train,yr_train)

#Predict the response for test dataset
y_pred = clf.predict(Xr_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
83/20:
# Train Decision Tree Classifer
from sklearn import metrics


clf = clf.fit(Xr_train,yr_train)

#Predict the response for test dataset
y_pred = clf.predict(Xr_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))
83/21:
# Train Decision Tree Classifer
from sklearn import metrics


clf = clf.fit(Xr_train,yr_train)

#Predict the response for test dataset
yr_pred = clf.predict(Xr_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))
83/22:
# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(Xr_train,yr_train)
#Predict the response for test dataset
yr_pred = clf.predict(Xr_test)
83/23:
# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(Xr_train,yr_train)
#Predict the response for test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)
83/24:
# Create Decision Tree classifier object
from sklearn import metrics


clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(Xr_train,yr_train)
#Predict the response for test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))
83/25:
# Create Decision Tree classifier object
from sklearn import metrics


clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(Xr_train,yr_train)
#Predict the response for test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = clf.predict(new_data)
print(yn_pred)
83/26:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
83/27:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))


# Create Decision Tree classifier object
from sklearn import metrics


clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = clf.fit(Xr_train,yr_train)
#Predict the response for test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))
83/28:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))


# Create Decision Tree classifier object
from sklearn import metrics


clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = tree_cv.fit(Xr_train,yr_train)
#Predict the response for test dataset
yr_pred = tree_cv.fit.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))
83/29:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))


# Create Decision Tree classifier object
from sklearn import metrics


clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = tree_cv.fit(Xr_train,yr_train)
#Predict the response for test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))
83/30:

from sklearn import metrics


clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = tree_cv.fit(Xr_train,yr_train)
#Predict the response for test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))
print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
83/31:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
83/32:
from sklearn import metrics

clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = tree_cv.fit(Xr_train,yr_train)
#Predict the response for test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
83/33:
from sklearn import metrics

clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = tree_cv.fit(Xr_train,yr_train)
#Predict the response for test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
83/34:
from sklearn import metrics

clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf = tree_cv.fit(Xr_train,yr_train)
#Predict the response for test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))
print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))

new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
83/35:
from sklearn import metrics

clf = DecisionTreeClassifier()
clf = tree_cv.fit(Xr_train,yr_train)
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))


new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
83/36:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
83/37:
from sklearn import metrics

clf = DecisionTreeClassifier()
clf = tree_cv.fit(Xr_train,yr_train)
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))


new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
84/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
84/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
84/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
84/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
84/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
84/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
84/7: df.describe(include="all")
84/8: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
84/9:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
84/10:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
84/11:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'Non-Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = ['Non-Churn', 'Churn']
legend_counts = [pivot_table[0].sum(), pivot_table[1].sum()]
legend_labels_with_counts = [f'{label} ({count})' for label, count in zip(legend_labels, legend_counts)]
ax.legend(legend_labels_with_counts)

plt.show()
84/12:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
84/13:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
84/14:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
84/15:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
84/16:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
84/17:
from sklearn import metrics

clf = DecisionTreeClassifier()
clf = tree_cv.fit(Xr_train,yr_train)
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))


new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
85/1:
# Splitting train and test set

X = df[['Drug Name', 'Drug Classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*', 'Chemical structure']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/2:
# Splitting train and test set

X = df[['Drug Name', 'Drug Classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*', 'Chemical structure']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/3:
# Splitting train and test set

X = df[['Drug Name', 'Drug Classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*', 'Chemical structure']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/4:
# Splitting train and test set

X = df[['Drug Name', 'Drug Classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*', 'Chemical structure']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/5:
# Splitting train and test set

X = df[['Drug Name', 'Drug Classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*', 'Chemical structure']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/6:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
85/7:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
85/8:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
85/9:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
85/10:
import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'large_dataset' with columns 'Drug Name', 'Target Class', and 'Probability*'
# You need to replace 'large_dataset' with your actual DataFrame name.

# Step 1: Load the Data
# Load your large dataset (replace 'large_dataset.csv' with your actual file path)
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Step 3: Generate a Pie Chart for Each Drug
for drug_name in unique_drugs:
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Create Pie Chart
    plt.figure(figsize=(8, 8))
    plt.pie(target_class_counts, labels=target_class_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(target_class_counts))))
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Distribution')

    # Display Pie Chart
    plt.show()
85/11:
# Splitting train and test set

X = df[['Drug Name', 'Drug Classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*', 'Chemical structure']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/12:
# Splitting train and test set

X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df['Probability*', 'Chemical structure']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/13:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/14:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
85/15:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/16:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique()
85/17:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/18:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique()
85/19:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/20:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
85/21:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/22:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
85/23:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/24:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
85/25:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
85/26:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
85/27:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
85/28:
import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'large_dataset' with columns 'Drug Name', 'Target Class', and 'Probability*'
# You need to replace 'large_dataset' with your actual DataFrame name.

# Step 1: Load the Data
# Load your large dataset (replace 'large_dataset.csv' with your actual file path)
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Step 3: Generate a Pie Chart for Each Drug
for drug_name in unique_drugs:
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Create Pie Chart
    plt.figure(figsize=(8, 8))
    plt.pie(target_class_counts, labels=target_class_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(target_class_counts))))
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Distribution')

    # Display Pie Chart
    plt.show()
85/29:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique()
85/30:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/31:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
85/32:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/33:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
85/34:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
85/35:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
85/36:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
85/37:
import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'large_dataset' with columns 'Drug Name', 'Target Class', and 'Probability*'
# You need to replace 'large_dataset' with your actual DataFrame name.

# Step 1: Load the Data
# Load your large dataset (replace 'large_dataset.csv' with your actual file path)
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Step 3: Generate a Pie Chart for Each Drug
for drug_name in unique_drugs:
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Create Pie Chart
    plt.figure(figsize=(8, 8))
    plt.pie(target_class_counts, labels=target_class_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(target_class_counts))))
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Distribution')

    # Display Pie Chart
    plt.show()
85/38:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique()
85/39:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/40:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
85/41:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/42:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/43:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
85/44:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/45:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique()
85/46:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/47:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
85/48:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/49:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/50:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/51:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique()
85/52:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/53:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/54:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
85/55:
print("Dataset with outliers")


# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:",(Y_train_data, "\n", (Y_test_data))
85/56:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
85/57:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique()
85/58:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
85/59:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
85/60:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
84/18:
# Random forest

import numpy as np
from sklearn import datasets, linear_model, metrics

# Create a Random Forest classifier
reg = linear_model.LinearRegression()

# train the model using the training sets
reg.fit(X_train, y_train)
 
# regression coefficients
print('Coefficients: ', reg.coef_)
 
# variance score: 1 means perfect prediction
print('Variance score: {}'.format(reg.score(X_test, y_test)))

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
84/19:
# Random forest

import numpy as np
from sklearn import datasets, linear_model, metrics

# Create a Random Forest classifier
reg = linear_model.LinearRegression()

# train the model using the training sets
reg.fit(X_train, y_train)
 


# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
84/20:
# Random forest

import numpy as np
from sklearn import datasets, linear_model, metrics

# Create a Random Forest classifier
reg = linear_model.LinearRegression()

# train the model using the training sets
reg.fit(X_train, y_train)
 
# regression coefficients
print('Coefficients: ', reg.coef_)
 
# variance score: 1 means perfect prediction
print('Variance score: {}'.format(reg.score(X_test, y_test)))

# Make predictions on the test dataset
y_pred = reg.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
86/1:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
87/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
87/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
87/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
87/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
87/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
87/7: df.describe(include="all")
87/8: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
87/9:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
87/10:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
87/11:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'Non-Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = ['Non-Churn', 'Churn']
legend_counts = [pivot_table[0].sum(), pivot_table[1].sum()]
legend_labels_with_counts = [f'{label} ({count})' for label, count in zip(legend_labels, legend_counts)]
ax.legend(legend_labels_with_counts)

plt.show()
87/12:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/13:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/14:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/15:
# Random forest

import numpy as np
from sklearn import datasets, linear_model, metrics

# Create a Random Forest classifier
reg = linear_model.LinearRegression()

# train the model using the training sets
reg.fit(X_train, y_train)
 

# Make predictions on the test dataset
y_pred = reg.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/16:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/17:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
87/18:
from sklearn import metrics

clf = DecisionTreeClassifier()
clf = tree_cv.fit(Xr_train,yr_train)
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))


new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
87/19:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/20:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
87/21:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
87/22:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
87/23:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
87/24:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
87/25: df.describe(include="all")
87/26: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
87/27:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
87/28:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Location', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Location', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Location")
plt.xlabel("Location")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = []
for location in pivot_table.index:
    non_churn_count = pivot_table.loc[location, 0]
    churn_count = pivot_table.loc[location, 1]
    legend_labels.append(f'{location} - Non-Churn ({non_churn_count}), Churn ({churn_count})')

ax.legend(legend_labels)

plt.show()
87/29:
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
df1 = df[['Subscription_Length_Months', 'Churn']]

# Create a pivot table to count churn and non-churn for each location
pivot_table = df1.pivot_table(index='Subscription_Length_Months', columns='Churn', aggfunc='size', fill_value=0)

# Set a larger figure size
plt.figure(figsize=(12, 8))

# Plot the grouped bar chart
ax = pivot_table.plot(kind='bar', stacked=True)
plt.title("Churn by Subscription Length (Months)")
plt.xlabel("Subscription Length (Months)")
plt.ylabel("Count")

# Add annotations to the bars
for p in ax.patches:
    if p.get_height() > 0:  
        if p.get_y() == 0: 
            ax.annotate(f'Non-Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')
        else:  # Churned
            ax.annotate(f'Churn: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height()), ha='center', va='top')

# Create a custom legend with counts
legend_labels = ['Non-Churn', 'Churn']
legend_counts = [pivot_table[0].sum(), pivot_table[1].sum()]
legend_labels_with_counts = [f'{label} ({count})' for label, count in zip(legend_labels, legend_counts)]
ax.legend(legend_labels_with_counts)

plt.show()
87/30:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/31:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/32:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/33:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/34:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
87/35:
from sklearn import metrics

clf = DecisionTreeClassifier()
clf = tree_cv.fit(Xr_train,yr_train)
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))


new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
87/36:
import seaborn as sns
sns.countplot(x='Age',data=df)
87/37:
import seaborn as sns
sns.countplot(x="Age", hue="gender", data=df)
87/38:
import seaborn as sns
sns.countplot(x="Gender", hue="gender", data=df)
87/39:
import seaborn as sns
sns.countplot(x="Gender", hue="Gender", data=df)
87/40:
import seaborn as sns
sns.countplot(x="Gender", y="Churn", hue="Gender", data=df)
87/41:
import seaborn as sns
import matplotlib.pyplot as plt

sns.barplot(x="Gender", y="Churn", data=df)
plt.show()
87/42:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/43:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/44:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
87/45:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
87/46:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
87/47:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
87/48:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
87/49: df.describe(include="all")
87/50: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
87/51:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
87/52:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/53:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/54:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/55:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/56:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
87/57:
from sklearn import metrics

clf = DecisionTreeClassifier()
clf = tree_cv.fit(Xr_train,yr_train)
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))


new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
87/58:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=customer_churn)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
87/59:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
87/60:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
87/61:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
87/62:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=customer_churn)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
87/63:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
87/64:
import matplotlib.pyplot as plt
import seaborn as sns

fig, ax = plt.subplots(figsize=(6,3), dpi=100)
sns.countplot(x='Sub Grade', hue='Loan Status', data=df)

ax.set_xlabel('Sub Grade')
ax.set_ylabel('Value count')

# Adding value counts on top of the bars
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.0f'), 
                (p.get_x() + p.get_width() / 2., p.get_height()), 
                ha = 'center', va = 'center', 
                xytext = (0, 9), 
                textcoords = 'offset points')

plt.show()
87/65:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')

plt.show()
87/66:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
87/67:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/68:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
87/69:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
87/70:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
87/71:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
87/72:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
87/73: df.describe(include="all")
87/74: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
87/75:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')

plt.show()
87/76:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
87/77:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/78:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/79:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/80:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/81:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
87/82:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
87/83:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
87/84:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
87/85:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
87/86: df.describe(include="all")
87/87: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
87/88:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
87/89:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
87/90:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/91:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/92:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/93:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
87/94:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/95:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/96:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/97:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/98:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
87/99:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
87/100:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
87/101:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
87/102:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
87/103: df.describe(include="all")
87/104: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
87/105:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
87/106:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
87/107:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
87/108:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/109:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/110:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/111:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/112:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
87/113:
from sklearn import metrics

clf = DecisionTreeClassifier()
clf = tree_cv.fit(Xr_train,yr_train)
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))


new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
87/114:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


print("Best score is {}".format(tree_cv.best_score_))
87/115:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/116:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/117:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/118:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/119:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/120:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


print("Best score is {}".format(param_dist))
87/121:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


print("Best score is {}".format(tree_cv.best_score_))
87/122:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/123:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


print("Best score is {}".format(data.best_score_))
87/124:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/125:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix
87/126:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix

from sklearn.metrics import classification_report
target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))
87/127:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix

from sklearn.metrics import classification_report
target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))
87/128:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix

from sklearn.metrics import classification_report
target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))
87/129:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import classification_report
target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))
87/130:
cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix
87/131:
cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix
87/132:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)



from sklearn.metrics import classification_report
target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)
87/133:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import classification_report
target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)
87/134:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)
87/135:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)
87/136:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/137:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/138:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/139:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/140:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/141:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/142:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/143:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/144:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/145:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(yr_test, yr_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(yr_test, yr_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/146:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}


# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/147:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(yr_test, yr_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(yr_test, yr_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/148:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}
87/149:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/150:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
y_pred = tree_cv.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/151:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
y_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/152:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
y_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/153:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/154:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(yr_test, yr_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(yr_test, yr_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/155:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/156:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
87/157:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
87/158:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
87/159:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
87/160:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
87/161: df.describe(include="all")
87/162: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
87/163:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
87/164:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
87/165:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
87/166:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/167:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/168:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/169:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/170:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/171:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/172:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/173:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/174:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/175:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(y_test, y_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/176:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}
87/177:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/178:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'non churn']
print(classification_report(yr_test, yr_pred, target_names=target_names))

cnf_matrix = metrics.confusion_matrix(yr_test, yr_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/179:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}
87/180:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/181:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'Non-Churn']
report = classification_report(yr_test, yr_pred, target_names=target_names)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/182:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

target_names = ['Churn', 'Non-Churn']
report = classification_report(yr_test, yr_pred, target_names=target_names)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/183:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/184:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/185:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/186:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/187:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/188:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/189:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/190:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/191:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/192:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/193:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}
87/194:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/195:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/196:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/197:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy)
87/198:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/199:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/200:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/201:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/202:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/203:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/204:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/205:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/206:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/207:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}
87/208:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/209:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}
87/210:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/211:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/212:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/213:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
87/214:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
87/215:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
87/216:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
87/217:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
87/218: df.describe(include="all")
87/219: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
87/220:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
87/221:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
87/222:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
87/223:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/224:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/225:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/226:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/227:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/228:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/229:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/230:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/231:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/232:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/233:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}
87/234:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/235:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/236:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/237:
from sklearn import metrics

clf = DecisionTreeClassifier()
clf = tree_cv.fit(Xr_train,yr_train)
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))


new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
87/238:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}
87/239:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/240:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/241:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/242:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/243:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/244:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/245:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/246:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/247:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/248:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/249:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/250:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/251:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/252:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}
87/253:
# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/254:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/255:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/256:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/257:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/258:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/259:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/260:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/261:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/262:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/263:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/264:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/265:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/266:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/267:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/268:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/269:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/270:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/271:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/272:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/273:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)


param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/274:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)


param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/275:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)


param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/276:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)


param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/277:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)


param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/278:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)


param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/279:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)


param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/280:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/281:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
87/282:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
87/283:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
87/284:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
87/285:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
87/286: df.describe(include="all")
87/287: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
87/288:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
87/289:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
87/290:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
87/291:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/292:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/293:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/294:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/295:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/296:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/297:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/298:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/299:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/300:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/301:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)


param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/302:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/303:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/304:
from sklearn import metrics

clf = DecisionTreeClassifier()
clf = tree_cv.fit(Xr_train,yr_train)
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))


new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
87/305:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/306:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/307:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/308:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/309:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/310:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/311:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/312:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/313:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/314:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/315:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/316:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/317:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/318:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/319:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = clf.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/320:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/321:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)


param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/322:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)


param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/323:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/324:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/325:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/326:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/327:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/328:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/329:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/330:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/331:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/332:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 4),
              "min_samples_leaf": randint(1, 2),
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the tuned parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/333:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating GridSearchCV object
tree_cv = GridSearchCV(tree, param_grid, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the best parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/334:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating GridSearchCV object
tree_cv = GridSearchCV(tree, param_grid, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the best parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/335:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating GridSearchCV object
tree_cv = GridSearchCV(tree, param_grid, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the best parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/336:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating GridSearchCV object
tree_cv = GridSearchCV(tree, param_grid, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the best parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/337:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating GridSearchCV object
tree_cv = GridSearchCV(tree, param_grid, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the best parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/338:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating GridSearchCV object
tree_cv = GridSearchCV(tree, param_grid, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the best parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/339:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating GridSearchCV object
tree_cv = GridSearchCV(tree, param_grid, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the best parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/340:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/341:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/342:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/343:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/344:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/345:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/346:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/347:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/348:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/349:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/350:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/351:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/352:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/353:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/354:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/355:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/356:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/357:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/358:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/359:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/360:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/361:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
87/362:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
87/363:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
87/364:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
87/365:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
87/366: df.describe(include="all")
87/367: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
87/368:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
87/369:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
87/370:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
87/371:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/372:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/373:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/374:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/375:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/376:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/377:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/378:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/379:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/380:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/381:
from scipy.stats import randint
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

param_grid = {"max_depth": [3, None],
              "max_features": [1, 2, 3, 4],
              "min_samples_leaf": [1, 2],
              "criterion": ["gini", "entropy"]}

# Instantiating Decision Tree classifier
tree = DecisionTreeClassifier()

# Instantiating GridSearchCV object
tree_cv = GridSearchCV(tree, param_grid, cv=5)

# Fitting the decision tree classifier to the training data
tree_cv.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)

# Printing the best parameters and score
print("Best score is {}".format(tree_cv.best_score_))
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
87/382:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/383:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = log.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/384:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(yr_test, yr_pred)
print(report)

# Generate the confusion matrix
cnf_matrix = confusion_matrix(yr_test, yr_pred)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['High Chance of Churn', 'Predicted Churn', 'Predicted Non-Churn'],
            yticklabels=['Actual High Chance of Churn', 'Actual Churn', 'Actual Non-Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/385:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report



# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred = knn.predict(Xr_test)
print(yr_pred)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred)
print("Accuracy:", accuracy_r)
87/386:
from sklearn import metrics

clf = DecisionTreeClassifier()
clf = tree_cv.fit(Xr_train,yr_train)
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))


new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
87/387:
from sklearn import metrics

clf = DecisionTreeClassifier()
clf = tree_cv.fit(Xr_train,yr_train)
yr_pred = tree_cv.predict(Xr_test)
print(yr_pred)

print("Accuracy:",metrics.accuracy_score(yr_test, yr_pred))


new_data = [
    ['100001', '3', 1, 50.00],
    ['100002', '1', 2, 100.00],
    ['100003', '4', 3, 150.00],
    ['100004', '1', 4, 200.00],
    ['100005', '0', 5, 250.00],
    ['100006', '3', 1, 50.00],
    ['100007', '1', 2, 100.00],
    ['100008', '2', 3, 150.00],
    ['100009', '2', 4, 200.00],
    ['100010', '1', 5, 250.00],
]

yn_pred = tree_cv.predict(new_data)
print(yn_pred)
87/388:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
87/389:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
87/390:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
87/391:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
87/392:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
87/393:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
87/394: df.describe(include="all")
87/395: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
87/396:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
87/397:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
87/398:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
87/399:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
87/400:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
87/401:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/402:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/403:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/404:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/405:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/406:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/407:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/408:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/409:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/410:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/411:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = log.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/412:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/413:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/414:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/415:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = knn.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/416:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/417:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/418:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/419:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/420:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred = clf.predict(X_test)
print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
87/421:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)
87/422:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/423:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
87/424:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_knn))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_knn)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/425:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)
87/426:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_tree))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_tree)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
87/427:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_clf = clf.predict(X_test)
print(y_pred_clf)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_clf)
print("Accuracy:", accuracy)
88/1:
# Fitting XGBoost to the training data
from sklearn.metrics import confusion_matrix
import xgboost as xgb

model = xgb.XGBClassifier()
model.fit(X_train, y_train)
88/2:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
88/3:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
88/4:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
88/5:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
88/6:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
88/7:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
88/8: df.describe(include="all")
88/9: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
88/10:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
88/11:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
88/12:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
88/13:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
88/14:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
88/15:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)
88/16:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
88/17:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
88/18:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_knn))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_knn)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
88/19:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)
88/20:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_tree))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_tree)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
88/21:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_clf = clf.predict(X_test)
print(y_pred_clf)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_clf)
print("Accuracy:", accuracy)
88/22:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
88/23:
# Fitting XGBoost to the training data
from sklearn.metrics import confusion_matrix
import xgboost as xgb

model = xgb.XGBClassifier()
model.fit(X_train, y_train)
88/24:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
88/25:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
88/26:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
88/27:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
88/28:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
88/29:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
88/30: df.describe(include="all")
88/31: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
88/32:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
88/33:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
88/34:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
88/35:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
88/36:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
88/37:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)
88/38:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
88/39:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
88/40:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_knn))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_knn)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
88/41:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)
88/42:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_tree))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_tree)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
88/43:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_clf = clf.predict(X_test)
print(y_pred_clf)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_clf)
print("Accuracy:", accuracy)
88/44:
# Fitting XGBoost to the training data
from sklearn.metrics import confusion_matrix
import xgboost as xgb

model = xgb.XGBClassifier()
model.fit(X_train, y_train)
88/45:
# Fitting XGBoost to the training data
from sklearn.metrics import confusion_matrix
import xgboost as xgb

dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)
model = xgb.XGBClassifier()
model.fit(dtrain)
88/46:
# Fitting XGBoost to the training data
from sklearn.metrics import confusion_matrix
import xgboost as xgb

model = xgb.XGBClassifier()
model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)
y_pred = model.predict(X_test)
88/47:
# Fitting XGBoost to the training data
from sklearn.metrics import confusion_matrix
import xgboost as xgb

X['Age'] = X['Age'].astype(str)

# Perform one-hot encoding
X_encoded = pd.get_dummies(X, columns=['Age'])

model = xgb.XGBClassifier()
model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)
y_pred = model.predict(X_test)
88/48:
# Import necessary libraries
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load a sample dataset (breast cancer dataset for example)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the XGBoost model
model_xgb = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss')
model_xgb.fit(X_train, y_train)

# Predict on the test set
y_pred_xgb = model_xgb.predict(X_test)

# Calculate accuracy
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f'Accuracy of XGBoost: {accuracy_xgb}')
88/49:
# Import necessary libraries
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# Initialize a base learner (Decision Tree for example)
base_learner = DecisionTreeClassifier(max_depth=1)

# Initialize and train the AdaBoost model
model_adaboost = AdaBoostClassifier(base_estimator=base_learner, n_estimators=50, random_state=42)
model_adaboost.fit(X_train, y_train)

# Predict on the test set
y_pred_adaboost = model_adaboost.predict(X_test)

# Calculate accuracy
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
print(f'Accuracy of AdaBoost: {accuracy_adaboost}')
88/50:
import xgboost as xgb
from sklearn.metrics import accuracy_score

# Assuming X_train and y_train are your training data
# Initialize and train the XGBoost model
model_xgb = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss')
model_xgb.fit(X_train, y_train)

# Make predictions on the test set
y_pred_xgb = model_xgb.predict(X_test)

# Calculate accuracy
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f'Accuracy of XGBoost: {accuracy_xgb}')
88/51:
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Initialize a base learner (Decision Tree for example)
base_learner = DecisionTreeClassifier(max_depth=1)

# Initialize and train the AdaBoost model
model_adaboost = AdaBoostClassifier(base_estimator=base_learner, n_estimators=50, random_state=42)
model_adaboost.fit(X_train, y_train)

# Make predictions on the test set
y_pred_adaboost = model_adaboost.predict(X_test)

# Calculate accuracy
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
print(f'Accuracy of AdaBoost: {accuracy_adaboost}')
88/52:
from sklearn.ensemble import AdaBoostClassifier


a_model = AdaBoostClassifier()
a_model.fit(X_train,y_train)
a_preds = a_model.predict(X_test)
print("AdaBoost Classifier accuracy: ", metrics.accuracy_score(y_test, a_preds))
print(classification_report(y_test, a_preds))
88/53:
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# Define the hyperparameter grids for each model
param_grid_logistic = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2']
}

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

param_grid_dt = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Models
models = {
    'Logistic Regression': (LogisticRegression(), param_grid_logistic),
    'K-Nearest Neighbors': (KNeighborsClassifier(), param_grid_knn),
    'Random Forest': (RandomForestClassifier(random_state=42), param_grid_rf),
    'Decision Tree': (DecisionTreeClassifier(), param_grid_dt)
}

# Perform hyperparameter tuning for each model
best_models = {}

for name, (model, param_grid) in models.items():
    grid_search = GridSearchCV(model, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_models[name] = grid_search.best_estimator_

    y_pred = grid_search.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy for {name}: {accuracy}')

# Access the best models using best_models dictionary
best_logistic = best_models['Logistic Regression']
best_knn = best_models['K-Nearest Neighbors']
best_rf = best_models['Random Forest']
best_dt = best_models['Decision Tree']
88/54:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Define hyperparameter grid for Logistic Regression
param_grid_logistic = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2']
}

# Initialize the Logistic Regression model
logistic = LogisticRegression()

# Grid Search
grid_search_logistic = GridSearchCV(logistic, param_grid=param_grid_logistic, scoring='accuracy', cv=5)
grid_search_logistic.fit(X_train, y_train)

# Get the best parameters and best estimator
best_params_logistic = grid_search_logistic.best_params_
best_estimator_logistic = grid_search_logistic.best_estimator_
88/55:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Define hyperparameter grid for Logistic Regression
param_grid_logistic = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l2']  # Use only 'l2' penalty for lbfgs solver
}

# Initialize the Logistic Regression model
logistic = LogisticRegression(solver='lbfgs')  # Specify the solver

# Grid Search
grid_search_logistic = GridSearchCV(logistic, param_grid=param_grid_logistic, scoring='accuracy', cv=5)
grid_search_logistic.fit(X_train, y_train)

# Get the best parameters and best estimator
best_params_logistic = grid_search_logistic.best_params_
best_estimator_logistic = grid_search_logistic.best_estimator_
88/56:
# Model building

# Logistic regression

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report
from scipy.stats import randint
from sklearn.datasets import load_iris


# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)


# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred_log = log.predict(Xr_test)
print(yr_pred_log)

# Calculate accuracy
accuracy_r = accuracy_score(yr_test, yr_pred_log)
print("Accuracy:", accuracy_r)
88/57:
# Model building

# Logistic regression

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report
from scipy.stats import randint
from sklearn.datasets import load_iris


# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)
88/58:
# Model building

# Logistic regression

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report
from scipy.stats import randint
from sklearn.datasets import load_iris

# Load the Iris dataset as an example
data = load_iris()
Xh, yh_pred = data.data, data.target
Xh_train, Xh_test, yh_train, yh_test = train_test_split(Xh, yh_pred, test_size=0.2, random_state=42)

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(Xh_train, yh_train)

# Make predictions on the test dataset
y_pred_log_h = log.predict(Xh_test)
print(y_pred_log_h)

# Calculate accuracy
accuracy_h = accuracy_score(yh_test, y_pred_log_h)
print("Accuracy:", accuracy_h)
88/59:
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Assuming X_train and y_train are your training data
# Initialize the Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Train the model
gb_classifier.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_gb = gb_classifier.predict(X_test)

# Calculate accuracy
accuracy_gb = accuracy_score(y_test, y_pred_gb)
print(f'Accuracy of Gradient Boosting Classifier: {accuracy_gb}')
88/60:
from sklearn.datasets import load_iris
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Initialize the Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Train the model
gb_classifier.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred_gb = gb_classifier.predict(Xr_test)

# Calculate accuracy
accuracy_gb = accuracy_score(yr_test, yr_pred_gb)
print(f'Accuracy of Gradient Boosting Classifier: {accuracy_gb}')
88/61:
from sklearn.datasets import load_iris
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Load the Iris dataset as an example
data = load_iris()
Xr, yr_pred = data.data, data.target
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr_pred, test_size=0.2, random_state=42)

# Initialize the Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Train the model
gb_classifier.fit(Xr_train, yr_train)

# Make predictions on the test dataset
yr_pred_gb = gb_classifier.predict(Xr_test)

# Calculate accuracy
accuracy_gb = accuracy_score(yr_test, yr_pred_gb)
print(f'Accuracy of Gradient Boosting Classifier: {accuracy_gb}')
88/62:
import lightgbm as lgb
from sklearn.metrics import accuracy_score

# Assuming X_train and y_train are your training data
# Initialize the LightGBM dataset
train_data = lgb.Dataset(X_train, label=y_train)

# Set hyperparameters for the model
params = {
    'objective': 'binary',
    'metric': 'binary_error',
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9
}

# Train the LightGBM model
num_round = 100
bst = lgb.train(params, train_data, num_round)

# Make predictions on the test dataset
y_pred_lgb = bst.predict(X_test, num_iteration=bst.best_iteration)
y_pred_lgb = [1 if x > 0.5 else 0 for x in y_pred_lgb]  # Convert probabilities to binary predictions

# Calculate accuracy
accuracy_lgb = accuracy_score(y_test, y_pred_lgb)
print(f'Accuracy of LightGBM Classifier: {accuracy_lgb}')
88/63:
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Assuming X_train and y_train are your training data

# Initialize the model
model = Sequential()

# Add layers to the model
model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))  # Input layer with 64 units and ReLU activation
model.add(Dense(units=32, activation='relu'))  # Hidden layer with 32 units and ReLU activation
model.add(Dense(units=1, activation='sigmoid'))  # Output layer with 1 unit (binary classification) and sigmoid activation

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Accuracy of the model: {accuracy}')
88/64:
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Assuming X_train and y_train are your training data

# Initialize the model
model = Sequential()

# Add layers to the model
model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))  # Input layer with 64 units and ReLU activation
model.add(Dense(units=32, activation='relu'))  # Hidden layer with 32 units and ReLU activation
model.add(Dense(units=1, activation='sigmoid'))  # Output layer with 1 unit (binary classification) and sigmoid activation

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Accuracy of the model: {accuracy}')
88/65:
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Assuming X and y are your feature and target data

# Initialize KFold with desired number of splits (K)
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Initialize the model (Logistic Regression in this case)
model = LogisticRegression()

# Store performance metrics for each fold
accuracy_scores = []

for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions on the test set
    y_pred = model.predict(X_test)
    
    # Calculate accuracy for this fold
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)

# Calculate average accuracy across all folds
average_accuracy = sum(accuracy_scores) / len(accuracy_scores)
print(f'Average Accuracy: {average_accuracy}')
88/66:
from sklearn.datasets import load_iris
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score


# Initialize the Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Train the model
gb_classifier.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_gb = gb_classifier.predict(X_test)

# Calculate accuracy
accuracy_gb = accuracy_score(y_test, y_pred_gb)
print(f'Accuracy of Gradient Boosting Classifier: {accuracy_gb}')
88/67:
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV

# Assuming X_train and y_train are your training data

# Initialize the Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier()

# Define hyperparameter grid for Grid Search
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5]
}

# Initialize Grid Search
grid_search = GridSearchCV(gb_classifier, param_grid, scoring='accuracy', cv=5)
grid_search.fit(X_train, y_train)

# Get the best parameters and best estimator
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_

# Evaluate the model with best parameters on test data
y_pred_gb = best_estimator.predict(X_test)
accuracy_gb = accuracy_score(y_test, y_pred_gb)
print(f'Best Accuracy: {accuracy_gb}')
88/68:
from sklearn.datasets import load_iris
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report


# Initialize the Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Train the model
gb_classifier.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_gb = gb_classifier.predict(X_test)

# Calculate accuracy
accuracy_gb = accuracy_score(y_test, y_pred_gb)
print(f'Accuracy of Gradient Boosting Classifier: {accuracy_gb}')

# K-Nearest Neighbors (KNN):

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
88/69:
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the hyperparameter search space for Gradient Boosting
param_dist_gb = {
    'n_estimators': randint(50, 200),  # Number of boosting stages
    'learning_rate': [0.01, 0.1, 0.2],  # Step size shrinking
    'max_depth': randint(3, 10)  # Maximum depth of the individual trees
}

# Initialize RandomizedSearchCV for Gradient Boosting
random_search_gb = RandomizedSearchCV(
    gb_classifier, param_distributions=param_dist_gb, n_iter=50, cv=5, n_jobs=-1, random_state=42
)

# Fit the model
random_search_gb.fit(X_train, y_train)

# Print the best parameters and their score
print("Best Parameters (Gradient Boosting): ", random_search_gb.best_params_)
print("Best Score (Gradient Boosting): ", random_search_gb.best_score_)

# Now, let's perform Random Search for K-Nearest Neighbors (KNN)

# Define the hyperparameter search space for KNN
param_dist_knn = {
    'n_neighbors': randint(1, 20),  # Number of neighbors
    'p': [1, 2],  # Power parameter for Minkowski metric
}

# Initialize RandomizedSearchCV for KNN
random_search_knn = RandomizedSearchCV(
    knn, param_distributions=param_dist_knn, n_iter=50, cv=5, n_jobs=-1, random_state=42
)

# Fit the model
random_search_knn.fit(X_train, y_train)

# Print the best parameters and their score
print("Best Parameters (KNN): ", random_search_knn.best_params_)
print("Best Score (KNN): ", random_search_knn.best_score_)
89/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
89/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
89/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
89/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
89/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
89/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
89/7: df.describe(include="all")
89/8: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
89/9:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
89/10:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
89/11:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
89/12:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
89/13:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
89/14:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)
89/15:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
89/16:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
89/17:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_knn))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_knn)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
89/18:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)
89/19:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_tree))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_tree)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
89/20:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_clf = clf.predict(X_test)
print(y_pred_clf)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_clf)
print("Accuracy:", accuracy)
89/21:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV



# Define hyperparameter grid for Logistic Regression
param_grid_logistic = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l2']  
}

# Initialize the Logistic Regression model
logistic = LogisticRegression(solver='lbfgs')

# Grid Search
grid_search_logistic = GridSearchCV(logistic, param_grid=param_grid_logistic, scoring='accuracy', cv=5)
grid_search_logistic.fit(X_train, y_train)

# Get the best parameters and best estimator
best_params_logistic = grid_search_logistic.best_params_
best_estimator_logistic = grid_search_logistic.best_estimator_
89/22:
from sklearn.neighbors import KNeighborsClassifier

# Define hyperparameter grid for KNN
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

# Initialize the KNN model
knn = KNeighborsClassifier()

# Grid Search
grid_search_knn = GridSearchCV(knn, param_grid=param_grid_knn, scoring='accuracy', cv=5)
grid_search_knn.fit(X_train, y_train)

# Get the best parameters and best estimator
best_params_knn = grid_search_knn.best_params_
best_estimator_knn = grid_search_knn.best_estimator_
89/23:
from sklearn.base import is_classifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the hyperparameter search space for Gradient Boosting
param_dist_gb = {
    'n_estimators': randint(50, 200),  # Number of boosting stages
    'learning_rate': [0.01, 0.1, 0.2],  # Step size shrinking
    'max_depth': randint(3, 10)  # Maximum depth of the individual trees
}

# Initialize RandomizedSearchCV for Gradient Boosting
random_search_gb = RandomizedSearchCV(
    is_classifier, param_distributions=param_dist_gb, n_iter=50, cv=5, n_jobs=-1, random_state=42
)

# Fit the model
random_search_gb.fit(X_train, y_train)

# Print the best parameters and their score
print("Best Parameters (Gradient Boosting): ", random_search_gb.best_params_)
print("Best Score (Gradient Boosting): ", random_search_gb.best_score_)

# Now, let's perform Random Search for K-Nearest Neighbors (KNN)

# Define the hyperparameter search space for KNN
param_dist_knn = {
    'n_neighbors': randint(1, 20),  # Number of neighbors
    'p': [1, 2],  # Power parameter for Minkowski metric
}

# Initialize RandomizedSearchCV for KNN
random_search_knn = RandomizedSearchCV(
    knn, param_distributions=param_dist_knn, n_iter=50, cv=5, n_jobs=-1, random_state=42
)

# Fit the model
random_search_knn.fit(X_train, y_train)

# Print the best parameters and their score
print("Best Parameters (KNN): ", random_search_knn.best_params_)
print("Best Score (KNN): ", random_search_knn.best_score_)
89/24:
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the hyperparameter search space for Gradient Boosting
param_dist_gb = {
    'n_estimators': randint(50, 200),  # Number of boosting stages
    'learning_rate': [0.01, 0.1, 0.2],  # Step size shrinking
    'max_depth': randint(3, 10)  # Maximum depth of the individual trees
}

# Initialize the Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Initialize RandomizedSearchCV for Gradient Boosting
random_search_gb = RandomizedSearchCV(
    gb_classifier, param_distributions=param_dist_gb, n_iter=50, cv=5, n_jobs=-1, random_state=42
)

# Fit the model
random_search_gb.fit(X_train, y_train)

# Print the best parameters and their score
print("Best Parameters (Gradient Boosting): ", random_search_gb.best_params_)
print("Best Score (Gradient Boosting): ", random_search_gb.best_score_)
89/25:
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the hyperparameter search space for Gradient Boosting
param_dist_gb = {
    'n_estimators': randint(50, 200),  # Number of boosting stages
    'learning_rate': [0.01, 0.1, 0.2],  # Step size shrinking
    'max_depth': randint(3, 10)  # Maximum depth of the individual trees
}

# Initialize the Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Initialize RandomizedSearchCV for Gradient Boosting
random_search_gb = RandomizedSearchCV(
    gb_classifier, param_distributions=param_dist_gb, n_iter=50, cv=5, n_jobs=-1, random_state=42
)

# Fit the model
random_search_gb.fit(X_train, y_train)

# Print the best parameters and their score
print("Best Parameters (Gradient Boosting): ", random_search_gb.best_params_)
print("Best Score (Gradient Boosting): ", random_search_gb.best_score_)
89/26:
from xgboost import XGBClassifier

# Model 1: XGBoost with default parameters
model_1 = XGBClassifier()

# Model 2: XGBoost with different parameters
model_2 = XGBClassifier(
    max_depth=5,
    learning_rate=0.1,
    n_estimators=100,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# Model 3: XGBoost with another set of parameters
model_3 = XGBClassifier(
    max_depth=3,
    learning_rate=0.2,
    n_estimators=200,
    subsample=0.9,
    colsample_bytree=0.9,
    random_state=42
)

# Model 4: XGBoost with more customized parameters
model_4 = XGBClassifier(
    max_depth=7,
    learning_rate=0.05,
    n_estimators=150,
    subsample=0.7,
    colsample_bytree=0.7,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=42
)

# Assuming X_train and y_train are your training data
model_1.fit(X_train, y_train)
model_2.fit(X_train, y_train)
model_3.fit(X_train, y_train)
model_4.fit(X_train, y_train)

# Now you can use these models for predictions on your test data.
y_pred_1 = model_1.predict(X_test)
y_pred_2 = model_2.predict(X_test)
y_pred_3 = model_3.predict(X_test)
y_pred_4 = model_4.predict(X_test)

# Evaluate the models as needed
90/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
90/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
90/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
90/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
90/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
90/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
90/7: df.describe(include="all")
90/8: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
90/9:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
90/10:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
90/11:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
90/12:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
90/13:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
90/14:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)
90/15:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
90/16:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
90/17:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_knn))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_knn)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
90/18:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)
90/19:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_tree))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_tree)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
90/20:
# Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report


# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_clf = clf.predict(X_test)
print(y_pred_clf)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_clf)
print("Accuracy:", accuracy)
90/21:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV



# Define hyperparameter grid for Logistic Regression
param_grid_logistic = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l2']  
}

# Initialize the Logistic Regression model
logistic = LogisticRegression(solver='lbfgs')

# Grid Search
grid_search_logistic = GridSearchCV(logistic, param_grid=param_grid_logistic, scoring='accuracy', cv=5)
grid_search_logistic.fit(X_train, y_train)

# Get the best parameters and best estimator
best_params_logistic = grid_search_logistic.best_params_
best_estimator_logistic = grid_search_logistic.best_estimator_
90/22:
from sklearn.neighbors import KNeighborsClassifier

# Define hyperparameter grid for KNN
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

# Initialize the KNN model
knn = KNeighborsClassifier()

# Grid Search
grid_search_knn = GridSearchCV(knn, param_grid=param_grid_knn, scoring='accuracy', cv=5)
grid_search_knn.fit(X_train, y_train)

# Get the best parameters and best estimator
best_params_knn = grid_search_knn.best_params_
best_estimator_knn = grid_search_knn.best_estimator_
90/23:
from xgboost import XGBClassifier

# Model 1: XGBoost with default parameters
model_1 = XGBClassifier()

# Model 2: XGBoost with different parameters
model_2 = XGBClassifier(
    max_depth=5,
    learning_rate=0.1,
    n_estimators=100,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# Model 3: XGBoost with another set of parameters
model_3 = XGBClassifier(
    max_depth=3,
    learning_rate=0.2,
    n_estimators=200,
    subsample=0.9,
    colsample_bytree=0.9,
    random_state=42
)

# Model 4: XGBoost with more customized parameters
model_4 = XGBClassifier(
    max_depth=7,
    learning_rate=0.05,
    n_estimators=150,
    subsample=0.7,
    colsample_bytree=0.7,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=42
)

# Assuming X_train and y_train are your training data
model_1.fit(X_train, y_train)
model_2.fit(X_train, y_train)
model_3.fit(X_train, y_train)
model_4.fit(X_train, y_train)

# Now you can use these models for predictions on your test data.
y_pred_1 = model_1.predict(X_test)
y_pred_2 = model_2.predict(X_test)
y_pred_3 = model_3.predict(X_test)
y_pred_4 = model_4.predict(X_test)

# Evaluate the models as needed
90/24:
from xgboost import XGBClassifier

# Model 1: XGBoost with default parameters
model_1 = XGBClassifier()

# Model 2: XGBoost with different parameters
model_2 = XGBClassifier(
    max_depth=5,
    learning_rate=0.1,
    n_estimators=100,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# Model 3: XGBoost with another set of parameters
model_3 = XGBClassifier(
    max_depth=3,
    learning_rate=0.2,
    n_estimators=200,
    subsample=0.9,
    colsample_bytree=0.9,
    random_state=42
)

# Model 4: XGBoost with more customized parameters
model_4 = XGBClassifier(
    max_depth=7,
    learning_rate=0.05,
    n_estimators=150,
    subsample=0.7,
    colsample_bytree=0.7,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=42
)

# Assuming X_train and y_train are your training data
model_1.fit(X_train, y_train)
model_2.fit(X_train, y_train)
model_3.fit(X_train, y_train)
model_4.fit(X_train, y_train)

# Now you can use these models for predictions on your test data.
y_pred_1 = model_1.predict(X_test)
y_pred_2 = model_2.predict(X_test)
y_pred_3 = model_3.predict(X_test)
y_pred_4 = model_4.predict(X_test)

# Evaluate the models as needed
90/25:
from xgboost import XGBClassifier

# Model 1: XGBoost with default parameters
model_1 = XGBClassifier()

# Model 2: XGBoost with different parameters
model_2 = XGBClassifier(
    max_depth=5,
    learning_rate=0.1,
    n_estimators=100,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# Model 3: XGBoost with another set of parameters
model_3 = XGBClassifier(
    max_depth=3,
    learning_rate=0.2,
    n_estimators=200,
    subsample=0.9,
    colsample_bytree=0.9,
    random_state=42
)

# Model 4: XGBoost with more customized parameters
model_4 = XGBClassifier(
    max_depth=7,
    learning_rate=0.05,
    n_estimators=150,
    subsample=0.7,
    colsample_bytree=0.7,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=42
)

# Assuming X_train and y_train are your training data
model_1.fit(X_train, y_train)
model_2.fit(X_train, y_train)
model_3.fit(X_train, y_train)
model_4.fit(X_train, y_train)

# Now you can use these models for predictions on your test data.
y_pred_1 = model_1.predict(X_test)
y_pred_2 = model_2.predict(X_test)
y_pred_3 = model_3.predict(X_test)
y_pred_4 = model_4.predict(X_test)

# Evaluate the models as needed
90/26:
# Assuming `X_train` and `X_test` are your training and test data DataFrames
# Convert categorical columns to numerical representation (e.g., one-hot encoding)
import xgboost


X_train_encoded = pd.get_dummies(X_train, columns=['Categorical_Column1', 'Categorical_Column2'])
X_test_encoded = pd.get_dummies(X_test, columns=['Categorical_Column1', 'Categorical_Column2'])

# Create DMatrix
dtrain = xgboost.DMatrix(X_train_encoded, label=y_train, enable_categorical=True)
dtest = xgboost.XGBClassifier.DMatrix(X_test_encoded, enable_categorical=True)

# Train the model
params = {'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42}
model = xgboost.XGBClassifier.train(params, dtrain)

# Make predictions
y_pred = model.predict(dtest)

# Evaluate the model
# (Use appropriate evaluation metric based on your problem - classification or regression)
90/27:
from xgboost import XGBClassifier

# Model 1: XGBoost with default parameters
model_1 = XGBClassifier()

# Model 2: XGBoost with different parameters
model_2 = XGBClassifier(
    max_depth=5,
    learning_rate=0.1,
    n_estimators=100,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# Model 3: XGBoost with another set of parameters
model_3 = XGBClassifier(
    max_depth=3,
    learning_rate=0.2,
    n_estimators=200,
    subsample=0.9,
    colsample_bytree=0.9,
    random_state=42
)

# Model 4: XGBoost with more customized parameters
model_4 = XGBClassifier(
    max_depth=7,
    learning_rate=0.05,
    n_estimators=150,
    subsample=0.7,
    colsample_bytree=0.7,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=42
)

# Assuming X_train and y_train are your training data
model_1.fit(X_train, y_train)
model_2.fit(X_train, y_train)
model_3.fit(X_train, y_train)
model_4.fit(X_train, y_train)

# Now you can use these models for predictions on your test data.
y_pred_1 = model_1.predict(X_test)
y_pred_2 = model_2.predict(X_test)
y_pred_3 = model_3.predict(X_test)
y_pred_4 = model_4.predict(X_test)

# Evaluate the models as needed
90/28:
from xgboost import XGBClassifier

# Model 1: XGBoost with default parameters
model_1 = XGBClassifier()

# Model 2: XGBoost with different parameters
model_2 = XGBClassifier(
    max_depth=5,
    learning_rate=0.1,
    n_estimators=100,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# Model 3: XGBoost with another set of parameters
model_3 = XGBClassifier(
    max_depth=3,
    learning_rate=0.2,
    n_estimators=200,
    subsample=0.9,
    colsample_bytree=0.9,
    random_state=42
)

# Model 4: XGBoost with more customized parameters
model_4 = XGBClassifier(
    max_depth=7,
    learning_rate=0.05,
    n_estimators=150,
    subsample=0.7,
    colsample_bytree=0.7,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=42
)

# Assuming X_train and y_train are your training data
model_1.fit(X_train, y_train)
model_2.fit(X_train, y_train)
model_3.fit(X_train, y_train)
model_4.fit(X_train, y_train)

# Now you can use these models for predictions on your test data.
y_pred_1 = model_1.predict(X_test)
y_pred_2 = model_2.predict(X_test)
y_pred_3 = model_3.predict(X_test)
y_pred_4 = model_4.predict(X_test)

# Evaluate the models as needed
90/29:
import xgboost as xgb

# Create regression matrices
dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=False)
dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=False)
90/30:
import xgboost as xgb

# Create regression matrices
dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)
dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)
90/31:
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost

# Logistic Regression
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)
logistic_accuracy = logistic_model.score(X_test, y_test)

# K-Nearest Neighbors (KNN)
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
knn_accuracy = knn_model.score(X_test, y_test)

# Decision Tree
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_accuracy = dt_model.score(X_test, y_test)

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_accuracy = rf_model.score(X_test, y_test)

# Continue with XGBoost code
params = {'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42}
xgb_model = xgboost.train(params, X_train)

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Compare the performance of all models
print(f'Logistic Regression Accuracy: {logistic_accuracy}')
print(f'K-Nearest Neighbors Accuracy: {knn_accuracy}')
print(f'Decision Tree Accuracy: {dt_accuracy}')
print(f'Random Forest Accuracy: {rf_accuracy}')
90/32:
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost

dtrain_reg = xgboost.XGBClassifier.DMatrix(X_train, y_train, enable_categorical=True)
dtest_reg = xgboost.XGBClassifier.DMatrix(X_test, y_test, enable_categorical=True)


# Logistic Regression
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)
logistic_accuracy = logistic_model.score(X_test, y_test)

# K-Nearest Neighbors (KNN)
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
knn_accuracy = knn_model.score(X_test, y_test)

# Decision Tree
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_accuracy = dt_model.score(X_test, y_test)

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_accuracy = rf_model.score(X_test, y_test)

# Continue with XGBoost code
params = {'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42}
xgb_model = xgboost.train(params, X_train)

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Compare the performance of all models
print(f'Logistic Regression Accuracy: {logistic_accuracy}')
print(f'K-Nearest Neighbors Accuracy: {knn_accuracy}')
print(f'Decision Tree Accuracy: {dt_accuracy}')
print(f'Random Forest Accuracy: {rf_accuracy}')
90/33:
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost

dtrain_reg = xgboost.XGBClassifier.DMatrix(X_train, y_train, enable_categorical=True)
dtest_reg = xgboost.XGBClassifier.DMatrix(X_test, y_test, enable_categorical=True)


# Logistic Regression
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)
logistic_accuracy = logistic_model.score(X_test, y_test)

# K-Nearest Neighbors (KNN)
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
knn_accuracy = knn_model.score(X_test, y_test)

# Decision Tree
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_accuracy = dt_model.score(X_test, y_test)

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_accuracy = rf_model.score(X_test, y_test)

# Continue with XGBoost code
params = {'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42}
xgb_model = xgboost.XGBClassifier.train(params, dtrain_reg)

# Make predictions
y_pred_xgb = xgb_model.predict(dtest_reg)

# Compare the performance of all models
print(f'Logistic Regression Accuracy: {logistic_accuracy}')
print(f'K-Nearest Neighbors Accuracy: {knn_accuracy}')
print(f'Decision Tree Accuracy: {dt_accuracy}')
print(f'Random Forest Accuracy: {rf_accuracy}')
90/34:
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Create regression matrices
dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)
dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)

# Logistic Regression
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)
logistic_accuracy = logistic_model.score(X_test, y_test)

# K-Nearest Neighbors (KNN)
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
knn_accuracy = knn_model.score(X_test, y_test)

# Decision Tree
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_accuracy = dt_model.score(X_test, y_test)

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_accuracy = rf_model.score(X_test, y_test)

# Continue with XGBoost code
params = {'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42}
xgb_model = xgb.train(params, dtrain_reg)

# Make predictions
y_pred_xgb = xgb_model.predict(dtest_reg)

# Evaluate XGBoost model (if necessary)
# ...

# Compare the performance of all models
print(f'Logistic Regression Accuracy: {logistic_accuracy}')
print(f'K-Nearest Neighbors Accuracy: {knn_accuracy}')
print(f'Decision Tree Accuracy: {dt_accuracy}')
print(f'Random Forest Accuracy: {rf_accuracy}')
90/35:
from sklearn.model_selection import GridSearchCV

# Define hyperparameters and their values to search
param_grid = {
    'penalty': ['l1', 'l2'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100]
}

# Create a grid search object
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')

# Fit the grid search to your data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train a new model with the best hyperparameters
best_logistic_model = LogisticRegression(**best_params)
best_logistic_model.fit(X_train, y_train)

# Evaluate the model
best_accuracy = best_logistic_model.score(X_test, y_test)

print(f'Best Hyperparameters: {best_params}')
print(f'Accuracy with Best Hyperparameters: {best_accuracy}')
90/36:
from sklearn.model_selection import GridSearchCV

# Define hyperparameters and their values to search
param_grid = {
    'penalty': ['l2'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100]
}

# Create a grid search object
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')

# Fit the grid search to your data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train a new model with the best hyperparameters
best_logistic_model = LogisticRegression(**best_params)
best_logistic_model.fit(X_train, y_train)

# Evaluate the model
best_accuracy = best_logistic_model.score(X_test, y_test)

print(f'Best Hyperparameters: {best_params}')
print(f'Accuracy with Best Hyperparameters: {best_accuracy}')
90/37:
from sklearn.model_selection import GridSearchCV

# Define hyperparameters and their values to search
param_grid = {
    'penalty': ['l1'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100]
}

# Create a grid search object
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')

# Fit the grid search to your data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train a new model with the best hyperparameters
best_logistic_model = LogisticRegression(**best_params)
best_logistic_model.fit(X_train, y_train)

# Evaluate the model
best_accuracy = best_logistic_model.score(X_test, y_test)

print(f'Best Hyperparameters: {best_params}')
print(f'Accuracy with Best Hyperparameters: {best_accuracy}')
90/38:
from sklearn.model_selection import GridSearchCV

# Define hyperparameters and their values to search
param_grid = {
    'penalty': ['l2'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100]
}

# Create a grid search object
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')

# Fit the grid search to your data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train a new model with the best hyperparameters
best_logistic_model = LogisticRegression(**best_params)
best_logistic_model.fit(X_train, y_train)

# Evaluate the model
best_accuracy = best_logistic_model.score(X_test, y_test)

print(f'Best Hyperparameters: {best_params}')
print(f'Accuracy with Best Hyperparameters: {best_accuracy}')
90/39:
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Define hyperparameters and their values to search
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance']
}

# Create a grid search object
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')

# Fit the grid search to your data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train a new model with the best hyperparameters
best_knn_model = KNeighborsClassifier(**best_params)
best_knn_model.fit(X_train, y_train)

# Evaluate the model
best_accuracy = best_knn_model.score(X_test, y_test)

print(f'Best Hyperparameters: {best_params}')
print(f'Accuracy with Best Hyperparameters: {best_accuracy}')
90/40:
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Define hyperparameters and their values to search
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance']
}

# Create a grid search object
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')

# Fit the grid search to your data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train a new model with the best hyperparameters
best_knn_model = KNeighborsClassifier(**best_params)
best_knn_model.fit(X_train, y_train)

# Evaluate the model
best_accuracy = best_knn_model.score(X_test, y_test)

print(f'Best Hyperparameters: {best_params}')
print(f'Accuracy with Best Hyperparameters: {best_accuracy}')
90/41:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

# Define hyperparameters and their values to search
param_grid = {
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a grid search object
grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')

# Fit the grid search to your data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train a new model with the best hyperparameters
best_dt_model = DecisionTreeClassifier(random_state=42, **best_params)
best_dt_model.fit(X_train, y_train)

# Evaluate the model
best_accuracy = best_dt_model.score(X_test, y_test)

print(f'Best Hyperparameters: {best_params}')
print(f'Accuracy with Best Hyperparameters: {best_accuracy}')
90/42:
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define hyperparameters and their values to search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a grid search object
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')

# Fit the grid search to your data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train a new model with the best hyperparameters
best_rf_model = RandomForestClassifier(random_state=42, **best_params)
best_rf_model.fit(X_train, y_train)

# Evaluate the model
best_accuracy = best_rf_model.score(X_test, y_test)

print(f'Best Hyperparameters: {best_params}')
print(f'Accuracy with Best Hyperparameters: {best_accuracy}')
90/43:
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Define hyperparameter distributions
param_distributions_lr = {
    'C': uniform(0.1, 10),
    'penalty': ['l1', 'l2']
}

param_distributions_knn = {
    'n_neighbors': randint(1, 20)
}

param_distributions_dt = {
    'max_depth': randint(1, 20),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 20)
}

param_distributions_rf = {
    'n_estimators': randint(50, 200),
    'max_depth': randint(1, 20),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 20)
}

# Create a RandomizedSearchCV object for each model
random_search_lr = RandomizedSearchCV(LogisticRegression(), param_distributions_lr, n_iter=100, cv=5, scoring='accuracy', random_state=42)
random_search_knn = RandomizedSearchCV(KNeighborsClassifier(), param_distributions_knn, n_iter=100, cv=5, scoring='accuracy', random_state=42)
random_search_dt = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions_dt, n_iter=100, cv=5, scoring='accuracy', random_state=42)
random_search_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions_rf, n_iter=100, cv=5, scoring='accuracy', random_state=42)

# Fit the RandomizedSearchCV objects to your data
random_search_lr.fit(X_train, y_train)
random_search_knn.fit(X_train, y_train)
random_search_dt.fit(X_train, y_train)
random_search_rf.fit(X_train, y_train)

# Get the best hyperparameters and models
best_params_lr = random_search_lr.best_params_
best_params_knn = random_search_knn.best_params_
best_params_dt = random_search_dt.best_params_
best_params_rf = random_search_rf.best_params_

best_lr_model = random_search_lr.best_estimator_
best_knn_model = random_search_knn.best_estimator_
best_dt_model = random_search_dt.best_estimator_
best_rf_model = random_search_rf.best_estimator_

# Evaluate the models
lr_accuracy = best_lr_model.score(X_test, y_test)
knn_accuracy = best_knn_model.score(X_test, y_test)
dt_accuracy = best_dt_model.score(X_test, y_test)
rf_accuracy = best_rf_model.score(X_test, y_test)

print(f'Logistic Regression Best Hyperparameters: {best_params_lr}')
print(f'Logistic Regression Accuracy: {lr_accuracy}')

print(f'K-Nearest Neighbors Best Hyperparameters: {best_params_knn}')
print(f'K-Nearest Neighbors Accuracy: {knn_accuracy}')

print(f'Decision Tree Best Hyperparameters: {best_params_dt}')
print(f'Decision Tree Accuracy: {dt_accuracy}')

print(f'Random Forest Best Hyperparameters: {best_params_rf}')
print(f'Random Forest Accuracy: {rf_accuracy}')
90/44:
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Define hyperparameter distributions
param_distributions_lr = {
    'C': uniform(0.1, 10),
    'penalty': ['l2']
}

param_distributions_knn = {
    'n_neighbors': randint(1, 20)
}

param_distributions_dt = {
    'max_depth': randint(1, 20),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 20)
}

param_distributions_rf = {
    'n_estimators': randint(50, 200),
    'max_depth': randint(1, 20),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 20)
}

# Create a RandomizedSearchCV object for each model
random_search_lr = RandomizedSearchCV(LogisticRegression(), param_distributions_lr, n_iter=100, cv=5, scoring='accuracy', random_state=42)
random_search_knn = RandomizedSearchCV(KNeighborsClassifier(), param_distributions_knn, n_iter=100, cv=5, scoring='accuracy', random_state=42)
random_search_dt = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions_dt, n_iter=100, cv=5, scoring='accuracy', random_state=42)
random_search_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions_rf, n_iter=100, cv=5, scoring='accuracy', random_state=42)

# Fit the RandomizedSearchCV objects to your data
random_search_lr.fit(X_train, y_train)
random_search_knn.fit(X_train, y_train)
random_search_dt.fit(X_train, y_train)
random_search_rf.fit(X_train, y_train)

# Get the best hyperparameters and models
best_params_lr = random_search_lr.best_params_
best_params_knn = random_search_knn.best_params_
best_params_dt = random_search_dt.best_params_
best_params_rf = random_search_rf.best_params_

best_lr_model = random_search_lr.best_estimator_
best_knn_model = random_search_knn.best_estimator_
best_dt_model = random_search_dt.best_estimator_
best_rf_model = random_search_rf.best_estimator_

# Evaluate the models
lr_accuracy = best_lr_model.score(X_test, y_test)
knn_accuracy = best_knn_model.score(X_test, y_test)
dt_accuracy = best_dt_model.score(X_test, y_test)
rf_accuracy = best_rf_model.score(X_test, y_test)

print(f'Logistic Regression Best Hyperparameters: {best_params_lr}')
print(f'Logistic Regression Accuracy: {lr_accuracy}')

print(f'K-Nearest Neighbors Best Hyperparameters: {best_params_knn}')
print(f'K-Nearest Neighbors Accuracy: {knn_accuracy}')

print(f'Decision Tree Best Hyperparameters: {best_params_dt}')
print(f'Decision Tree Accuracy: {dt_accuracy}')

print(f'Random Forest Best Hyperparameters: {best_params_rf}')
print(f'Random Forest Accuracy: {rf_accuracy}')
90/45:
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Define hyperparameter distributions
param_distributions_lr = {
    'C': uniform(0.1, 10),
    'penalty': ['l2']
}

param_distributions_knn = {
    'n_neighbors': randint(1, 20)
}

param_distributions_dt = {
    'max_depth': randint(1, 20),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 20)
}

param_distributions_rf = {
    'n_estimators': randint(50, 200),
    'max_depth': randint(1, 20),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 20)
}

# Create a RandomizedSearchCV object for each model
random_search_lr = RandomizedSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_distributions_lr, n_iter=100, cv=5, scoring='accuracy', random_state=42)
random_search_knn = RandomizedSearchCV(KNeighborsClassifier(), param_distributions_knn, n_iter=100, cv=5, scoring='accuracy', random_state=42)
random_search_dt = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions_dt, n_iter=100, cv=5, scoring='accuracy', random_state=42)
random_search_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions_rf, n_iter=100, cv=5, scoring='accuracy', random_state=42)

# Fit the RandomizedSearchCV objects to your data
random_search_lr.fit(X_train, y_train)
random_search_knn.fit(X_train, y_train)
random_search_dt.fit(X_train, y_train)
random_search_rf.fit(X_train, y_train)

# Get the best hyperparameters and models
best_params_lr = random_search_lr.best_params_
best_params_knn = random_search_knn.best_params_
best_params_dt = random_search_dt.best_params_
best_params_rf = random_search_rf.best_params_

best_lr_model = random_search_lr.best_estimator_
best_knn_model = random_search_knn.best_estimator_
best_dt_model = random_search_dt.best_estimator_
best_rf_model = random_search_rf.best_estimator_

# Evaluate the models
lr_accuracy = best_lr_model.score(X_test, y_test)
knn_accuracy = best_knn_model.score(X_test, y_test)
dt_accuracy = best_dt_model.score(X_test, y_test)
rf_accuracy = best_rf_model.score(X_test, y_test)

print(f'Logistic Regression Best Hyperparameters: {best_params_lr}')
print(f'Logistic Regression Accuracy: {lr_accuracy}')

print(f'K-Nearest Neighbors Best Hyperparameters: {best_params_knn}')
print(f'K-Nearest Neighbors Accuracy: {knn_accuracy}')

print(f'Decision Tree Best Hyperparameters: {best_params_dt}')
print(f'Decision Tree Accuracy: {dt_accuracy}')

print(f'Random Forest Best Hyperparameters: {best_params_rf}')
print(f'Random Forest Accuracy: {rf_accuracy}')
90/46:
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define hyperparameters and their ranges
param_grid_lr = {
    'C': [0.1, 1, 10, 100],
    'solver': ['lbfgs', 'liblinear'],
    'max_iter': [100, 1000]
}

# Create a GridSearchCV object
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to your data
grid_search_lr.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params_lr = grid_search_lr.best_params_
best_model_lr = grid_search_lr.best_estimator_
90/47:
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
from sklearn.linear_model import LogisticRegression

param_distributions_lr = {
    'C': uniform(0.1, 10),
    'penalty': ['l2']
}

# Create a GridSearchCV object
random_search_lr = RandomizedSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_distributions_lr, n_iter=100, cv=5, scoring='accuracy', random_state=42)

# Fit the GridSearchCV object to your data
random_search_lr.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params_lr = random_search_lr.best_params_
best_lr_model = random_search_lr.best_estimator_
lr_accuracy = best_lr_model.score(X_test, y_test)

print(f'Logistic Regression Best Hyperparameters: {best_params_lr}')
print(f'Logistic Regression Accuracy: {lr_accuracy}')
90/48:
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define hyperparameters and their ranges
param_grid_lr = {
    'C': [0.1, 1, 10, 100],
    'solver': ['lbfgs', 'liblinear'],
    'max_iter': [100, 1000]
}

# Create a GridSearchCV object
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to your data
grid_search_lr.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params_lr = grid_search_lr.best_params_
best_model_lr = grid_search_lr.best_estimator_
90/49:
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define hyperparameters and their ranges
param_grid_lr = {
    'C': [0.1, 1, 10, 100],
    'solver': ['lbfgs', 'liblinear'],
    'max_iter': [100, 1000]
}

# Create a GridSearchCV object
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to your data
grid_search_lr.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params_lr = grid_search_lr.best_params_
best_model_lr = grid_search_lr.best_estimator_

lr_accuracy = best_model_lr.score(X_test, y_test)

print(f'Logistic Regression Accuracy: {lr_accuracy}')
90/50:
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define hyperparameters and their ranges
param_grid_lr = {
    'C': [0.1, 1, 10, 100],
    'solver': ['lbfgs', 'liblinear'],
    'max_iter': [100, 1000]
}

# Create a GridSearchCV object
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to your data
grid_search_lr.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params_lr = grid_search_lr.best_params_
best_model_lr = grid_search_lr.best_estimator_

lr_accuracy = best_model_lr.score(X_test, y_test)

print(f'Logistic Regression Accuracy: {lr_accuracy}')
90/51:
from sklearn.ensemble import AdaBoostClassifier


a_model = AdaBoostClassifier()
a_model.fit(X_train,y_train)
a_preds = a_model.predict(X_test)
print("AdaBoost Classifier accuracy: ", metrics.accuracy_score(y_test, a_preds))
print(classification_report(y_test, a_preds))
90/52:
from sklearn.ensemble import RandomForestClassifier


model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "auto",
                                  max_leaf_nodes = 30)
model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print("Random forest classifier accuracy: ",metrics.accuracy_score(y_test, prediction_test))

print(classification_report(y_test, prediction_test))
90/53:
from sklearn.ensemble import RandomForestClassifier


model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "sqrt",
                                  max_leaf_nodes = 30)

model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print("Random forest classifier accuracy: ",metrics.accuracy_score(y_test, prediction_test))

print(classification_report(y_test, prediction_test))
90/54:
from sklearn.tree import DecisionTreeClassifier


dt_model = DecisionTreeClassifier()
dt_model.fit(X_train,y_train)
predictdt_y = dt_model.predict(X_test)
accuracy_dt = dt_model.score(X_test,y_test)
print("Decision Tree accuracy is :",accuracy_dt)
print(classification_report(y_test, predictdt_y))
90/55:
from sklearn.neighbors import KNeighborsClassifier


knn_model = KNeighborsClassifier(n_neighbors = 11) 
knn_model.fit(X_train,y_train)
predicted_y = knn_model.predict(X_test)
accuracy_knn = knn_model.score(X_test,y_test)
print("KNN accuracy:",accuracy_knn)
print(classification_report(y_test, predicted_y))
91/1:
import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'large_dataset' with columns 'Drug Name', 'Target Class', and 'Probability*'
# You need to replace 'large_dataset' with your actual DataFrame name.

# Step 1: Load the Data
# Load your large dataset (replace 'large_dataset.csv' with your actual file path)
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Step 3: Generate a Pie Chart for Each Drug
for drug_name in unique_drugs:
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Create Pie Chart
    plt.figure(figsize=(8, 8))
    wedges, texts, autotexts = plt.pie(target_class_counts, autopct='', startangle=140, colors=plt.cm.Paired(range(len(target_class_counts))))
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {count}' for label, count in zip(target_class_counts.index, target_class_counts)]
    plt.legend(wedges, legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

    # Display Pie Chart
    plt.show()
91/2:
import pandas as pd
import matplotlib.pyplot as plt



# Step 1: Load the Data
# Load your large dataset (replace 'large_dataset.csv' with your actual file path)
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Step 3: Generate a Pie Chart for Each Drug
for drug_name in unique_drugs:
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Create Pie Chart
    plt.figure(figsize=(8, 8))
    wedges, texts, autotexts = plt.pie(
        target_class_counts,
        autopct='%1.1f%%',  # Display percentages
        startangle=140,
        colors=colors,
    )
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {count}' for label, count in zip(target_class_counts.index, target_class_counts)]
    plt.legend(wedges, legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

    # Display Pie Chart
    plt.show()
91/3:
import pandas as pd
import matplotlib.pyplot as plt



# Step 1: Load the Data
# Load your large dataset (replace 'large_dataset.csv' with your actual file path)
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Step 3: Generate a Pie Chart for Each Drug
for drug_name in unique_drugs:
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Create Pie Chart
    plt.figure(figsize=(8, 8))
    wedges, texts, autotexts = plt.pie(
        None,
        autopct='%1.1f%%',  # Display percentages
        startangle=140,
        colors=colors,
    )
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {count}' for label, count in zip(target_class_counts.index, target_class_counts)]
    plt.legend(wedges, legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

    # Display Pie Chart
    plt.show()
91/4:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Step 3: Generate a Pie Chart for Each Drug
for drug_name in unique_drugs:
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Create Pie Chart
    plt.figure(figsize=(8, 8))
    wedges, texts, autotexts = plt.pie(
        percentages,  # Use percentages instead of counts
        autopct='%1.1f%%',  # Display percentages
        startangle=140,
        colors=colors,
    )
    plt.axis('equal')
    plt.title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    plt.legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

    # Display Pie Chart
    plt.show()
91/5:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Create a grid of subplots
fig, axes = plt.subplots(2, 5, figsize=(20, 10))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 5
    col = i % 5

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
91/6:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Create a grid of subplots (1 row, 2 columns)
fig, axes = plt.subplots(1, 2, figsize=(20, 10))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Create Pie Chart
    wedges, texts, autotexts = axes[i].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[i].axis('equal')
    axes[i].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[i].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
91/7:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Create a grid of subplots
fig, axes = plt.subplots(1, len(unique_drugs), figsize=(20, 5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Create Pie Chart
    wedges, texts, autotexts = axes[i].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[i].axis('equal')
    axes[i].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[i].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
91/8:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Create a single row with len(unique_drugs) pie charts
plt.figure(figsize=(20, 5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Create Pie Chart
    plt.subplot(1, len(unique_drugs), i+1)
    wedges, texts, autotexts = plt.pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    plt.axis('equal')
    plt.title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    plt.legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
91/9:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Create a grid with 5 rows and 2 columns for the pie charts
fig, axes = plt.subplots(5, 2, figsize=(15, 20))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
91/10:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Create a grid with 5 rows and 2 columns for the pie charts
fig, axes = plt.subplots(5, 2, figsize=(10, 15))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
91/11:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
91/12:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

model = DecisionTreeRegressor()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/13:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

model = DecisionTreeRegressor()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/14:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

model = DecisionTreeRegressor()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/15:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

model = DecisionTreeRegressor()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/16:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
91/17:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
91/18:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
91/19:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
91/20:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
91/21:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique()
91/22:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
91/23:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
91/24:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
91/25:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

model = DecisionTreeRegressor()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/26:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

model = DecisionTreeRegressor()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/27:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

model = DecisionTreeRegressor()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)
print(y_pred)
91/28:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create Decision Tree classifier object

model = DecisionTreeRegressor()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)
print(y_pred)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/29:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Create Decision Tree classifier object

model = DecisionTreeRegressor()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)
print(y_pred)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/30:
# Model building

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Create Decision Tree classifier object

model = DecisionTreeRegressor()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/31:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Create Decision Tree classifier object

model = DecisionTreeRegressor()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/32:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/33:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/34:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/35:
# Create a Linear Regression model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/36:
# Create a Linear Regression model
from statistics import LinearRegression
from sklearn.metrics import mean_squared_error

model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/37:
# Create a Linear Regression model
from statistics import LinearRegression
from sklearn.metrics import mean_squared_error

model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
91/38:
from sklearn.ensemble import RandomForestRegressor

# Create Random Forest Regressor object
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust the number of estimators

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions
rf_y_pred = rf_model.predict(X_test)

# Evaluate the model
rf_mse = mean_squared_error(y_test, rf_y_pred)
print("Random Forest Mean Squared Error:", rf_mse)
91/39:
from sklearn.ensemble import RandomForestRegressor

# Create Random Forest Regressor object
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust the number of estimators

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions
rf_y_pred = rf_model.predict(X_test)

# Evaluate the model
rf_mse = mean_squared_error(y_test, rf_y_pred)
print("Random Forest Mean Squared Error:", rf_mse)
91/40:
from sklearn.metrics import mean_absolute_error, r2_score

# Make predictions for Decision Tree
dt_y_pred = model.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)
91/41:
# Make predictions for Random Forest
rf_y_pred = rf_model.predict(X_test)

# Calculate Mean Absolute Error (MAE)
rf_mae = mean_absolute_error(y_test, rf_y_pred)
print("Random Forest Mean Absolute Error:", rf_mae)

# Calculate R-squared (R2) score
rf_r2 = r2_score(y_test, rf_y_pred)
print("Random Forest R-squared (R2) Score:", rf_r2)
91/42:
from sklearn.metrics import mean_absolute_error, r2_score

# Make predictions for Decision Tree
dt_y_pred = model.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)
91/43:
from sklearn.ensemble import RandomForestRegressor

# Create Random Forest Regressor object
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust the number of estimators

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions
rf_y_pred = rf_model.predict(X_test)

# Evaluate the model
rf_mse = mean_squared_error(y_test, rf_y_pred)
print("Random Forest Mean Squared Error:", rf_mse)
91/44:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)
93/1:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=6) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
94/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
94/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
94/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
94/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
display(Name_df)
display(Age_df)
display(Churn_df)
94/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
94/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
94/7: df.describe(include="all")
94/8: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
94/9:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
94/10:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
94/11:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
94/12:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
94/13:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
94/14:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)
94/15:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
94/16:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=6) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
94/17:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_knn))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_knn)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
94/18:
from sklearn.tree import DecisionTreeClassifier


dt_model = DecisionTreeClassifier()
dt_model.fit(X_train,y_train)
predictdt_y = dt_model.predict(X_test)
accuracy_dt = dt_model.score(X_test,y_test)
print("Decision Tree accuracy is :",accuracy_dt)
print(classification_report(y_test, predictdt_y))
94/19:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)
94/20:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_tree))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_tree)
print(cnf_matrix)

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
94/21:
from sklearn.ensemble import RandomForestClassifier


model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "sqrt",
                                  max_leaf_nodes = 30)

model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print("Random forest classifier accuracy: ",metrics.accuracy_score(y_test, prediction_test))

print(classification_report(y_test, prediction_test))
94/22:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)
94/23:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=4) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
94/24:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=1) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
94/25:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=10) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
94/26:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
94/27:
# Creating adaboost classifier model
from sklearn.ensemble import AdaBoostClassifier


adb = AdaBoostClassifier()
adb_model = adb.fit(X_train,y_train)
94/28:
# Creating adaboost classifier model
from sklearn.ensemble import AdaBoostClassifier


adb = AdaBoostClassifier()
adb_model = adb.fit(X_train,y_train)
94/29:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)
94/30:
# Creating adaboost classifier model
from sklearn.ensemble import AdaBoostRegressor

model_ABR = AdaBoostRegressor()
model_ABR.fit(X_train, y_train)
print(model_ABR)
94/31:
from sklearn.ensemble import AdaBoostClassifier


model_ABC = AdaBoostClassifier()
model_ABC.fit(X_train, y_train)
print(model_ABC)
predicted_y = model_ABC.predict(X_test)
print(model_ABC)
94/32:
from sklearn.ensemble import AdaBoostClassifier


model_ABC = AdaBoostClassifier()
model_ABC.fit(X_train, y_train)
print(model_ABC)
expected_y  = y_test
predicted_y = model_ABC.predict(X_test)
print(metrics.classification_report(expected_y, predicted_y))
print(metrics.confusion_matrix(expected_y, predicted_y))
94/33:
from sklearn.ensemble import AdaBoostClassifier


model_ABC = AdaBoostClassifier()
model_ABC.fit(X_train, y_train)
print(model_ABC)
expected_y  = y_test
predicted_y = model_ABC.predict(X_test)
print("AdaBoost Classifier accuracy: ", metrics.accuracy_score(y_test, predicted_y))
print(metrics.classification_report(expected_y, predicted_y))
print(metrics.confusion_matrix(expected_y, predicted_y))
92/1:
# Get the features of the 10 drugs
drug_features_10 = df[-14, :-1]

# Predict the strength of the drug interaction for the 10 drugs
y_pred_10 = model.predict(drug_features_10)

# Print the results
print("Predicted strength of drug interaction for the 10 drugs:", y_pred_10)
92/2:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
92/3:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
92/4:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
92/5:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
92/6:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
92/7:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique()
92/8:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
92/9:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
92/10:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
92/11:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)
92/12:
from sklearn.metrics import mean_absolute_error, r2_score

# Make predictions for Decision Tree
dt_y_pred = model.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)
92/13:
# Get the features of the 10 drugs
drug_features_10 = df[-14, :-1]

# Predict the strength of the drug interaction for the 10 drugs
y_pred_10 = model.predict(drug_features_10)

# Print the results
print("Predicted strength of drug interaction for the 10 drugs:", y_pred_10)
94/34:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)
94/35:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_tree))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_tree)
print(cnf_matrix)
94/36:
from sklearn.ensemble import RandomForestClassifier


model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "sqrt",
                                  max_leaf_nodes = 30)

model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print("Random forest classifier accuracy: ",metrics.accuracy_score(y_test, prediction_test))

print(classification_report(y_test, prediction_test))
94/37:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_knn))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_knn)
print(cnf_matrix)
92/14:
import pandas as pd

# Load the drug data
df = pd.read_csv("drug_data.csv")

# Get the features of the last 10 drugs
drug_features_10 = df.iloc[-10:, :-1]

# Predict the strength of the drug interaction for the last 10 drugs
y_pred_10 = model.predict(drug_features_10)

# Print the results
print("Predicted strength of drug interaction for the last 10 drugs:", y_pred_10)
92/15:
import pandas as pd

# Load the drug data
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Get the features of the last 10 drugs
drug_features_10 = df.iloc[-10:, :-1]

# Predict the strength of the drug interaction for the last 10 drugs
y_pred_10 = model.predict(drug_features_10)

# Print the results
print("Predicted strength of drug interaction for the last 10 drugs:", y_pred_10)
92/16:
import pandas as pd

# Load the drug data
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Get the features of the last 10 drugs
drug_features_10 = df.iloc[-10:, :-1]

# Get the features in the order they were in the training data
features = df.columns[:-1]

# Remove the features that the model has not seen before
drug_features_10 = drug_features_10.drop(columns=["Probability*"])

# Predict the strength of the drug interaction for the last 10 drugs
y_pred_10 = model.predict(drug_features_10)

# Print the results
print("Predicted strength of drug interaction for the last 10 drugs:", y_pred_10)
92/17:
import pandas as pd

df = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Probability*', 'Chemical structure']]
 

# Get the features of the last 10 drugs
drug_features_10 = df.iloc[-10:, :-1]

# Get the features in the order they were in the training data
features = df.columns[:-1]

# Remove the features that the model has not seen before
drug_features_10 = drug_features_10.drop(columns=["Probability*"])

# Predict the strength of the drug interaction for the last 10 drugs
y_pred_10 = model.predict(drug_features_10)

# Print the results
print("Predicted strength of drug interaction for the last 10 drugs:", y_pred_10)
92/18:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Calculate the predicted strength of drug interaction for each combination
predicted_drug_interaction_strength = pd.Series(y_pred, index=df_test.index)
92/19:
# Model building
from os import F_TEST
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Calculate the predicted strength of drug interaction for each combination
predicted_drug_interaction_strength = pd.Series(y_pred, index=F_TEST.index)
92/20:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Calculate the predicted strength of drug interaction for each combination
predicted_drug_interaction_strength = pd.Series(y_pred, index=X_test.index)
92/21:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Calculate the predicted strength of drug interaction for each combination
predicted_drug_interaction_strength = pd.Series(y_pred, index=y_test.index)
92/22:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Flatten the y_pred array
y_pred_flat = np.flatten(y_pred)

# Calculate the predicted strength of drug interaction for each combination
predicted_drug_interaction_strength = pd.Series(y_pred_flat, index=y_test.index)
92/23:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Flatten the y_pred array
y_pred_flat = np.flatten(y_pred)

# Calculate the predicted strength of drug interaction for each combination
predicted_drug_interaction_strength = pd.Series(y_pred_flat, index=y_test.index)
94/38:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
# display(Name_df)
# display(Age_df)
display(Churn_df)
94/39:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)

# Perform k-fold cross-validation
cv_scores = cross_val_score(log, X, y, cv=KFold, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')
94/40:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(log, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')
94/41:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)
94/42:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(knn, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')
94/43:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)

# Perform k-fold cross-validation
cv_scores = cross_val_score(clf X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')
94/44:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)

# Perform k-fold cross-validation
cv_scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')
94/45:
from sklearn.ensemble import RandomForestClassifier


model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "sqrt",
                                  max_leaf_nodes = 30)

model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print("Random forest classifier accuracy: ",metrics.accuracy_score(y_test, prediction_test))

print(classification_report(y_test, prediction_test))

# Perform k-fold cross-validation
cv_scores = cross_val_score(model_rf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')
94/46:
# Assuming you're using a simple feedforward neural network in TensorFlow
from symbol import tfpdef
from sklearn.base import _num_features


model = tfpdef.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(_num_features,)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1)  # Output layer for regression (change for classification)
])

model.compile(optimizer='adam', loss='mean_squared_error')  # Adjust loss function for your problem

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))
94/47:
# Assuming you're using a simple feedforward neural network in TensorFlow
from symbol import vfpdef
from sklearn.base import _num_features


model = vfpdef.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(_num_features,)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1)  # Output layer for regression (change for classification)
])

model.compile(optimizer='adam', loss='mean_squared_error')  # Adjust loss function for your problem

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))
95/1:
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Initialize and train your model (for example, Linear Regression)
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on the training set
y_train_pred = model.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
95/2:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
95/3:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
95/4:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
95/5:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
# display(Name_df)
# display(Age_df)
display(Churn_df)
95/6:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
95/7:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
95/8: df.describe(include="all")
95/9: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
95/10:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
95/11:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
95/12:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
95/13:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
95/14:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
95/15:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(log, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')
95/16:
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Initialize and train your model (for example, Linear Regression)
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on the training set
y_train_pred = model.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
95/17:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)
95/18:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(knn, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')
95/19:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_knn))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_knn)
print(cnf_matrix)
95/20:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)

# Perform k-fold cross-validation
cv_scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')
95/21:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_tree))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_tree)
print(cnf_matrix)
95/22:
from sklearn.ensemble import RandomForestClassifier


model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "sqrt",
                                  max_leaf_nodes = 30)

model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print("Random forest classifier accuracy: ",metrics.accuracy_score(y_test, prediction_test))

print(classification_report(y_test, prediction_test))

# Perform k-fold cross-validation
cv_scores = cross_val_score(model_rf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')
95/23:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)

# Perform k-fold cross-validation
cv_scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
95/24:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(knn, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
95/25:
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Initialize and train your model (for example, Linear Regression)
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = model.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)


# Predict on the training set
y_train_pred = model.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
95/26:
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Initialize and train your model (for example, Linear Regression)
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = model.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)


# Predict on the training set
y_train_pred = model.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
95/27:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(log, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = log.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = log.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
95/28:
# K-Nearest Neighbors (KNN):

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(knn, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = knn.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = knn.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
95/29:
from sklearn.ensemble import RandomForestClassifier


model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "sqrt",
                                  max_leaf_nodes = 30)

model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print("Random forest classifier accuracy: ",metrics.accuracy_score(y_test, prediction_test))

print(classification_report(y_test, prediction_test))

# Perform k-fold cross-validation
cv_scores = cross_val_score(model_rf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model_rf.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model_rf.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
95/30:
from sklearn.ensemble import RandomForestClassifier


model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "sqrt",
                                  max_leaf_nodes = 30)

model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print("Random forest classifier accuracy: ",metrics.accuracy_score(y_test, prediction_test))


# Perform k-fold cross-validation
cv_scores = cross_val_score(model_rf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model_rf.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model_rf.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
95/31:
from sklearn.linear_model import Lasso

# Create a Lasso regression model with a specified alpha value
lasso = Lasso(alpha=0.1)  # You can adjust the alpha value

# Fit the model to your data
lasso.fit(X_train, y_train)  

# Make predictions
predictions = lasso.predict(X_test)  

# Evaluate the model
mse = mean_squared_error(y_test, predictions)
95/32:
from sklearn.linear_model import Lasso

# Create a Lasso regression model with a specified alpha value
lasso = Lasso(alpha=0.1)  # You can adjust the alpha value

# Fit the model to your data
lasso.fit(X_train, y_train)  

# Make predictions
predictions = lasso.predict(X_test)  

# Evaluate the model
mse = mean_squared_error(y_test, predictions) 
print(mse)
95/33:
from sklearn.linear_model import Ridge

# Create a Ridge regression model with a specified alpha value
ridge = Ridge(alpha=0.1)  # You can adjust the alpha value

# Fit the model to your data
ridge.fit(X_train, y_train)  # X_train and y_train are your training data

# Make predictions
predictions = ridge.predict(X_test)  # X_test is your test data

# Evaluate the model
mse = mean_squared_error(y_test, predictions)  # y_test is your test labels
95/34:
from sklearn.linear_model import Ridge

# Create a Ridge regression model with a specified alpha value
ridge = Ridge(alpha=0.1)  # You can adjust the alpha value

# Fit the model to your data
ridge.fit(X_train, y_train)  # X_train and y_train are your training data

# Make predictions
predictions = ridge.predict(X_test)  # X_test is your test data

# Evaluate the model
mse = mean_squared_error(y_test, predictions)  # y_test is your test labels
print(mse)
95/35:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters you want to tune
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100], 
    'penalty': ['l2']
}

# Create a logistic regression model
logistic_regression = LogisticRegression()

# Create a GridSearchCV object
grid_search = GridSearchCV(logistic_regression, param_grid, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to your data
grid_search.fit(X_train, y_train)  # X_train and y_train are your training data

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Make predictions using the best model
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test)  # X_test is your test data

# Evaluate the model
accuracy = accuracy_score(y_test, predictions)  # y_test is your test labels
print("Accuracy:", accuracy)
95/36:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(log, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = log.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = log.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")

model_score = log.score(X_test, y_test)
print(model_score)
95/37: df.Churn.value_counts()
95/38:
df.Churn.value_counts()
sns.countplot(df['Churn'])
95/39:
df.Churn.value_counts()
sns.countplot(df['Churn.value_counts()'])
95/40: df.Churn.value_counts()
95/41:
df.Churn.value_counts()
a= df[df.Churn==0]
b= df[df.Churn==1]
95/42:
df.Churn.value_counts()
a= df[df.Churn==0]
b= df[df.Churn==1]

a1=a.sample(n=480,random_state=3)
b1=b.sample(n=480,random_state=3)

print(a1.shape)
print(b1.shape)
95/43:
df.Churn.value_counts()
a= df[df.Churn==0]
b= df[df.Churn==1]

a1=a.sample(n=480,random_state=3)
b1=b.sample(n=480,random_state=3)

print(a1.shape)
print(b1.shape)

data_balanced_df = pd.DataFrame()
data_balanced_df = data_balanced_df.append(a1 )
data_balanced_df = data_balanced_df.append(b1 )

print(data_balanced_df.shape)
95/44:
df.Churn.value_counts()
a= df[df.Churn==0]
b= df[df.Churn==1]

a1=a.sample(n=480,random_state=3)
b1=b.sample(n=480,random_state=3)

print(a1.shape)
print(b1.shape)

data_balanced_df = pd.DataFrame()
data_balanced_df = pd.concat([a1, b1], axis=0)

print(data_balanced_df.shape)
95/45:
df.Churn.value_counts()
a= df[df.Churn==0]
b= df[df.Churn==1]

a1=a.sample(n=480,random_state=3)
b1=b.sample(n=480,random_state=3)

print(a1.shape)
print(b1.shape)

data_balanced_df = pd.DataFrame()
data_balanced_df = pd.concat([a1, b1], axis=0)

print(data_balanced_df.shape)

data_balanced_df.Churn.value_counts()
95/46: df.Churn.value_counts()
95/47:
df.Churn.value_counts()
a= df[df.Churn==0]
b= df[df.Churn==1]

a1=a.sample(n=50221,random_state=3)
b1=b.sample(n=50221,random_state=3)

print(a1.shape)
print(b1.shape)

data_balanced_df = pd.DataFrame()
data_balanced_df = pd.concat([a1, b1], axis=0)

print(data_balanced_df.shape)

data_balanced_df.Churn.value_counts()
95/48:
df.Churn.value_counts()
a= df[df.Churn==0]
b= df[df.Churn==1]

a1 = a.sample(n=min(50221, a.shape[0]), random_state=3)
b1 = b.sample(n=min(50221, b.shape[0]), random_state=3)

print(a1.shape)
print(b1.shape)

data_balanced_df = pd.DataFrame()
data_balanced_df = pd.concat([a1, b1], axis=0)

print(data_balanced_df.shape)

data_balanced_df.Churn.value_counts()
95/49:
X_balanced = data_balanced_df.drop(['Churn'] , axis = 1)
y_balanced = data_balanced_df['Churn']
95/50:
X_balanced = data_balanced_df.drop(['Churn'] , axis = 1)
y_balanced = data_balanced_df['Churn']

X_balanced_train, X_balanced_test, y_balanced_train, y_balanced_test = train_test_split(X_balanced, y_balanced, test_size=0.25, random_state=1)

print(X_balanced_train.shape)
print(y_balanced_train.shape)
print(X_balanced_test.shape)
print(y_balanced_test.shape)
95/51:
# Fit the model on train
model_balanced = LogisticRegression(solver="liblinear")
model_balanced.fit(X_balanced_train, y_balanced_train)

#predict on test
y_balanced_predict = model_balanced.predict(X_balanced_test)


coef_df = pd.DataFrame(model_balanced.coef_)
coef_df['intercept'] = model_balanced.intercept_
print(coef_df)
95/52:
# Fit the model on train
model_balanced = LogisticRegression(solver="liblinear")
model_balanced.fit(X_balanced_train, y_balanced_train)

#predict on test
y_balanced_predict = model_balanced.predict(X_balanced_test)


coef_df = pd.DataFrame(model_balanced.coef_)
coef_df['intercept'] = model_balanced.intercept_
print(coef_df)

model_balanced_score = model_balanced.score(X_balanced_test, y_balanced_test)
print(model_balanced_score)
95/53:
# Fit the model on train
model_balanced = LogisticRegression(solver="lbfgs")
model_balanced.fit(X_balanced_train, y_balanced_train)

#predict on test
y_balanced_predict = model_balanced.predict(X_balanced_test)


coef_df = pd.DataFrame(model_balanced.coef_)
coef_df['intercept'] = model_balanced.intercept_
print(coef_df)

model_balanced_score = model_balanced.score(X_balanced_test, y_balanced_test)
print(model_balanced_score)
95/54:
from sklearn.model_selection import GridSearchCV

logModel = LogisticRegression()
param_grid = [    
    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],
    'C' : np.logspace(-4, 4, 20),
    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],
    'max_iter' : [100, 1000,2500, 5000]
    }
]
clf = GridSearchCV(logModel, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)
best_clf = clf.fit(X,y)
best_clf.best_estimator_
95/55:
from sklearn.model_selection import GridSearchCV

logModel = LogisticRegression()
param_grid = [    
    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],
    'C' : np.logspace(-4, 4, 20),
    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],
    'max_iter' : [100, 1000,2500, 5000]
    }
]
clf = GridSearchCV(logModel, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)
best_clf = clf.fit(X,y)
best_clf.best_estimator_
print (f'Accuracy - : {best_clf.score(X,y):.3f}')
96/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
96/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
96/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
96/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
# display(Name_df)
# display(Age_df)
display(Churn_df)
96/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
96/6:
data_types = df.dtypes
display(data_types)
df['Age'] = df['Age'].astype('category')
display(df['Age'])
df['Location'] = df['Location'].astype('category')
display(df['Location'])
96/7: df.describe(include="all")
96/8: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
96/9:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
96/10:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
96/11:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
96/12:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
96/13:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
96/14:
# Model building

# Logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import classification_report

# Assuming X_train and y_train are your training data
log = LogisticRegression()
log.fit(X_train, y_train)

# Make predictions on the test dataset
y_pred_log = log.predict(X_test)
print(y_pred_log)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy:", accuracy)

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(log, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = log.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = log.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
96/15:
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define hyperparameters and their ranges
param_grid_lr = {
    'penalty' : ['l1', 'l2', 'elasticnet', 'none'],
    'C': [0.1, 1, 10, 100],
    'solver': ['lbfgs','newton-cg','liblinear','sag','saga'],
    'max_iter': [100, 1000]
}

# Create a GridSearchCV object
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to your data
grid_search_lr.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params_lr = grid_search_lr.best_params_
best_model_lr = grid_search_lr.best_estimator_
96/16:
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define hyperparameters and their ranges
param_grid_lr = {
    'penalty' : ['l1', 'l2', 'elasticnet', 'none'],
    'C': [0.1, 1, 10, 100],
    'solver': ['lbfgs','newton-cg','liblinear','sag','saga'],
    'max_iter': [1000, 10000]
}

# Create a GridSearchCV object
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to your data
grid_search_lr.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params_lr = grid_search_lr.best_params_
best_model_lr = grid_search_lr.best_estimator_
96/17:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

# Define hyperparameters and their ranges
param_grid_dt = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a GridSearchCV object
grid_search_dt = GridSearchCV(DecisionTreeClassifier(), param_grid_dt, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to your data
grid_search_dt.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params_dt = grid_search_dt.best_params_
best_model_dt = grid_search_dt.best_estimator_
96/18:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

%%time
grid_search.fit(X_train, y_train)
96/19:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)
96/20:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

score_df = pd.DataFrame(grid_search.cv_results_)
score_df.head()
96/21:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

grid_search.best_estimator_
96/22:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

grid_search.best_estimator_

dt_best = grid_search.best_estimator_

evaluate_model(dt_best)
96/23:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

grid_search.best_estimator_

dt_best = grid_search.best_estimator_
96/24:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

grid_search.best_estimator_

dt_best = grid_search.best_estimator_

best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test)  # X_test is your test data

accuracy = accuracy_score(y_test, predictions)  # y_test is your test labels
print("Accuracy:", accuracy)
96/25:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [10000,100000],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

grid_search.best_estimator_

dt_best = grid_search.best_estimator_

best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test) 

accuracy = accuracy_score(y_test, predictions)  
print("Accuracy:", accuracy)
96/26:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)
96/27:
df.Churn.value_counts()
a= df[df.Churn==0]
b= df[df.Churn==1]

a1 = a.sample(n=min(50221, a.shape[0]), random_state=3)
b1 = b.sample(n=min(50221, b.shape[0]), random_state=3)

print(a1.shape)
print(b1.shape)

data_balanced_df = pd.DataFrame()
data_balanced_df = pd.concat([a1, b1], axis=0)

print(data_balanced_df.shape)

data_balanced_df.Churn.value_counts()

X_balanced = data_balanced_df.drop(['Churn'] , axis = 1)
y_balanced = data_balanced_df['Churn']

X_balanced_train, X_balanced_test, y_balanced_train, y_balanced_test = train_test_split(X_balanced, y_balanced, test_size=0.25, random_state=1)

print(X_balanced_train.shape)
print(y_balanced_train.shape)
print(X_balanced_test.shape)
print(y_balanced_test.shape)
96/28:
# Fit the model on train
model_balanced = LogisticRegression(solver="lbfgs")
model_balanced.fit(X_balanced_train, y_balanced_train)

#predict on test
y_balanced_predict = model_balanced.predict(X_balanced_test)


coef_df = pd.DataFrame(model_balanced.coef_)
coef_df['intercept'] = model_balanced.intercept_
print(coef_df)

model_balanced_score = model_balanced.score(X_balanced_test, y_balanced_test)
print(model_balanced_score)
96/29:
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define hyperparameters and their ranges
param_grid_lr = {
    'C': [0.1, 1, 10, 100],
    'solver': ['lbfgs', 'liblinear'],
    'max_iter': [100, 1000]
}

# Create a GridSearchCV object
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to your data
grid_search_lr.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params_lr = grid_search_lr.best_params_
best_model_lr = grid_search_lr.best_estimator_
96/30:
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define hyperparameters and their ranges
param_grid_lr = {
    'C': [0.1, 1, 10, 100],
    'solver': ['lbfgs', 'liblinear'],
    'max_iter': [100, 1000]
}

# Create a GridSearchCV object
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to your data
grid_search_lr.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params_lr = grid_search_lr.best_params_
best_model_lr = grid_search_lr.best_estimator_

# Evaluate the best model on the test data
lr_accuracy = best_model_lr.score(X_test, y_test)

print(f'Logistic Regression Accuracy: {lr_accuracy}')
96/31:
# Fit the model on train
model_balanced = LogisticRegression(solver="lbfgs")
model_balanced.fit(X_balanced_train, y_balanced_train)

#predict on test
y_balanced_predict = model_balanced.predict(X_balanced_test)


coef_df = pd.DataFrame(model_balanced.coef_)
coef_df['intercept'] = model_balanced.intercept_
print(coef_df)

model_balanced_score = model_balanced.score(X_balanced_test, y_balanced_test)
print(model_balanced_score)
96/32:
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define hyperparameters and their ranges
param_grid_lr = {
    'C': [0.1, 1, 10, 100],
    'solver': ['lbfgs', 'liblinear'],
    'max_iter': [100, 1000]
}

# Create a GridSearchCV object
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to your data
grid_search_lr.fit(X_balanced_train, y_balanced_train)

# Get the best hyperparameters and model
best_params_lr = grid_search_lr.best_params_
best_model_lr = grid_search_lr.best_estimator_

# Evaluate the best model on the test data
lr_accuracy = best_model_lr.score(X_balanced_test, y_balanced_test)

print(f'Logistic Regression Accuracy: {lr_accuracy}')
96/33:
#Removing missing values 
df.dropna(inplace = True)
#Remove customer IDs from the data set
df2 = df.iloc[:,1:]
#Convertin the predictor variable in a binary numeric variable
df2['Churn'].replace(to_replace='Yes', value=1, inplace=True)
df2['Churn'].replace(to_replace='No',  value=0, inplace=True)

#Let's convert all the categorical variables into dummy variables
df_dummies = pd.get_dummies(df2)
df_dummies.head()
96/34:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
# display(Name_df)
# display(Age_df)
display(Churn_df)
96/35:
#Get Correlation of "Churn" with other variables:
plt.figure(figsize=(15,8))
df_dummies.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')
96/36:
ax = df['Subscription_Length_Months'].value_counts().plot(kind = 'bar',rot = 0, width = 0.3)
ax.set_ylabel('# of Customers')
ax.set_title('# of Customers by Subscription_Length_Months')
96/37:
colors = ['#4D3425','#E4512B']
ax = (df['Churn'].value_counts()*100.0 /len(df)).plot(kind='bar',
                                                                           stacked = True,
                                                                          rot = 0,
                                                                          color = colors,
                                                                         figsize = (8,6))
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
ax.set_ylabel('% Customers',size = 14)
ax.set_xlabel('Churn',size = 14)
ax.set_title('Churn Rate', size = 14)

# create a list to collect the plt.patches data
totals = []

# find the values and append to list
for i in ax.patches:
    totals.append(i.get_width())

# set individual bar lables using above list
total = sum(totals)
for i in ax.patches:
    # get_width pulls left or right; get_y pushes up or down
    ax.text(i.get_x()+.15, i.get_height()-4.0, \
            str(round((i.get_height()/total), 1))+'%',
            fontsize=12,
            color='white',
           weight = 'bold',
           size = 14)
96/38:
import matplotlib.ticker as mtick

colors = ['#4D3425','#E4512B']
ax = (df['Churn'].value_counts()*100.0 /len(df)).plot(kind='bar',
                                                   stacked = True,
                                                   rot = 0,
                                                   color = colors,
                                                   figsize = (8,6))
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
ax.set_ylabel('% Customers', size = 14)
ax.set_xlabel('Churn', size = 14)
ax.set_title('Churn Rate', size = 14)

# create a list to collect the plt.patches data
totals = []

# find the values and append to list
for i in ax.patches:
    totals.append(i.get_width())

# set individual bar labels using above list
total = sum(totals)
for i in ax.patches:
    # get_width pulls left or right; get_y pushes up or down
    ax.text(i.get_x() + 0.15, i.get_height() - 4.0, \
            str(round((i.get_height() / total), 1)) + '%',
            fontsize=12,
            color='white',
            weight='bold', size=14)

plt.show()
96/39:
import matplotlib.ticker as mtick

colors = ['#4D3425','#E4512B']
ax = (df['Churn'].value_counts()*100.0 /len(df)).plot(kind='bar',
                                                   stacked = True,
                                                   rot = 0,
                                                   color = colors,
                                                   figsize = (8,6))
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
ax.set_ylabel('% Customers', size = 14)
ax.set_xlabel('Churn', size = 14)
ax.set_title('Churn Rate', size = 14)

# create a list to collect the plt.patches data
totals = []

# find the values and append to list
for i in ax.patches:
    totals.append(i.get_width())

# set individual bar labels using above list
total = sum(totals)
for i in ax.patches:
    # get_width pulls left or right; get_y pushes up or down
    ax.text(i.get_x() + 0.15, i.get_height() - 4.0, \
            str(round((i.get_height() / total), 1)) + '%',
            fontsize=12,
            color='white',
            weight='bold')

plt.show()
96/40:
# Running logistic regression model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
result = model.fit(X_train, y_train)
from sklearn import metrics
prediction_test = model.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))
96/41:
# Running logistic regression model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
result = model.fit(X_train, y_train)
from sklearn import metrics
prediction_test = model.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))

weights = pd.Series(model.coef_[0],
                 index=X.columns.values)
print (weights.sort_values(ascending = False)[:10].plot(kind='bar'))
96/42:
# Running logistic regression model
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
model = LogisticRegression()
result = model.fit(X_train, y_train)
from sklearn import metrics
prediction_test = model.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(log, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = log.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = log.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
96/43:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)
96/44:
from sklearn.svm import SVC

model.svm = SVC(kernel='linear') 
model.svm.fit(X_train,y_train)
preds = model.svm.predict(X_test)
metrics.accuracy_score(y_test, preds)
97/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
97/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
97/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
97/4:
#Removing missing values 
df.dropna(inplace = True)
#Remove customer IDs from the data set
df2 = df.iloc[:,1:]
#Convertin the predictor variable in a binary numeric variable
df2['Churn'].replace(to_replace='Yes', value=1, inplace=True)
df2['Churn'].replace(to_replace='No',  value=0, inplace=True)

#Let's convert all the categorical variables into dummy variables
df_dummies = pd.get_dummies(df2)
df_dummies.head()
97/5:
# AdaBoost Algorithm
from sklearn.ensemble import AdaBoostClassifier
model = AdaBoostClassifier()
# n_estimators = 50 (default value) 
# base_estimator = DecisionTreeClassifier (default value)
model.fit(X_train,y_train)
preds = model.predict(X_test)
metrics.accuracy_score(y_test, preds)
97/6:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
97/7:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
97/8:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
97/9:
#Removing missing values 
df.dropna(inplace = True)
#Remove customer IDs from the data set
df2 = df.iloc[:,1:]
#Convertin the predictor variable in a binary numeric variable
df2['Churn'].replace(to_replace='Yes', value=1, inplace=True)
df2['Churn'].replace(to_replace='No',  value=0, inplace=True)

#Let's convert all the categorical variables into dummy variables
df_dummies = pd.get_dummies(df2)
df_dummies.head()
97/10:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [10000,100000],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

grid_search.best_estimator_

dt_best = grid_search.best_estimator_

best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test) 

accuracy = accuracy_score(y_test, predictions)  
print("Accuracy:", accuracy)
97/11:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [10000,100000],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

grid_search.best_estimator_

dt_best = grid_search.best_estimator_

best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test) 

accuracy = accuracy_score(y_test, predictions)  
print("Accuracy:", accuracy)
97/12:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
97/13:
#Removing missing values 
df.dropna(inplace = True)
#Remove customer IDs from the data set
df2 = df.iloc[:,1:]
#Convertin the predictor variable in a binary numeric variable
df2['Churn'].replace(to_replace='Yes', value=1, inplace=True)
df2['Churn'].replace(to_replace='No',  value=0, inplace=True)

#Let's convert all the categorical variables into dummy variables
df_dummies = pd.get_dummies(df2)
df_dummies.head()
97/14:
#Removing missing values 
telecom_cust.dropna(inplace = True)
#Remove customer IDs from the data set
df2 = telecom_cust.iloc[:,1:]
#Convertin the predictor variable in a binary numeric variable
df2['Churn'].replace(to_replace='Yes', value=1, inplace=True)
df2['Churn'].replace(to_replace='No',  value=0, inplace=True)

#Let's convert all the categorical variables into dummy variables
df_dummies = pd.get_dummies(df2)
df_dummies.head()
97/15:
#Removing missing values 
telecom_cust.dropna(inplace = True)
#Remove customer IDs from the data set
df2 = telecom_cust.iloc[:,1:]
#Convertin the predictor variable in a binary numeric variable
df2['Churn'].replace(to_replace='Yes', value=1, inplace=True)
df2['Churn'].replace(to_replace='No',  value=0, inplace=True)

#Let's convert all the categorical variables into dummy variables
df_dummies = pd.get_dummies(df2)
df_dummies.head()
97/16:
#Removing missing values 
df.dropna(inplace = True)
#Remove customer IDs from the data set
df2 = df.iloc[:,1:]
#Convertin the predictor variable in a binary numeric variable
df2['Churn'].replace(to_replace='Yes', value=1, inplace=True)
df2['Churn'].replace(to_replace='No',  value=0, inplace=True)

#Let's convert all the categorical variables into dummy variables
df_dummies = pd.get_dummies(df2)
df_dummies.head()
97/17:
from scipy.sparse import csr_matrix

#Removing missing values 
df.dropna(inplace = True)
#Remove customer IDs from the data set
df2 = df.iloc[:,1:]
#Convertin the predictor variable in a binary numeric variable
df2['Churn'].replace(to_replace='Yes', value=1, inplace=True)
df2['Churn'].replace(to_replace='No',  value=0, inplace=True)

#Let's convert all the categorical variables into dummy variables
df_dummies = pd.get_dummies(df2)
df_dummies.head()

# Assuming df2 is your DataFrame with categorical variables
df_dummies_sparse = pd.get_dummies(df2, sparse=True).to_coo()

# Convert to CSR matrix
df_dummies_sparse = csr_matrix(df_dummies_sparse)
97/18:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
# display(Name_df)
# display(Age_df)
display(Churn_df)
97/19:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
97/20:
#Get Correlation of "Churn" with other variables:
plt.figure(figsize=(15,8))
df_dummies.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')
97/21:
#Get Correlation of "Churn" with other variables:
plt.figure(figsize=(15,8))
df_dummies.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')
97/22: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
97/23:
ax = df['Subscription_Length_Months'].value_counts().plot(kind = 'bar',rot = 0, width = 0.3)
ax.set_ylabel('# of Customers')
ax.set_title('# of Customers by Subscription_Length_Months')
97/24:
import matplotlib.ticker as mtick

colors = ['#4D3425','#E4512B']
ax = (df['Churn'].value_counts()*100.0 /len(df)).plot(kind='bar',
                                                   stacked = True,
                                                   rot = 0,
                                                   color = colors,
                                                   figsize = (8,6))
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
ax.set_ylabel('% Customers', size = 14)
ax.set_xlabel('Churn', size = 14)
ax.set_title('Churn Rate', size = 14)

# create a list to collect the plt.patches data
totals = []

# find the values and append to list
for i in ax.patches:
    totals.append(i.get_width())

# set individual bar labels using above list
total = sum(totals)
for i in ax.patches:
    # get_width pulls left or right; get_y pushes up or down
    ax.text(i.get_x() + 0.15, i.get_height() - 4.0, \
            str(round((i.get_height() / total), 1)) + '%',
            fontsize=12,
            color='white',
            weight='bold')

plt.show()
97/25:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
97/26:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
97/27:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
97/28:
# Running logistic regression model
from asyncio import log
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
model = LogisticRegression()
result = model.fit(X_train, y_train)
from sklearn import metrics
prediction_test = model.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(log, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = log.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = log.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
97/29:
# Running logistic regression model
from asyncio import log
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error


model = LogisticRegression()
result = model.fit(X_train, y_train)
from sklearn import metrics
prediction_test = model.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(log, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = log.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = log.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
97/30:
# Running logistic regression model
from asyncio import log
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error


model = LogisticRegression()
result = model.fit(X_train, y_train)
from sklearn import metrics
prediction_test = model.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = log.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = log.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
97/31:
# Running logistic regression model
from asyncio import log
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error


model = LogisticRegression()
result = model.fit(X_train, y_train)
from sklearn import metrics
prediction_test = model.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)


print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
97/32:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_log))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_log)
print(cnf_matrix)
97/33:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, prediction_test))

cnf_matrix = metrics.confusion_matrix(y_test, prediction_test)
print(cnf_matrix)
97/34:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)

# Perform k-fold cross-validation
cv_scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = clf.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = clf.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
97/35:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)

# Perform k-fold cross-validation
cv_scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = clf.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = clf.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
97/36:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [10000,100000],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

grid_search.best_estimator_

dt_best = grid_search.best_estimator_

best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test) 

accuracy = accuracy_score(y_test, predictions)  
print("Accuracy:", accuracy)
97/37:
# Evaluation Model

from sklearn import metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_tree))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred_tree)
print(cnf_matrix)
97/38:
from sklearn.ensemble import RandomForestClassifier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)
model_rf = RandomForestClassifier(n_estimators=1000 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "auto",
                                  max_leaf_nodes = 30)
model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print (metrics.accuracy_score(y_test, prediction_test))


# Perform k-fold cross-validation
cv_scores = cross_val_score(model_rf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model_rf.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model_rf.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
97/39:
from sklearn.ensemble import RandomForestClassifier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)
model_rf = RandomForestClassifier(n_estimators=1000 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "auto",
                                  max_leaf_nodes = 30)
model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print (metrics.accuracy_score(y_test, prediction_test))


# Perform k-fold cross-validation
cv_scores = cross_val_score(model_rf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model_rf.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model_rf.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
97/40:
# AdaBoost Algorithm
from sklearn.ensemble import AdaBoostClassifier
model = AdaBoostClassifier()
# n_estimators = 50 (default value) 
# base_estimator = DecisionTreeClassifier (default value)
model.fit(X_train,y_train)
preds = model.predict(X_test)
metrics.accuracy_score(y_test, preds)
97/41:
from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(X_train, y_train)
preds = model.predict(X_test)
metrics.accuracy_score(y_test, preds)
97/42:
from sklearn.ensemble import RandomForestClassifier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)
model_rf = RandomForestClassifier(n_estimators=1000, oob_score=True, n_jobs=-1,
                                  random_state=50, max_features="sqrt",
                                  max_leaf_nodes=30)

model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print (metrics.accuracy_score(y_test, prediction_test))


# Perform k-fold cross-validation
cv_scores = cross_val_score(model_rf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model_rf.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model_rf.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
97/43:
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split


# Define base models
base_models = [
    ("logistic_regression", LogisticRegression()),
    ("decision_tree", DecisionTreeClassifier()),
    ("random_forest", RandomForestClassifier())
]

# Initialize XGBoost model
xgb_model = xgb.XGBClassifier()

# Train and boost the models
for model_name, base_model in base_models:
    base_model.fit(X_train, y_train)
    predictions = base_model.predict(X_train)  # You can use X_test for predictions on the test set
    
    # Add the predictions as a new feature to the training set
    X_train[model_name] = predictions

# Train the XGBoost model on the new features
xgb_model.fit(X_train, y_train)

# Make predictions using the boosted model
predictions = xgb_model.predict(X_test)

# Evaluate the performance of the boosted model
accuracy = (predictions == y_test).mean()
print(f"Accuracy of Boosted Model: {accuracy}")
97/44:
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pandas as pd

# Assuming you have already defined X and y

# Define base models
base_models = [
    ("logistic_regression", LogisticRegression()),
    ("decision_tree", DecisionTreeClassifier()),
    ("random_forest", RandomForestClassifier())
]

# Initialize XGBoost model
xgb_model = xgb.XGBClassifier()

# Create empty DataFrames to store base model predictions
train_preds = pd.DataFrame()
test_preds = pd.DataFrame()

# Train and boost the models
for model_name, base_model in base_models:
    base_model.fit(X_train, y_train)
    train_preds[model_name] = base_model.predict(X_train)
    test_preds[model_name] = base_model.predict(X_test)

# Concatenate base model predictions with original features
X_train_augmented = pd.concat([X_train, train_preds], axis=1)
X_test_augmented = pd.concat([X_test, test_preds], axis=1)

# Train the XGBoost model on the new features
xgb_model.fit(X_train_augmented, y_train)

# Make predictions using the boosted model
predictions = xgb_model.predict(X_test_augmented)

# Evaluate the performance of the boosted model
accuracy = (predictions == y_test).mean()
print(f"Accuracy of Boosted Model: {accuracy}")
97/45:
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pandas as pd

# Assuming you have already defined X and y

# Define base models
base_models = [
    ("logistic_regression", LogisticRegression()),
    ("decision_tree", DecisionTreeClassifier()),
    ("random_forest", RandomForestClassifier())
]

# Initialize XGBoost model
xgb_model = xgb.XGBClassifier()

# Create empty DataFrames to store base model predictions
train_preds = pd.DataFrame()
test_preds = pd.DataFrame()

# Train and boost the models
for model_name, base_model in base_models:
    base_model.fit(X_train, y_train)
    train_preds[model_name] = base_model.predict(X_train)
    test_preds[model_name] = base_model.predict(X_test)

# Concatenate base model predictions with original features
X_train_augmented = pd.concat([X_train, train_preds], axis=1)
X_test_augmented = pd.concat([X_test, test_preds], axis=1)

# Train the XGBoost model on the new features
xgb_model.fit(X_train_augmented, y_train)

# Make predictions using the boosted model
predictions = xgb_model.predict(X_test_augmented)

# Evaluate the performance of the boosted model
accuracy = (predictions == y_test).mean()
print(f"Accuracy of Boosted Model: {accuracy}")
97/46:
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Define base models
base_models = [
    ("logistic_regression", LogisticRegression()),
    ("decision_tree", DecisionTreeClassifier()),
    ("random_forest", RandomForestClassifier())
]

# Initialize XGBoost model
xgb_model = xgb.XGBClassifier()

# Create empty DataFrames to store base model predictions
train_probs = pd.DataFrame()
test_probs = pd.DataFrame()

# Train and boost the models
for model_name, base_model in base_models:
    base_model.fit(X_train, y_train)
    train_probs[model_name] = base_model.predict_proba(X_train)[:, 1]
    test_probs[model_name] = base_model.predict_proba(X_test)[:, 1]

# Train the XGBoost model on the new features
xgb_model.fit(train_probs, y_train)

# Use test set probabilities for prediction
xgb_probs = test_probs.values
predictions = (xgb_model.predict(xgb_probs) > 0.5).astype(int)

# Evaluate the performance of the boosted model
accuracy = (predictions == y_test).mean()
print(f"Accuracy of Boosted Model: {accuracy}")
97/47:
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Define base models
base_models = [
    ("logistic_regression", LogisticRegression()),
    ("decision_tree", DecisionTreeClassifier()),
    ("random_forest", RandomForestClassifier())
]

# Initialize XGBoost model
xgb_model = xgb.XGBClassifier()

# Create empty DataFrames to store base model predictions
train_probs = pd.DataFrame()
test_probs = pd.DataFrame()

# Train and boost the models
for model_name, base_model in base_models:
    base_model.fit(X_train, y_train)
    train_probs[model_name] = base_model.predict_proba(X_train)[:, 1]
    test_probs[model_name] = base_model.predict_proba(X_test)[:, 1]

# Rename the feature names in the test set
test_probs = test_probs.rename(columns={"decision_tree": "decision_tree_proba",
                                            "logistic_regression": "logistic_regression_proba",
                                            "random_forest": "random_forest_proba"})

# Train the XGBoost model on the new features
xgb_model.fit(train_probs, y_train, feature_names=["decision_tree_proba", "logistic_regression_proba", "random_forest_proba"])

# Use test set probabilities for prediction
xgb_probs = test_probs.values
predictions = (xgb_model.predict(xgb_probs) > 0.5).astype(int)

# Evaluate the performance of the boosted model
accuracy = (predictions == y_test).mean()
print(f"Accuracy of Boosted Model: {accuracy}")
97/48:
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Define base models
base_models = [
    ("logistic_regression", LogisticRegression()),
    ("decision_tree", DecisionTreeClassifier()),
    ("random_forest", RandomForestClassifier())
]

# Initialize XGBoost model
xgb_model = xgb.XGBClassifier()

# Create empty DataFrames to store base model predictions
train_probs = pd.DataFrame()
test_probs = pd.DataFrame()

# Train and boost the models
for model_name, base_model in base_models:
    base_model.fit(X_train, y_train)
    train_probs[model_name] = base_model.predict_proba(X_train)[:, 1]
    test_probs[model_name] = base_model.predict_proba(X_test)[:, 1]

# Rename the feature names in the test set
test_probs = test_probs.rename(columns={"decision_tree": "decision_tree_proba",
                                            "logistic_regression": "logistic_regression_proba",
                                            "random_forest": "random_forest_proba"})

# Pass the feature names to the XGBoost model explicitly
xgb_model.fit(train_probs, y_train, feature_names=["decision_tree_proba", "logistic_regression_proba", "random_forest_proba"])

# Use test set probabilities for prediction
xgb_probs = test_probs.values
predictions = (xgb_model.predict(xgb_probs) > 0.5).astype(int)

# Evaluate the performance of the boosted model
accuracy = (predictions == y_test).mean()
print(f"Accuracy of Boosted Model: {accuracy}")
97/49:
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 500],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.01, 0.001],
    'reg_alpha': [0.1, 1, 10],
    'reg_lambda': [0.1, 1, 10]
}

# Create the GridSearchCV object
gscv = GridSearchCV(xgb.XGBClassifier(), param_grid, cv=5)

# Train the model
gscv.fit(X_train, y_train)

# Get the best model
best_model = gscv.best_estimator_

# Evaluate the best model on the test set
preds = best_model.predict(X_test)
accuracy = metrics.accuracy_score(y_test, preds)
print(f"Accuracy of best model: {accuracy}")
98/1:
# AdaBoost Algorithm
from sklearn.ensemble import AdaBoostClassifier
model = AdaBoostClassifier()
# n_estimators = 50 (default value) 
# base_estimator = DecisionTreeClassifier (default value)
model.fit(X_train,y_train)
preds = model.predict(X_test)
metrics.accuracy_score(y_test, preds)

# Perform k-fold cross-validation
cv_scores = cross_val_score(model_rf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
98/2:
# AdaBoost Algorithm
from sklearn.ensemble import AdaBoostClassifier
model = AdaBoostClassifier()
# n_estimators = 50 (default value) 
# base_estimator = DecisionTreeClassifier (default value)
model.fit(X_train,y_train)
preds = model.predict(X_test)
metrics.accuracy_score(y_test, preds)

# Perform k-fold cross-validation
cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
98/3:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
98/4:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
98/5:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
98/6:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
# display(Name_df)
# display(Age_df)
display(Churn_df)
98/7:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
98/8: df.describe(include="all")
98/9: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
98/10:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
98/11:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
98/12:
ax = df['Subscription_Length_Months'].value_counts().plot(kind = 'bar',rot = 0, width = 0.3)
ax.set_ylabel('# of Customers')
ax.set_title('# of Customers by Subscription_Length_Months')
98/13:
import matplotlib.ticker as mtick

colors = ['#4D3425','#E4512B']
ax = (df['Churn'].value_counts()*100.0 /len(df)).plot(kind='bar',
                                                   stacked = True,
                                                   rot = 0,
                                                   color = colors,
                                                   figsize = (8,6))
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
ax.set_ylabel('% Customers', size = 14)
ax.set_xlabel('Churn', size = 14)
ax.set_title('Churn Rate', size = 14)

# create a list to collect the plt.patches data
totals = []

# find the values and append to list
for i in ax.patches:
    totals.append(i.get_width())

# set individual bar labels using above list
total = sum(totals)
for i in ax.patches:
    # get_width pulls left or right; get_y pushes up or down
    ax.text(i.get_x() + 0.15, i.get_height() - 4.0, \
            str(round((i.get_height() / total), 1)) + '%',
            fontsize=12,
            color='white',
            weight='bold')

plt.show()
98/14:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
98/15:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
98/16:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
98/17:
# Running logistic regression model
from asyncio import log
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error


model = LogisticRegression()
result = model.fit(X_train, y_train)
from sklearn import metrics
prediction_test = model.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)


print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
98/18:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred_tree = clf.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)

# Perform k-fold cross-validation
cv_scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = clf.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = clf.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
98/19:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [10000,100000],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

grid_search.best_estimator_

dt_best = grid_search.best_estimator_

best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test) 

accuracy = accuracy_score(y_test, predictions)  
print("Accuracy:", accuracy)
98/20:
from sklearn.ensemble import RandomForestClassifier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)
model_rf = RandomForestClassifier(n_estimators=1000, oob_score=True, n_jobs=-1,
                                  random_state=50, max_features="sqrt",
                                  max_leaf_nodes=30)

model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print (metrics.accuracy_score(y_test, prediction_test))


# Perform k-fold cross-validation
cv_scores = cross_val_score(model_rf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model_rf.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model_rf.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
98/21:
# AdaBoost Algorithm
from sklearn.ensemble import AdaBoostClassifier
model_ADB = AdaBoostClassifier()
# n_estimators = 50 (default value) 
# base_estimator = DecisionTreeClassifier (default value)
model_ADB.fit(X_train,y_train)
preds = model_ADB.predict(X_test)
metrics.accuracy_score(y_test, preds)

# Perform k-fold cross-validation
cv_scores = cross_val_score(model_ADB, X, y, cv=kf, scoring='accuracy')  

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model_ADB.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model_ADB.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
98/22:
from xgboost import XGBClassifier
model_XGB = XGBClassifier()
model_XGB.fit(X_train, y_train)
preds = model_XGB.predict(X_test)
metrics.accuracy_score(y_test, preds)

# Perform k-fold cross-validation
cv_scores = cross_val_score(model_XGB, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = model_XGB.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = model_XGB.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
98/23:
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 500],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.01, 0.001],
    'reg_alpha': [0.1, 1, 10],
    'reg_lambda': [0.1, 1, 10]
}

# Create the GridSearchCV object
gscv = GridSearchCV(xgb.XGBClassifier(), param_grid, cv=5)

# Train the model
gscv.fit(X_train, y_train)

# Get the best model
best_model = gscv.best_estimator_

# Evaluate the best model on the test set
preds = best_model.predict(X_test)
accuracy = metrics.accuracy_score(y_test, preds)
print(f"Accuracy of best model: {accuracy}")
98/24:
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Create a list of models
models = []
models.append(xgb.XGBClassifier())
models.append(LogisticRegression())
models.append(DecisionTreeClassifier())
models.append(RandomForestClassifier())

# Create the VotingClassifier object
ensemble = VotingClassifier(estimators=models)

# Evaluate the ensemble on the test set
preds = ensemble.predict(X_test)
accuracy = metrics.accuracy_score(y_test, preds)
print(f"Accuracy of ensemble: {accuracy}")
98/25:
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Create a list of models
models = []
models.append(xgb.XGBClassifier())
models.append(LogisticRegression())
models.append(DecisionTreeClassifier())
models.append(RandomForestClassifier())

# Create the VotingClassifier object
ensemble = VotingClassifier(estimators=models)
ensemble.fit(X_train, y_train)



# Evaluate the ensemble on the test set
preds = ensemble.predict(X_test)
accuracy = metrics.accuracy_score(y_test, preds)
print(f"Accuracy of ensemble: {accuracy}")
98/26:
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Create a list of estimators
models = [xgb.XGBClassifier()]
models.append(LogisticRegression())
models.append(DecisionTreeClassifier())
models.append(RandomForestClassifier())

# Create a list of weights
weights = [0.25, 0.25, 0.25, 0.25]

# Create the VotingClassifier object
ensemble = VotingClassifier(estimators=models, weights=weights)

# Fit the model
ensemble.fit(X_train, y_train)

# Evaluate the ensemble on the test set
preds = ensemble.predict(X_test)
accuracy = metrics.accuracy_score(y_test, preds)
print(f"Accuracy of ensemble: {accuracy}")
98/27:
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Create a list of estimators
models = [xgb.XGBClassifier()]
models.append(LogisticRegression())
models.append(DecisionTreeClassifier())
models.append(RandomForestClassifier())

# Create a list of weights
weights = [0.25, 0.25, 0.25, 0.25]

# Create the VotingClassifier object
ensemble = VotingClassifier(estimators=models, weights=weights)

# Fit the model
ensemble.fit(X_train, y_train)

# Evaluate the ensemble on the test set
preds = ensemble.predict(X_test)
accuracy = metrics.accuracy_score(y_test, preds)
print(f"Accuracy of ensemble: {accuracy}")
98/28:
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Create a list of estimators
models = [xgb.XGBClassifier()]
models.append(LogisticRegression())
models.append(DecisionTreeClassifier())
models.append(RandomForestClassifier())

# Create a list of weights
weights = [0.25, 0.25, 0.25, 0.25]

# Create the VotingClassifier object
ensemble = VotingClassifier(estimators=models, weights=weights)

# Fit the model
ensemble.fit(X_train, y_train)

# Evaluate the ensemble on the test set
preds = ensemble.predict(X_test)
accuracy = metrics.accuracy_score(y_test, preds)
print(f"Accuracy of ensemble: {accuracy}")
98/29:
import xgboost as xgb

# Create a single XGBoost classifier object
xgb_clf = xgb.XGBClassifier()

# Wrap the XGBoost classifier object in a list
xgb_clf_list = [xgb_clf]

# Iterate over the list of XGBoost classifier objects
for clf in xgb_clf_list:
  # Do something with the classifier object
  clf.fit(X_train, y_train)
  clf.predict(X_test)
98/30:
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Create a list of estimators
models = [xgb.XGBClassifier()]
models.append(LogisticRegression())
models.append(DecisionTreeClassifier())
models.append(RandomForestClassifier())

# Create a list of weights
weights = [0.25, 0.25, 0.25, 0.25]

# Create the VotingClassifier object
ensemble = VotingClassifier(estimators=models, weights=weights)

# Fit the model
ensemble.fit(X_train, y_train)

# Evaluate the ensemble on the test set
preds = ensemble.predict(X_test)
accuracy = metrics.accuracy_score(y_test, preds)
print(f"Accuracy of ensemble: {accuracy}")
98/31:
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Create a list of estimators
models = [xgb.XGBClassifier()]
models.append(LogisticRegression())
models.append(DecisionTreeClassifier())
models.append(RandomForestClassifier())

# Create a list of weights
weights = [0.25, 0.25, 0.25, 0.25]

# Create the VotingClassifier object
ensemble = VotingClassifier(estimators=models, weights=weights)

# Fit the model
ensemble.fit(X_train, y_train)

# Evaluate the ensemble on the test set
preds = ensemble.predict(X_test)
accuracy = metrics.accuracy_score(y_test, preds)
print(f"Accuracy of ensemble: {accuracy}")
98/32:
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Create a list of estimators
models = [(xgb.XGBClassifier(), "XGBoost")]
models.append((LogisticRegression(), "Logistic Regression"))
models.append((DecisionTreeClassifier(), "Decision Tree"))
models.append((RandomForestClassifier(), "Random Forest"))

# Create a list of weights
weights = [0.25, 0.25, 0.25, 0.25]

# Create the VotingClassifier object
ensemble = VotingClassifier(estimators=models, weights=weights)

# Fit the model
ensemble.fit(X_train, y_train)

# Evaluate the ensemble on the test set
preds = ensemble.predict(X_test)
accuracy = metrics.accuracy_score(y_test, preds)
print(f"Accuracy of ensemble {accuracy}")
98/33:
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import StackingClassifier

base_models = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),
    ('knn', KNeighborsClassifier())
]

# Step 2: Define the meta-model
meta_model = LogisticRegression()

# Step 3: Create the StackingClassifier
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)

# Step 4: Train the StackingClassifier
stacking_model.fit(X_train, y_train)

y_pred = stacking_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
98/34:
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import StackingClassifier

base_models = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),
    ('knn', KNeighborsClassifier())
]

# Step 2: Define the meta-model
meta_model = LogisticRegression()

# Step 3: Create the StackingClassifier
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)

# Step 4: Train the StackingClassifier
stacking_model.fit(X_train, y_train)

y_pred = stacking_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Perform k-fold cross-validation
cv_scores = cross_val_score(stacking_model, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = stacking_model.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = stacking_model.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
98/35:
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Assuming you have your data in X_train, y_train for training and X_test, y_test for testing

# Step 1: Define the base models
base_models = [
    ('lr', LogisticRegression(max_iter=1000)),
    ('dt', DecisionTreeClassifier()),
    ('rf', RandomForestClassifier(n_estimators=100))
]

# Step 2: Define the meta-model (Gradient Boosting)
meta_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Step 3: Create the StackingClassifier
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)

# Step 4: Train the StackingClassifier
stacking_model.fit(X_train, y_train)

# Step 5: Evaluate the model
y_pred = stacking_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
98/36:
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

# Define hyperparameters and their ranges
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [10000,100000],
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

grid_search.best_estimator_

dt_best = grid_search.best_estimator_

best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test) 

accuracy = accuracy_score(y_test, predictions)  
print("Accuracy:", accuracy)
98/37:
# Create Decision Tree classifier object

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

dt = DecisionTreeClassifier()
clf = dt.fit(X_train,y_train)
y_pred_tree = dt.predict(X_test)
print(y_pred_tree)
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)

# Perform k-fold cross-validation
cv_scores = cross_val_score(dt, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = dt.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = dt.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
98/38:
from sklearn.ensemble import RandomForestClassifier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)
rf = RandomForestClassifier(n_estimators=1000, oob_score=True, n_jobs=-1,
                                  random_state=50, max_features="sqrt",
                                  max_leaf_nodes=30)

rf.fit(X_train, y_train)

# Make predictions
prediction_test = rf.predict(X_test)
print (metrics.accuracy_score(y_test, prediction_test))


# Perform k-fold cross-validation
cv_scores = cross_val_score(rf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = rf.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = rf.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
98/39:
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Step 1: Define the base models
base_models = [
    ('lr', LogisticRegression(max_iter=1000)),
    ('dt', DecisionTreeClassifier()),
    ('rf', RandomForestClassifier(n_estimators=100))
]

# Step 2: Define the meta-model (Gradient Boosting)
meta_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Step 3: Create the StackingClassifier
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)

# Step 4: Train the StackingClassifier
stacking_model.fit(X_train, y_train)

# Step 5: Evaluate the model
y_pred = stacking_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
99/1:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
99/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
99/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
99/4:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
99/5:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
99/6:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique()
99/7:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
99/8:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
99/9:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
99/10:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Flatten the y_pred array
y_pred_flat = np.flatten(y_pred)

# Calculate the predicted strength of drug interaction for each combination
predicted_drug_interaction_strength = pd.Series(y_pred_flat, index=y_test.index)
99/11:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy.ndarray as nd

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Flatten the y_pred array
y_pred_flat = nd.flatten(y_pred)

# Calculate the predicted strength of drug interaction for each combination
predicted_drug_interaction_strength = pd.Series(y_pred_flat, index=y_test.index)
99/12:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy.ndarray as nd

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)
99/13:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as nd

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)
99/14:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Make predictions for Decision Tree
dt_y_pred = model.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)
99/15:
# Model building
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

lr = LogisticRegression()
lr.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = lr.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Make predictions for Decision Tree
lr_y_pred = lr.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, lr_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
lr_r2 = r2_score(y_test, lr_y_pred)
print("Decision Tree R-squared (R2) Score:", lr_r2)
99/16:
# Model building
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

lr = LogisticRegression()
lr.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = lr.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Make predictions for Decision Tree
lr_y_pred = lr.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, lr_y_pred)
print("LogisticRegression:", dt_mae)

# Calculate R-squared (R2) score
lr_r2 = r2_score(y_test, lr_y_pred)
print("LogisticRegression:", lr_r2)
98/40:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Create and train the model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate on training set
train_preds = model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_preds)

# Evaluate on testing set
test_preds = model.predict(X_test)
test_accuracy = accuracy_score(y_test, test_preds)

print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")
98/41:
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)
)

train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)

plt.plot(train_sizes, train_scores_mean, label='Training Accuracy')
plt.plot(train_sizes, test_scores_mean, label='Testing Accuracy')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
99/17:
import numpy as np
from sklearn.linear_model import LogisticRegression

# Flatten the y_train array
y_train_flat = np.ravel(y_train)

# Train the LogisticRegression model
lr = LogisticRegression()
lr.fit(X_train, y_train_flat)
99/18:
import numpy as np
from sklearn.linear_model import LogisticRegression

# Flatten the y_train array
y_train_flat = np.ravel(y_train)

# Train the LogisticRegression model
lr = LogisticRegression()
lr.fit(X_train, y_train_flat)
99/19:
import numpy as np
from sklearn.linear_model import LogisticRegression

# Flatten the y_train array
y_train_flat = np.ravel(y_train)

# Train the LogisticRegression model
lr = LogisticRegression()
lr.fit(X_train, y_train_flat)
98/42:
feature_importance = model.feature_importances_
# Assign the feature names to the importance values
feature_names = X.columns
feature_importance_dict = dict(zip(feature_names, feature_importance))
98/43:
feature_importance = model.feature_importances_
# Assign the feature names to the importance values
feature_names = df.columns
feature_importance_dict = dict(zip(feature_names, feature_importance))
98/44:
feature_importance = model.feature_importances_
# Assign the feature names to the importance values
feature_names = df.columns
feature_importance_dict = dict(zip(feature_names, feature_importance))
print(feature_importance_dict)
98/45:
from pyexpat import model


feature_importance = model.feature_importances_
# Assign the feature names to the importance values
feature_names = df.columns
feature_importance_dict = dict(zip(feature_names, feature_importance))
print(feature_importance_dict)
98/46:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Create and train the model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate on training set
train_preds = model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_preds)

# Evaluate on testing set
test_preds = model.predict(X_test)
test_accuracy = accuracy_score(y_test, test_preds)

print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

feature_importance = model.feature_importances_
# Assign the feature names to the importance values
feature_names = df.columns
feature_importance_dict = dict(zip(feature_names, feature_importance))
print(feature_importance_dict)
98/47:
# Splitting train and test data

X = df[['CustomerID', 'Name', 'Subscription_Length_Months']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
98/48:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
98/49:
# Running logistic regression model
from asyncio import log
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error


lr = LogisticRegression()
result = lr.fit(X_train, y_train)
from sklearn import metrics
prediction_test = lr.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(lr, X, y, cv=kf, scoring='accuracy')

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = lr.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = lr.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)


print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
98/50:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Create and train the model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate on training set
train_preds = model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_preds)

# Evaluate on testing set
test_preds = model.predict(X_test)
test_accuracy = accuracy_score(y_test, test_preds)

print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

feature_importance = model.feature_importances_
# Assign the feature names to the importance values
feature_names = df.columns
feature_importance_dict = dict(zip(feature_names, feature_importance))
print(feature_importance_dict)
98/51:
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Assuming you have a DataFrame 'df' with features and target variable 'churn'

# Step 1: Prepare the data
# Assuming 'df' has features (X) and target variable 'churn' (y)
X = df[['CustomerID', 'Name', 'Subscription_Length_Months']]
y = df['churn']

# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Create and train the Decision Tree model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

# Step 5: Evaluate the model
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)

print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")
98/52:
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Assuming you have a DataFrame 'df' with features and target variable 'churn'

# Step 1: Prepare the data
# Assuming 'df' has features (X) and target variable 'churn' (y)
X = df[['CustomerID', 'Name', 'Subscription_Length_Months']]
y = df['Churn']

# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Create and train the Decision Tree model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

# Step 5: Evaluate the model
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)

print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")
98/53:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Assuming you have X_train, y_train defined from previous code

# Define the hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the Grid Search object
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,  # Number of cross-validation folds
    scoring='accuracy',  # Evaluation metric
    verbose=1  # Output progress
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_test = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Testing Accuracy with Best Model: {test_accuracy}")
99/20:
from sklearn.linear_model import LogisticRegression
from sklearn.utils.validation import check_X_y

# Check the consistency of the training features and target variable
X_train, y_train = check_X_y(X_train, y_train)

# Train the LogisticRegression model
lr = LogisticRegression()
lr.fit(X_train, y_train)
98/54:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Assuming you have X_train, y_train defined from previous code

# Define the hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the Grid Search object
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,  # Number of cross-validation folds
    scoring='accuracy',  # Evaluation metric
    verbose=1  # Output progress
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Testing Accuracy with Best Model: {test_accuracy}")
99/21:
# Predict antagonism, synergism, and agonism of the 10 drugs
y_pred_10 = dt.predict(df[df["drug_name"].isin(["drug_name_1", "drug_name_2", ..., "drug_name_10"])])

# Print the results
print("Predicted antagonism, synergism, and agonism of the 10 drugs:")
for i in range(len(y_pred_10)):
    if y_pred_10[i] == 1:
        print("Antagonism")
    elif y_pred_10[i] == 2:
        print("Synergism")
    elif y_pred_10[i] == 3:
        print("Agonism")
99/22:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = dt.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Make predictions for Decision Tree
dt_y_pred = dt.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)

# Predict antagonism, synergism, and agonism of the 10 drugs
y_pred_10 = dt.predict(df[df["drug_name"].isin(["drug_name_1", "drug_name_2", ..., "drug_name_10"])])

# Print the results
print("Predicted antagonism, synergism, and agonism of the 10 drugs:")
for i in range(len(y_pred_10)):
    if y_pred_10[i] == 1:
        print("Antagonism")
    elif y_pred_10[i] == 2:
        print("Synergism")
    elif y_pred_10[i] == 3:
        print("Agonism")
99/23:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = dt.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Make predictions for Decision Tree
dt_y_pred = dt.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)

# Predict antagonism, synergism, and agonism of the 10 drugs
y_pred_10 = dt.predict(df[df["Drug Name"].isin(["drug_name_1", "drug_name_2", ..., "drug_name_10"])])

# Print the results
print("Predicted antagonism, synergism, and agonism of the 10 drugs:")
for i in range(len(y_pred_10)):
    if y_pred_10[i] == 1:
        print("Antagonism")
    elif y_pred_10[i] == 2:
        print("Synergism")
    elif y_pred_10[i] == 3:
        print("Agonism")
98/55:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Assuming you have X_train, y_train defined from previous code

# Define the hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the Grid Search object
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,  # Number of cross-validation folds
    scoring='accuracy',  # Evaluation metric
    verbose=1  # Output progress
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)

print(f"Testing Accuracy with Best Model: {test_accuracy}")
print(f"Testing Accuracy with Best Model: {test_accuracy}")
100/1:
import pandas as pd

# Load the drug dataset
df = pd.read_csv("drug_dataset.csv")
100/2:
import pandas as pd

# Load the drug dataset
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
100/3:
# Remove any unnecessary columns
df.drop(["Drug ID"], axis=1, inplace=True)

# Convert categorical features to numerical features
df_encoded = df.copy()
for col in df.columns:
    if pd.api.types.is_string_dtype(df[col]):
        df_encoded[col] = pd.factor_encode(df_encoded[col])

# Scale the numerical features
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_encoded)
100/4:
import pandas as pd

# Load the drug dataset
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
print(df)
100/5:
# Remove any unnecessary columns
df.drop(["Probability*"], axis=1, inplace=True)

# Convert categorical features to numerical features
df_encoded = df.copy()
for col in df.columns:
    if pd.api.types.is_string_dtype(df[col]):
        df_encoded[col] = pd.factor_encode(df_encoded[col])

# Scale the numerical features
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_encoded)
100/6:
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Load the data
df = pd.read_csv("data.csv")

# Create a OneHotEncoder object
encoder = OneHotEncoder(sparse=False)

# Fit the encoder to the data
encoder.fit(df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']])

# Transform the data
df_encoded = encoder.transform(df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']])

# Add the encoded features to the DataFrame
df_encoded = pd.DataFrame(df_encoded, columns=encoder.get_feature_names())
df = df.join(df_encoded)

# Drop the original categorical features
df.drop(['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure'], axis=1, inplace=True)
100/7:
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Create a OneHotEncoder object
encoder = OneHotEncoder(sparse=False)

# Fit the encoder to the data
encoder.fit(df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']])

# Transform the data
df_encoded = encoder.transform(df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']])

# Add the encoded features to the DataFrame
df_encoded = pd.DataFrame(df_encoded, columns=encoder.get_feature_names())
df = df.join(df_encoded)

# Drop the original categorical features
df.drop(['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure'], axis=1, inplace=True)
100/8:
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Create a OneHotEncoder object
encoder = OneHotEncoder(sparse=False)

# Fit the encoder to the data
encoder.fit(df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']])

# Transform the data
df_encoded = encoder.transform(df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']])

# Create a list of the encoded feature names
encoded_feature_names = []
for i in range(encoder.n_values_):
    encoded_feature_names.append(encoder.category_encodings_[i][0])

# Add the encoded features to the DataFrame
df_encoded = pd.DataFrame(df_encoded, columns=encoded_feature_names)
df = df.join(df_encoded)

# Drop the original categorical features
df.drop(['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure'], axis=1, inplace=True)
100/9:
# Create a list of the encoded feature names
from json import encoder


encoded_feature_names = []
for i in range(encoder.n_values_):
    encoded_feature_names.append(encoder.category_encodings_[i][0])

# Add the encoded features to the DataFrame
df_encoded = pd.DataFrame(df_encoded, columns=encoded_feature_names)
100/10:
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Create a OneHotEncoder object
encoder = OneHotEncoder(sparse=False)

# Fit the encoder to the data
encoder.fit(df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']])

# Transform the data
df_encoded = encoder.transform(df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']])

# Create a list of the encoded feature names
encoded_feature_names = []
for i in range(encoder.n_values_):
    encoded_feature_names.append(encoder.category_encodings_[i][0])

# Add the encoded features to the DataFrame
df_encoded = pd.DataFrame(df_encoded, columns=encoded_feature_names)
df = df.join(df_encoded)

# Drop the original categorical features
df.drop(['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure'], axis=1, inplace=True)
100/11:
# Get the number of unique values for each categorical feature
n_values = []
for col in df.columns:
    if pd.api.types.is_string_dtype(df[col]):
        n_values.append(len(df[col].unique()))

# Create a list of the encoded feature names
encoded_feature_names = []
for i in range(len(n_values)):
    for j in range(n_values[i]):
        encoded_feature_names.append(col + "_" + str(j))

# Add the encoded features to the DataFrame
df_encoded = pd.DataFrame(df_encoded, columns=encoded_feature_names)
100/12:
# Get the number of unique values for each categorical feature
n_values = df['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Chemical structure']
for col in df.columns:
    if pd.api.types.is_string_dtype(df[col]):
        n_values.append(len(df[col].unique()))

# Create a list of the encoded feature names
encoded_feature_names = []
for i in range(len(n_values)):
    for j in range(n_values[i]):
        encoded_feature_names.append(col + "_" + str(j))

# Add the encoded features to the DataFrame
df_encoded = pd.DataFrame(df_encoded, columns=encoded_feature_names)
100/13:
# Get the number of unique values for each categorical feature
n_values = df[]
for col in df.columns:
    if pd.api.types.is_string_dtype(df[col]):
        n_values.append(len(df[col].unique()))

# Create a list of the encoded feature names
encoded_feature_names = []
for i in range(len(n_values)):
    for j in range(n_values[i]):
        encoded_feature_names.append(col + "_" + str(j))

# Add the encoded features to the DataFrame
df_encoded = pd.DataFrame(df_encoded, columns=encoded_feature_names)
100/14:
# Get the number of unique values for each categorical feature
n_values = []
for col in df.columns:
    if pd.api.types.is_string_dtype(df[col]):
        n_values.append(len(df[col].unique()))

# Create a list of the encoded feature names
encoded_feature_names = []
for i in range(len(n_values)):
    for j in range(n_values[i]):
        encoded_feature_names.append(col + "_" + str(j))

# Add the encoded features to the DataFrame
df_encoded = pd.DataFrame(df_encoded, columns=encoded_feature_names)
100/15:
# Get the number of unique values for each categorical feature
n_values = []
for col in df.columns:
    if pd.api.types.is_string_dtype(df[col]):
        n_values.append(len(df[col].unique()))

# Create a list of the encoded feature names
encoded_feature_names = []
for i in range(len(n_values)):
    for j in range(n_values[i]):
        encoded_feature_names.append(col + "_" + str(j))

# Add the encoded features to the DataFrame
df_encoded = pd.DataFrame(df_encoded, columns=encoded_feature_names)
print(df_encoded)
100/16:
import mlxtend.frequent_patterns as apriori

# Generate association rules
association_rules = apriori(df_encoded, min_support=0.2, min_confidence=0.5)
100/17:
import mlxtend.frequent_patterns as apriori

# Generate association rules
association_rules = apriori(df_encoded, min_support=0.2, min_confidence=0.5)

# Print the association rules
print(association_rules)
100/18:
import mlxtend.frequent_patterns as apriori

# Generate association rules
association_rules = apriori(df_encoded, min_support=0.2, min_confidence=0.5)

# Print the association rules
print(association_rules)
100/19:
from json import encoder
import pandas as pd
from mlxtend.frequent_patterns import apriori

# Encode the categorical features
df_encoded = encoder.transform(df)

# Generate association rules
association_rules = apriori(df_encoded, min_support=0.2, min_confidence=0.5)

# Print the association rules
print(association_rules)
100/20:
from json import encoder
import pandas as pd
from mlxtend.frequent_patterns import apriori

# Encode the categorical features
df_encoded = encoder.fit_transform(df)

# Generate association rules
association_rules = apriori(df_encoded, min_support=0.2, min_confidence=0.5)

# Print the association rules
print(association_rules)
100/21:
from json import encoder
import pandas as pd
from mlxtend.frequent_patterns import apriori

# Encode the categorical features
df_encoded = encoder.fit_transform(df)

# Generate association rules
association_rules = apriori(df_encoded, min_support=0.2, min_confidence=0.5)

# Print the association rules
print(association_rules)
100/22:
from json import encoder
import pandas as pd
from mlxtend.frequent_patterns import apriori

# Encode the categorical features
df_encoded = encoder.fit_transform(df)

# Generate association rules
association_rules = apriori(df_encoded, min_support=0.2, min_confidence=0.5)

# Print the association rules
print(association_rules)
100/23:
import mlxtend.frequent_patterns as apriori

# Generate association rules
association_rules = apriori(df_encoded, min_support=0.2, min_confidence=0.5)

# Print the association rules
print(association_rules)
100/24:
import mlxtend.frequent_patterns as apriori

# Generate association rules
association_rules = apriori(df_encoded, min_support=0.2, min_confidence=0.5)

# Print the association rules
print(association_rules)
99/24:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = dt.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Make predictions for Decision Tree
dt_y_pred = dt.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)

# Predict the drug combination effect of Eplerenone and Amlodipine

new_drug_combination = ["Eplerenone", "Amlodipine"]

# Encode the new drug combination
new_drug_combination_encoded = pd.factor_encode(new_drug_combination)

# Make a prediction
y_pred = lr.predict(new_drug_combination_encoded)

# If the prediction is 1, then the drug combination is likely to be synergistic.
if y_pred == 1:
    print("The drug combination is likely to be synergistic.")
else:
    print("The drug combination is not likely to be synergistic.")
99/25:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = dt.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Make predictions for Decision Tree
dt_y_pred = dt.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)
99/26:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = dt.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Make predictions for Decision Tree
dt_y_pred = dt.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)
99/27:
# Model building
import pandas as pd
from functools import lru_cache
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

dt = Decilru_cacheonTreeRegressor()
dt.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = dt.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Make predictions for Decision Tree
dt_y_pred = dt.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)

new_drug_combination = ["Eplerenone", "Amlodipine"]
new_drug_combination_encoded = encoder.transform(new_drug_combination)

# Make a prediction
y_pred = dt.predict(new_drug_combination_encoded)

# If the prediction is 1, then the drug combination is likely to be synergistic.

if y_pred == 1:
    print("The drug combination is likely to be synergistic.")
else:
    print("The drug combination is not likely to be synergistic.")
99/28:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
99/29:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
99/30:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
99/31:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
99/32:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
99/33:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique()
99/34:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
99/35:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
99/36:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
99/37:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

dt = Decilru_cacheonTreeRegressor()
dt.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = dt.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Make predictions for Decision Tree
dt_y_pred = dt.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)

new_drug_combination = ["Eplerenone", "Amlodipine"]
new_drug_combination_encoded = encoder.transform(new_drug_combination)

# Make a prediction
y_pred = dt.predict(new_drug_combination_encoded)

# If the prediction is 1, then the drug combination is likely to be synergistic.

if y_pred == 1:
    print("The drug combination is likely to be synergistic.")
else:
    print("The drug combination is not likely to be synergistic.")
99/38:
# Model building
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

dt = Decilru_cacheonTreeRegressor()
dt.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = dt.predict(X_test)

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Decision tree Mean Squared Error:", mse)

# Make predictions for Decision Tree
dt_y_pred = dt.predict(X_test)

# Calculate Mean Absolute Error (MAE)
dt_mae = mean_absolute_error(y_test, dt_y_pred)
print("Decision Tree Mean Absolute Error:", dt_mae)

# Calculate R-squared (R2) score
dt_r2 = r2_score(y_test, dt_y_pred)
print("Decision Tree R-squared (R2) Score:", dt_r2)

new_drug_combination = ["Eplerenone", "Amlodipine"]
new_drug_combination_encoded = encoder.transform(new_drug_combination)

# Make a prediction
y_pred = dt.predict(new_drug_combination_encoded)

# If the prediction is 1, then the drug combination is likely to be synergistic.

if y_pred == 1:
    print("The drug combination is likely to be synergistic.")
else:
    print("The drug combination is not likely to be synergistic.")
99/39:
dt = DecisionTreeRegressor()
new_drug_combination = ["Eplerenone", "Amlodipine"]
new_drug_combination_encoded = encoder.transform(new_drug_combination)

# Make a prediction
y_pred = dt.predict(new_drug_combination_encoded)

# If the prediction is 1, then the drug combination is likely to be synergistic.
if y_pred == 1:
    print("The drug combination is likely to be synergistic.")
else:
    print("The drug combination is not likely to be synergistic.")
101/1:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
101/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
101/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
101/4:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
101/5:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
101/6:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique()
101/7:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
101/8:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
101/9:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
101/10:


dt = DecisionTreeRegressor()
new_drug_combination = ["Eplerenone", "Amlodipine"]
new_drug_combination_encoded = encoder.transform(new_drug_combination)

# Make a prediction
y_pred = dt.predict(new_drug_combination_encoded)

# If the prediction is 1, then the drug combination is likely to be synergistic.
if y_pred == 1:
    print("The drug combination is likely to be synergistic.")
else:
    print("The drug combination is not likely to be synergistic.")
101/11:
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score

# Assuming X_train and y_train are already defined from previous steps

# Step 3: Model Selection
model = DecisionTreeRegressor()  # Using Decision Tree Regressor

# Step 4: Model Training with Cross-Validation
num_folds = 5  # Number of cross-validation folds

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='neg_mean_squared_error')

# Convert negative scores to positive (since sklearn returns negated values for regression)
cv_scores = -cv_scores

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')

# Calculate the mean and standard deviation of the scores
mean_cv_score = cv_scores.mean()
std_cv_score = cv_scores.std()

print(f'Mean CV Score (MSE): {mean_cv_score}')
print(f'Standard Deviation of CV Scores: {std_cv_score}')
101/12:
from sklearn.model_selection import GridSearchCV

# Define hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a grid search object
grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f'Best Hyperparameters: {best_params}')

# Get the best model
best_model = grid_search.best_estimator_
101/13:
from sklearn.model_selection import GridSearchCV

# Define hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a grid search object
grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f'Best Hyperparameters: {best_params}')

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions on the test set
y_pred = best_model.predict(X_test)

# Calculate the mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error (MSE) on Test Set: {mse}')
101/14:
from sklearn.model_selection import GridSearchCV

# Define hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a grid search object
grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f'Best Hyperparameters: {best_params}')

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions on the test set
y_pred = best_model.predict(X_test)

# Calculate the mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error (MSE) on Test Set: {mse}')

new_drugs_features = preprocess_and_extract_features(new_drugs)
101/15:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
print(df['Drug Name'].unique() )

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique()
101/16:
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Define your data
data = {
    'Drug_Name': [
        'Eplerenone', 'Spironolactone', 'Lisinopril', 'Enalapril', 'Benazepril',
        'Doxazosin', 'Prazosin', 'Terazosin', 'Amlodipine', 'Nifedipine', 
        'Hydralazine', 'Metoprolol', 'Verapamil', 'Atenolol'
    ],
    'Drug_Class': [
        'Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5',
        'Class_6', 'Class_7', 'Class_8', 'Class_9', 'Class_10', 
        'Class_11', 'Class_12', 'Class_13', 'Class_14'
    ],
    'Target_Class': [
        'Target_Class_A', 'Target_Class_B', 'Target_Class_C', 'Target_Class_D', 'Target_Class_E',
        'Target_Class_F', 'Target_Class_G', 'Target_Class_H', 'Target_Class_I', 'Target_Class_J', 
        'Target_Class_K', 'Target_Class_L', 'Target_Class_M', 'Target_Class_N'
    ],
    'Target': [
        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14
    ],
    'Chemical_Structure': [
        'SMILES_1', 'SMILES_2', 'SMILES_3', 'SMILES_4', 'SMILES_5',
        'SMILES_6', 'SMILES_7', 'SMILES_8', 'SMILES_9', 'SMILES_10',
        'SMILES_11', 'SMILES_12', 'SMILES_13', 'SMILES_14'
    ]
}

# Create a DataFrame
df = pd.DataFrame(data)

# Label encode categorical features
label_encoder = LabelEncoder()
df['Drug_Class_Label'] = label_encoder.fit_transform(df['Drug_Class'])
df['Target_Class_Label'] = label_encoder.fit_transform(df['Target_Class'])

# Now you have a DataFrame with label-encoded categorical features
101/17:
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Define your data
data = {
    'Drug_Name': [
        'Eplerenone', 'Spironolactone', 'Lisinopril', 'Enalapril', 'Benazepril',
        'Doxazosin', 'Prazosin', 'Terazosin', 'Amlodipine', 'Nifedipine', 
        'Hydralazine', 'Metoprolol', 'Verapamil', 'Atenolol'
    ],
    'Drug_Class': [
        'Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5',
        'Class_6', 'Class_7', 'Class_8', 'Class_9', 'Class_10', 
        'Class_11', 'Class_12', 'Class_13', 'Class_14'
    ],
    'Target_Class': [
        'Target_Class_A', 'Target_Class_B', 'Target_Class_C', 'Target_Class_D', 'Target_Class_E',
        'Target_Class_F', 'Target_Class_G', 'Target_Class_H', 'Target_Class_I', 'Target_Class_J', 
        'Target_Class_K', 'Target_Class_L', 'Target_Class_M', 'Target_Class_N'
    ],
    'Target': [
        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14
    ],
    'Chemical_Structure': [
        'SMILES_1', 'SMILES_2', 'SMILES_3', 'SMILES_4', 'SMILES_5',
        'SMILES_6', 'SMILES_7', 'SMILES_8', 'SMILES_9', 'SMILES_10',
        'SMILES_11', 'SMILES_12', 'SMILES_13', 'SMILES_14'
    ]
}

# Create a DataFrame
df = pd.DataFrame(data)

# Label encode categorical features
label_encoder = LabelEncoder()
df['Drug_Class_Label'] = label_encoder.fit_transform(df['Drug_Class'])
df['Target_Class_Label'] = label_encoder.fit_transform(df['Target_Class'])
display(data)
101/18:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
print(df['Drug Name'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 

# Create a DataFrame
df = pd.DataFrame(data)
print(data)
101/19:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 

# Create a DataFrame
df = pd.DataFrame(data)
print(data)
101/20:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 

# Create a DataFrame
df = pd.DataFrame(data)
print(data)
101/21:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name  ']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 

# Create a DataFrame
df = pd.DataFrame(data)
print(data)
101/22:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 

# Create a DataFrame
df = pd.DataFrame(data)
print(data)
101/23:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 

# Create a DataFrame
df = pd.DataFrame(data)
print(data)
101/24:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
101/25:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
101/26:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
101/27:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
101/28:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
101/29:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 

# Create a DataFrame
df = pd.DataFrame(data)
print(data)
101/30:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
101/31:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
101/32:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
101/33:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
101/34:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
101/35:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
101/36:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
101/37:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 

# Create a DataFrame
df = pd.DataFrame(data)
print(data)
101/38:
# Splitting train and test set
X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
101/39:
# Splitting train and test set
X = df[['Drug_Name', 'Drug_classes', 'Target_Class', 'Target', 'Common_name', 'Uniprot_ID', 'ChEMBL_ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
101/40:
# Splitting train and test set

df = pd.DataFrame(data)
print(data)

X = df[['Drug_Name', 'Drug_classes', 'Target_Class', 'Target', 'Common_name', 'Uniprot_ID', 'ChEMBL_ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
101/41:
# Splitting train and test set

df = pd.DataFrame(data)

X = df[['Drug_Name', 'Drug_classes', 'Target_Class', 'Target', 'Common_name', 'Uniprot_ID', 'ChEMBL_ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/1:
# Splitting train and test set

df = pd.DataFrame(data)

X = df[['Drug_Name', 'Drug_Class', 'Target_Class', 'Target', 'Common_Name', 'Uniprot_ID', 'ChEMBL_ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/2:
# Splitting train and test set

X = df[['Drug_Name', 'Drug_Class', 'Target_Class', 'Target', 'Common_Name', 'Uniprot_ID', 'ChEMBL_ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/3:
# Splitting train and test set

X = data[['Drug_Name', 'Drug_Class', 'Target_Class', 'Target', 'Common_Name', 'Uniprot_ID', 'ChEMBL_ID']]
y = data[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/4:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique()
102/5:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
102/6:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
102/7:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
102/8:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
102/9:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
102/10:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique()
102/11:
# Splitting train and test set

X = data[['Drug_Name', 'Drug_Class', 'Target_Class', 'Target', 'Common_Name', 'Uniprot_ID', 'ChEMBL_ID']]
y = data[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/12:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 

# Create a DataFrame
df = pd.DataFrame(data)
print(data)
102/13:
# Label Encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 

# Create a DataFrame
df = pd.DataFrame(data)
print(data)
102/14:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 

# Create a DataFrame
df = pd.DataFrame(data)
print(data)
102/15:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique()
102/16:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique()
102/17:
# Splitting train and test set

X = df[['Drug_Name', 'Drug_Class', 'Target_Class', 'Target', 'Common_Name', 'Uniprot_ID', 'ChEMBL_ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/18:
# Splitting train and test set

X = df[['Drug Name', 'Drug Class', 'Target Class', 'Target', 'Common Name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/19:
# Splitting train and test set

X = df[['Drug Name', 'Drug Class', 'Target Class', 'Target', 'Common Name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/20:
# Splitting train and test set

X = df[['Drug Name', 'Drug Class', 'Target Class', 'Target', 'Common Name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/21:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
print(df['Drug Name'])


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
print(df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
print(df['Target Class'])

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
print(df['Target'])

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
print(df['Common name'])

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
print(df['Uniprot ID'])

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
print(df['ChEMBL ID'])

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
print(df['Chemical structure'])
102/22:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
print(df['Drug Name'].unique())


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
print(df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
print(df['Target Class'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
print(df['Target'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
print(df['Common name'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
print(df['Uniprot ID'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
print(df['ChEMBL ID'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
print(df['Chemical structure'].unique())
102/23:
# Splitting train and test set

X = df[['Drug Name', 'Drug Class', 'Target Class', 'Target', 'Common Name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/24:
# Splitting train and test set

X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/25:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
print("Drug name:", df['Drug Name'].unique())


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
print("Drug classes:",df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
print("Target Class:",df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
print("Target:",df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
print("Common name:",df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
print("Uniprot ID:",df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
print("ChEMBL ID:",df['Drug classes'].unique()))

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
print("Chemical structure:",df['Drug classes'].unique())
102/26:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
print("Drug name:", df['Drug Name'].unique())


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
print("Drug classes:",df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
print("Target Class:",df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
print("Target:",df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
print("Common name:",df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
print("Uniprot ID:",df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
print("ChEMBL ID:",df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
print("Chemical structure:",df['Drug classes'].unique())
102/27:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
print("Drug name:", df['Drug Name'].unique())


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
print("Drug classes:", df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
print("Target Class:", df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
print("Target:", df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
print("Common name:", df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
print("Uniprot ID:", df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
print("ChEMBL ID:", df['Drug classes'].unique())

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
print("Chemical structure:", df['Drug classes'].unique())
102/28:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
print("Drug name:", df['Drug Name'])


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
print("Drug classes:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
print("Target Class:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
print("Target:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
print("Common name:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
print("Uniprot ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
print("ChEMBL ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
print("Chemical structure:", df['Drug classes'])
102/29:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
display("Drug name:", df['Drug Name'].unique())
print("Drug name:", df['Drug Name'])


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
print("Drug classes:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
print("Target Class:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
print("Target:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
print("Common name:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
print("Uniprot ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
print("ChEMBL ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
print("Chemical structure:", df['Drug classes'])
102/30:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
display("Drug name:", df['Drug Name'].unique())
print("Drug name:", df['Drug Name'])


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
display("Drug classes:", df['Drug classes'].unique())
print("Drug classes:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
display("Target Class:", df['Target Class'].unique())
print("Target Class:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
display("Target:", df['Target'].unique())
print("Target:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
display("Common name:", df['Common name'].unique())
print("Common name:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
display("Uniprot ID:", df['Uniprot ID'].unique())
print("Uniprot ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
display("ChEMBL ID:", df['ChEMBL ID'].unique())
print("ChEMBL ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
display("Chemical structure:", df['Chemical structure'].unique())
print("Chemical structure:", df['Chemical structure'])
102/31:
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming 'data' is your DataFrame

# Feature Extraction
X = data[['Drug name', 'Drug classes', 'Target Class', 'Chemical structure']]
y = data['Interaction_Type']  # Assuming 'Interaction_Type' is your target variable

# Encode Categorical Variables
X_encoded = pd.get_dummies(X, columns=['Drug name'])

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree Classifier
model = DecisionTreeClassifier()

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
102/32:
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming 'data' is your DataFrame

# Feature Extraction
X = df[['Drug name', 'Drug classes', 'Target Class', 'Chemical structure']]
y = df['Interaction_Type']  # Assuming 'Interaction_Type' is your target variable

# Encode Categorical Variables
X_encoded = pd.get_dummies(X, columns=['Drug name'])

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree Classifier
model = DecisionTreeClassifier()

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
102/33:
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming 'data' is your DataFrame

# Feature Extraction
X = df[['Drug Name', 'Drug classes', 'Target Class', 'ChEMBL ID']]
y = df['Interaction_Type']  # Assuming 'Interaction_Type' is your target variable

# Encode Categorical Variables
X_encoded = pd.get_dummies(X, columns=['Drug name'])

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree Classifier
model = DecisionTreeClassifier()

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
102/34:
import numpy as np
from sklearn.cluster import KMeans

# Assuming 'data' is your DataFrame

# Select features for clustering
features = data[['Drug name', 'Drug classes', 'Target Class', 'Chemical structure']].values

# Fit K-means clustering model
kmeans = KMeans(n_clusters=3, random_state=42)  # You can adjust the number of clusters as needed
clusters = kmeans.fit_predict(features)

# Add cluster labels to the DataFrame
data['Cluster'] = clusters

# Now 'Cluster' column will contain the cluster labels for each drug
102/35:
import numpy as np
from sklearn.cluster import KMeans

# Assuming 'data' is your DataFrame

# Select features for clustering
features = df[['Drug name', 'Drug classes', 'Target Class', 'Chemical structure']].values

# Fit K-means clustering model
kmeans = KMeans(n_clusters=3, random_state=42)  # You can adjust the number of clusters as needed
clusters = kmeans.fit_predict(features)

# Add cluster labels to the DataFrame
data['Cluster'] = clusters

# Now 'Cluster' column will contain the cluster labels for each drug
102/36:
import numpy as np
from sklearn.cluster import KMeans

# Assuming 'data' is your DataFrame

# Select features for clustering
features = df[['Drug Name', 'Drug classes', 'Target Class', 'ChEMBL ID']]

# Fit K-means clustering model
kmeans = KMeans(n_clusters=3, random_state=42)  # You can adjust the number of clusters as needed
clusters = kmeans.fit_predict(features)

# Add cluster labels to the DataFrame
data['Cluster'] = clusters

# Now 'Cluster' column will contain the cluster labels for each drug
102/37:
import numpy as np
from sklearn.cluster import KMeans

# Assuming 'data' is your DataFrame

# Select features for clustering
features = df[['Drug Name', 'Drug classes', 'Target Class', 'ChEMBL ID']]

# Fit K-means clustering model
kmeans = KMeans(n_clusters=3, random_state=42)  # You can adjust the number of clusters as needed
clusters = kmeans.fit_predict(features)

# Add cluster labels to the DataFrame
df['Cluster'] = clusters

# Now 'Cluster' column will contain the cluster labels for each drug
102/38:
# Assuming 'data' is your DataFrame
import pandas as pd

# Define the number of bins you want
num_bins = 100 

# Create bins for the 'probability' column
df['Probability_bins'] = pd.cut(df['Probability*'], num_bins, labels=False)
102/39:
# Assuming 'data' is your DataFrame
import pandas as pd

# Define the number of bins you want
num_bins = 100 

# Create bins for the 'probability' column
df['Probability'] = pd.cut(df['Probability*'], num_bins, labels=False)
print(df['Probability'])
102/40:
# Splitting train and test set

X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/41:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
102/42:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
102/43:
import numpy as np
from sklearn.cluster import KMeans

# Assuming 'data' is your DataFrame

# Select features for clustering
features = df[['Drug Name', 'Drug classes', 'Target Class', 'ChEMBL ID']]

# Fit K-means clustering model
kmeans = KMeans(n_clusters=3, random_state=42)  # You can adjust the number of clusters as needed
clusters = kmeans.fit_predict(features)

# Add cluster labels to the DataFrame
df['Cluster'] = clusters
102/44:
import numpy as np
from sklearn.cluster import KMeans

# Assuming 'data' is your DataFrame

# Select features for clustering
features = df[['Drug Name', 'Drug classes', 'Target Class', 'Probability', 'Chemical structure']]

# Fit K-means clustering model
kmeans = KMeans(n_clusters=3, random_state=42)  # You can adjust the number of clusters as needed
clusters = kmeans.fit_predict(features)

# Add cluster labels to the DataFrame
df['Cluster'] = clusters
102/45:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score

# Assuming X_train and y_train are already defined from previous steps

# Step 3: Model Selection
model = DecisionTreeClassifier()  # Using Decision Tree Classifier

# Step 4: Model Training with Cross-Validation
num_folds = 5  # Number of cross-validation folds

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=num_folds)

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')

# Calculate the mean and standard deviation of the scores
mean_cv_score = cv_scores.mean()
std_cv_score = cv_scores.std()

print(f'Mean CV Score: {mean_cv_score}')
print(f'Standard Deviation of CV Scores: {std_cv_score}')
102/46:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
102/47:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
102/48:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score

# Assuming X_train and y_train are already defined from previous steps

# Step 3: Model Selection
model = DecisionTreeClassifier()  # Using Decision Tree Classifier

# Step 4: Model Training with Cross-Validation
num_folds = 5  # Number of cross-validation folds

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=num_folds)

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')

# Calculate the mean and standard deviation of the scores
mean_cv_score = cv_scores.mean()
std_cv_score = cv_scores.std()

print(f'Mean CV Score: {mean_cv_score}')
print(f'Standard Deviation of CV Scores: {std_cv_score}')
102/49:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
102/50:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
102/51:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
102/52:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
102/53:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
102/54:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
display("Drug name:", df['Drug Name'].unique())
print("Drug name:", df['Drug Name'])


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
display("Drug classes:", df['Drug classes'].unique())
print("Drug classes:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
display("Target Class:", df['Target Class'].unique())
print("Target Class:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
display("Target:", df['Target'].unique())
print("Target:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
display("Common name:", df['Common name'].unique())
print("Common name:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
display("Uniprot ID:", df['Uniprot ID'].unique())
print("Uniprot ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
display("ChEMBL ID:", df['ChEMBL ID'].unique())
print("ChEMBL ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
display("Chemical structure:", df['Chemical structure'].unique())
print("Chemical structure:", df['Chemical structure'])
102/55:
# Assuming 'data' is your DataFrame
import pandas as pd

# Define the number of bins you want
num_bins = 100 

# Create bins for the 'probability' column
df['Probability'] = pd.cut(df['Probability*'], num_bins, labels=False)
print(df['Probability'])
102/56:
# Splitting train and test set

X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/57:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
102/58:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
102/59:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score

# Assuming X_train and y_train are already defined from previous steps

# Step 3: Model Selection
model = DecisionTreeClassifier()  # Using Decision Tree Classifier

# Step 4: Model Training with Cross-Validation
num_folds = 5  # Number of cross-validation folds

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=num_folds)

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')

# Calculate the mean and standard deviation of the scores
mean_cv_score = cv_scores.mean()
std_cv_score = cv_scores.std()

print(f'Mean CV Score: {mean_cv_score}')
print(f'Standard Deviation of CV Scores: {std_cv_score}')
102/60:
# Predict the drug combination effect of Eplerenone and Amlodipine
from json import encoder
from numpy import dtype
import pandas as pd
from functools import lru_cache
from sklearn.tree import DecisionTreeRegressor
102/61:
import pandas as pd

df = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID', 'Probability*', 'Chemical structure']]
 

# Get the features of the last 10 drugs
drug_features_10 = df.iloc[-10:, :-1]

# Get the features in the order they were in the training data
features = df.columns[:-1]

# Remove the features that the model has not seen before
drug_features_10 = drug_features_10.drop(columns=["Probability*"])

# Predict the strength of the drug interaction for the last 10 drugs
y_pred_10 = model.predict(drug_features_10)

# Print the results
print("Predicted strength of drug interaction for the last 10 drugs:", y_pred_10)
102/62:
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score

# Assuming X_train and y_train are already defined from previous steps

# Step 3: Model Selection
model = DecisionTreeRegressor()  # Using Decision Tree Regressor

# Step 4: Model Training with Cross-Validation
num_folds = 5  # Number of cross-validation folds

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='neg_mean_squared_error')

# Convert negative scores to positive (since sklearn returns negated values for regression)
cv_scores = -cv_scores

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')

# Calculate the mean and standard deviation of the scores
mean_cv_score = cv_scores.mean()
std_cv_score = cv_scores.std()

print(f'Mean CV Score (MSE): {mean_cv_score}')
print(f'Standard Deviation of CV Scores: {std_cv_score}')
102/63:
from sklearn.model_selection import GridSearchCV

# Define hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a grid search object
grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f'Best Hyperparameters: {best_params}')

# Get the best model
best_model = grid_search.best_estimator_
102/64:
from sklearn.model_selection import GridSearchCV

# Define hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a grid search object
grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f'Best Hyperparameters: {best_params}')

# Get the best model
best_model = grid_search.best_estimator_

from sklearn.metrics import mean_squared_error

# Assuming 'best_model' and 'X_test' are already defined from previous steps

# Make predictions on the test set
y_pred = best_model.predict(X_test)

# Calculate the mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error (MSE) on Test Set: {mse}')
102/65:
# Splitting train and test set

X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/66:
# Splitting train and test set

X = df[['Drug Name', 'Drug classes', 'Target Class', 'Target', 'Common name', 'Uniprot ID', 'ChEMBL ID']]
y = df[['Probability*', 'Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/67:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
102/68:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
102/69:
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score

# Assuming X_train and y_train are already defined from previous steps

# Step 3: Model Selection
model = DecisionTreeRegressor()  # Using Decision Tree Regressor

# Step 4: Model Training with Cross-Validation
num_folds = 5  # Number of cross-validation folds

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='neg_mean_squared_error')

# Convert negative scores to positive (since sklearn returns negated values for regression)
cv_scores = -cv_scores

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')

# Calculate the mean and standard deviation of the scores
mean_cv_score = cv_scores.mean()
std_cv_score = cv_scores.std()

print(f'Mean CV Score (MSE): {mean_cv_score}')
print(f'Standard Deviation of CV Scores: {std_cv_score}')
102/70:
from sklearn.model_selection import GridSearchCV

# Define hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a grid search object
grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f'Best Hyperparameters: {best_params}')

# Get the best model
best_model = grid_search.best_estimator_

from sklearn.metrics import mean_squared_error

# Assuming 'best_model' and 'X_test' are already defined from previous steps

# Make predictions on the test set
y_pred = best_model.predict(X_test)

# Calculate the mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error (MSE) on Test Set: {mse}')
102/71:
features_for_prediction = df[['Drug Name', 'Drug classes', 'Target Class', 'Chemical structure']]
predictions = decision_tree_model.predict(features_for_prediction)
df['Predicted Interaction'] = predictions
102/72:
from sklearn.model_selection import GridSearchCV

# Define hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a grid search object
grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f'Best Hyperparameters: {best_params}')

# Get the best model
best_model = grid_search.best_estimator_

from sklearn.metrics import mean_squared_error

# Assuming 'best_model' and 'X_test' are already defined from previous steps

# Make predictions on the test set
y_pred = best_model.predict(X_test)

# Calculate the mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error (MSE) on Test Set: {mse}')

features_for_prediction = df[['Drug Name', 'Drug classes', 'Target Class', 'Chemical structure']]
predictions = decision_tree_model.predict(features_for_prediction)
df['Predicted Interaction'] = predictions
102/73:
from sklearn.model_selection import GridSearchCV

# Step 3: Model Selection
decision_tree_model = DecisionTreeRegressor() 

# Define hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a grid search object
grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f'Best Hyperparameters: {best_params}')

# Get the best model
best_model = grid_search.best_estimator_

from sklearn.metrics import mean_squared_error

# Assuming 'best_model' and 'X_test' are already defined from previous steps

# Make predictions on the test set
y_pred = best_model.predict(X_test)

# Calculate the mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error (MSE) on Test Set: {mse}')

features_for_prediction = df[['Drug Name', 'Drug classes', 'Target Class', 'Chemical structure']]
predictions = decision_tree_model.predict(features_for_prediction)
df['Predicted Interaction'] = predictions
102/74:
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

# Assuming 'X_train' and 'y_train' are your training features and labels
decision_tree_model = DecisionTreeRegressor()

# Define hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a grid search object
grid_search = GridSearchCV(decision_tree_model, param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f'Best Hyperparameters: {best_params}')

# Get the best model
best_model = grid_search.best_estimator_

# Assuming 'X_test' is your test data
y_pred = best_model.predict(X_test)

# Calculate the mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error (MSE) on Test Set: {mse}')

# Assuming 'features_for_prediction' is your new data
predictions = best_model.predict(features_for_prediction)
df['Predicted Interaction'] = predictions
102/75:
# Splitting train and test set

X = df ['Drug Name', 'Drug classes', 'Target Class']
y = df['Chemical structure']  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/76:
# Splitting train and test set

X = df [['Drug Name', 'Drug classes', 'Target Class']]
y = df[['Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/77:
# Assuming train_Y and test_Y are pandas Series objects
scaler = StandardScaler()

# Fit the scaler to the train set and transform the data
Y_train_data = scaler.fit_transform(y_train.values.reshape(-1, 1))
Y_test_data = scaler.transform(y_test.values.reshape(-1, 1))

display("Y train2 and Y test2 scaling:", Y_train_data, "\n", Y_test_data)
102/78:

from sklearn.model_selection import cross_val_score

# Assuming X_train and y_train are already defined from previous steps

# Step 3: Model Selection
model = DecisionTreeRegressor()  # Using Decision Tree Regressor

# Step 4: Model Training with Cross-Validation
num_folds = 5  # Number of cross-validation folds

# Perform cross-validationfrom sklearn.tree import DecisionTreeRegressor
cv_scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='neg_mean_squared_error')

# Convert negative scores to positive (since sklearn returns negated values for regression)
cv_scores = -cv_scores

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')

# Calculate the mean and standard deviation of the scores
mean_cv_score = cv_scores.mean()
std_cv_score = cv_scores.std()

print(f'Mean CV Score (MSE): {mean_cv_score}')
print(f'Standard Deviation of CV Scores: {std_cv_score}')
102/79:

from sklearn.model_selection import cross_val_score

# Assuming X_train and y_train are already defined from previous steps

# Step 3: Model Selection
model = DecisionTreeRegressor()  # Using Decision Tree Regressor

# Step 4: Model Training with Cross-Validation
num_folds = 5  # Number of cross-validation folds

# Perform cross-validationfrom sklearn.tree import DecisionTreeRegressor
cv_scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='neg_mean_squared_error')

# Convert negative scores to positive (since sklearn returns negated values for regression)
cv_scores = -cv_scores

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')

# Calculate the mean and standard deviation of the scores
mean_cv_score = cv_scores.mean()
std_cv_score = cv_scores.std()

print(f'Mean CV Score (MSE): {mean_cv_score}')
print(f'Standard Deviation of CV Scores: {std_cv_score}')

# Assuming 'features_for_prediction' is your new data
predictions = best_model.predict(features_for_prediction)
df['Predicted Interaction'] = predictions
102/80:

from sklearn.model_selection import cross_val_score

# Assuming X_train and y_train are already defined from previous steps

# Step 3: Model Selection
model = DecisionTreeRegressor()  # Using Decision Tree Regressor

# Step 4: Model Training with Cross-Validation
num_folds = 5  # Number of cross-validation folds

# Perform cross-validationfrom sklearn.tree import DecisionTreeRegressor
cv_scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='neg_mean_squared_error')

# Convert negative scores to positive (since sklearn returns negated values for regression)
cv_scores = -cv_scores

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')

# Calculate the mean and standard deviation of the scores
mean_cv_score = cv_scores.mean()
std_cv_score = cv_scores.std()

print(f'Mean CV Score (MSE): {mean_cv_score}')
print(f'Standard Deviation of CV Scores: {std_cv_score}')

features_for_prediction = df[['Drug Name', 'Drug classes', 'Target Class', 'Chemical structure']]

# Use the trained model to make predictions
predictions = decision_tree_model.predict(features_for_prediction)

# 'predictions' will contain the predicted interactions for the existing data
# You can add these predictions back to your DataFrame if needed
df['Predicted Interaction'] = predictions

# Now 'df' will have an additional column 'Predicted Interaction' with the model's predictions
print(df)
102/81:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score

# Assuming X_train and y_train are already defined from previous steps

# Step 3: Model Selection
model = DecisionTreeClassifier()  # Using Decision Tree Classifier

# Step 4: Model Training with Cross-Validation
num_folds = 5  # Number of cross-validation folds

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=num_folds)

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')

# Calculate the mean and standard deviation of the scores
mean_cv_score = cv_scores.mean()
std_cv_score = cv_scores.std()

print(f'Mean CV Score: {mean_cv_score}')
print(f'Standard Deviation of CV Scores: {std_cv_score}')


features_for_prediction = df[['Drug Name', 'Drug classes', 'Target Class', 'Chemical structure']]

# Use the trained model to make predictions
predictions = decision_tree_model.predict(features_for_prediction)

# 'predictions' will contain the predicted interactions for the existing data
# You can add these predictions back to your DataFrame if needed
df['Predicted Interaction'] = predictions

# Now 'df' will have an additional column 'Predicted Interaction' with the model's predictions
print(df)
102/82:
# Splitting train and test set

data = df [['Drug Name', 'Drug classes', 'Target Class', 'Chemical structure']]


# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(data, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/83:
# Splitting train and test set

X = df [['Drug Name', 'Drug classes', 'Target Class']]
y = df[['Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
102/84:
from sklearn.tree import DecisionTreeRegressor

# Create a decision tree regressor
decision_tree_model = DecisionTreeRegressor(random_state=42)

# Fit the model to the training data
decision_tree_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = decision_tree_model.predict(X_test)

# Evaluate the model (e.g., calculate mean squared error)
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error (MSE) on Test Set: {mse}')
102/85:
from sklearn.tree import DecisionTreeRegressor

# Create a decision tree regressor
decision_tree_model = DecisionTreeRegressor(random_state=42)

# Fit the model to the training data
decision_tree_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = decision_tree_model.predict(X_test)

# Evaluate the model (e.g., calculate mean squared error)
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error (MSE) on Test Set: {mse}')

features_for_prediction = df[['Drug Name', 'Drug classes', 'Target Class', 'Chemical structure']]

# Use the trained model to make predictions
predictions = decision_tree_model.predict(features_for_prediction)

# 'predictions' will contain the predicted interactions for the existing data
# You can add these predictions back to your DataFrame if needed
df['Predicted Interaction'] = predictions

# Now 'df' will have an additional column 'Predicted Interaction' with the model's predictions
print(df)
102/86:
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 4: Model Evaluation
y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)

# Step 5: Predict Interactions for 10 Drugs
drugs_of_interest = ...  # Select the 10 drugs you're interested in
features_for_prediction = preprocess_features(drugs_of_interest)  # Make sure to preprocess these features
predictions = model.predict(features_for_prediction)
102/87:
model = DecisionTreeClassifier()
model.fit(X_train, y_train)
102/88:
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 4: Model Evaluation
y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)

# Step 5: Predict Interactions for 10 Drugs
drugs_of_interest = ...  # Select the 10 drugs you're interested in
features_for_prediction = preprocess_features(drugs_of_interest)  # Make sure to preprocess these features
predictions = model.predict(features_for_prediction)
102/89:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

# Assuming X_train, y_train, X_test, y_test are available from your preprocessing and splitting steps

# Step 3: Model Selection and Training
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 4: Model Evaluation
y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)

# Step 5: Predict Interactions for 10 Drugs
drugs_of_interest = ...  # Select the 10 drugs you're interested in
features_for_prediction = preprocess_features(df[1])  # Make sure to preprocess these features
predictions = model.predict(df['Target Class'])

# 'predictions' will contain the predicted interactions for the 10 drugs
102/90:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

# Assuming X_train, y_train, X_test, y_test are available from your preprocessing and splitting steps

# Step 3: Model Selection and Training
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 4: Model Evaluation
y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)

# Step 5: Predict Interactions for 10 Drugs
drugs_of_interest = ...  # Select the 10 drugs you're interested in
features_for_prediction = preprocess_features(df[1])  # Make sure to preprocess these features
predictions = model.predict(df['Target Class'])

# 'predictions' will contain the predicted interactions for the 10 drugs
102/91:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report


model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 2: Model Evaluation
from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)

# Step 3: Predict Interactions for 10 Drugs
# Assuming 'features_of_interest' is a DataFrame containing the features of the 10 drugs
predictions = model.predict('Chemical structure')

# 'predictions' will contain the predicted interactions for the 10 drugs

# You can then use 'predictions' for further analysis or decision-making.
102/92:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report


model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 2: Model Evaluation
from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)

# Step 3: Predict Interactions for 10 Drugs
# Assuming 'features_of_interest' is a DataFrame containing the features of the 10 drugs
predictions = model.predict(y_train['Chemical structure'])

# 'predictions' will contain the predicted interactions for the 10 drugs

# You can then use 'predictions' for further analysis or decision-making.
102/93:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report


model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 2: Model Evaluation
from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)

# Step 3: Predict Interactions for 10 Drugs
# Assuming 'features_of_interest' is a DataFrame containing the features of the 10 drugs
predictions = model.predict(Y_train_data['Chemical structure'])

# 'predictions' will contain the predicted interactions for the 10 drugs

# You can then use 'predictions' for further analysis or decision-making.
102/94:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report


model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 2: Model Evaluation
from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)

# Step 3: Predict Interactions for 10 Drugs
# Assuming 'features_of_interest' is a DataFrame containing the features of the 10 drugs
predictions = model.predict(y_train['Chemical structure'])

# 'predictions' will contain the predicted interactions for the 10 drugs

# You can then use 'predictions' for further analysis or decision-making.
102/95:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report


model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 2: Model Evaluation
from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)

# Step 3: Predict Interactions for 10 Drugs
# Assuming 'features_of_interest' is a DataFrame containing the features of the 10 drugs
# Assuming 'y_train' is a DataFrame containing 'Chemical structure'

# Reshape the data
predictions = model.predict(y_train['Chemical structure'].values.reshape(-1, 1))

# 'predictions' will contain the predicted interactions for the 10 drugs


# 'predictions' will contain the predicted interactions for the 10 drugs

# You can then use 'predictions' for further analysis or decision-making.
104/1:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
104/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
104/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
104/4:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
104/5:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
104/6:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
display("Drug name:", df['Drug Name'].unique())
print("Drug name:", df['Drug Name'])


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
display("Drug classes:", df['Drug classes'].unique())
print("Drug classes:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
display("Target Class:", df['Target Class'].unique())
print("Target Class:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
display("Target:", df['Target'].unique())
print("Target:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
display("Common name:", df['Common name'].unique())
print("Common name:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
display("Uniprot ID:", df['Uniprot ID'].unique())
print("Uniprot ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
display("ChEMBL ID:", df['ChEMBL ID'].unique())
print("ChEMBL ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
display("Chemical structure:", df['Chemical structure'].unique())
print("Chemical structure:", df['Chemical structure'])
104/7:
# Splitting train and test set

X = df [['Drug Name', 'Drug classes', 'Target Class']]
y = df[['Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
104/8:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
104/9:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report


model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 2: Model Evaluation
from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)

# Step 3: Predict Interactions for 10 Drugs
# Assuming 'features_of_interest' is a DataFrame containing the features of the 10 drugs
# Assuming 'y_train' is a DataFrame containing 'Chemical structure'

# Reshape the data
predictions = model.predict(['Drug Name Drug classes    Target Class'].values.reshape(-1, 1))

# 'predictions' will contain the predicted interactions for the 10 drugs


# 'predictions' will contain the predicted interactions for the 10 drugs

# You can then use 'predictions' for further analysis or decision-making.
104/10:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
104/11:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
104/12:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
104/13:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
104/14:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
104/15:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
display("Drug name:", df['Drug Name'].unique())
print("Drug name:", df['Drug Name'])


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
display("Drug classes:", df['Drug classes'].unique())
print("Drug classes:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
display("Target Class:", df['Target Class'].unique())
print("Target Class:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
display("Target:", df['Target'].unique())
print("Target:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
display("Common name:", df['Common name'].unique())
print("Common name:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
display("Uniprot ID:", df['Uniprot ID'].unique())
print("Uniprot ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
display("ChEMBL ID:", df['ChEMBL ID'].unique())
print("ChEMBL ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
display("Chemical structure:", df['Chemical structure'].unique())
print("Chemical structure:", df['Chemical structure'])
104/16:
# Splitting train and test set

X = df [['Drug Name', 'Drug classes', 'Target Class']]
y = df[['Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
104/17:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
104/18:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report


model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 2: Model Evaluation
from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)

# Step 3: Predict Interactions for 10 Drugs
# Assuming 'features_of_interest' is a DataFrame containing the features of the 10 drugs
# Assuming 'y_train' is a DataFrame containing 'Chemical structure'

# Reshape the data
predictions = model.predict(['Drug Name Drug classes    Target Class'].values.reshape(-1, 1))

# 'predictions' will contain the predicted interactions for the 10 drugs


# 'predictions' will contain the predicted interactions for the 10 drugs

# You can then use 'predictions' for further analysis or decision-making.
104/19:
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Train the decision tree model.
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate the decision tree model.
y_pred = model.predict(X_test)
104/20:
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Train the decision tree model.
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate the decision tree model.
y_pred = model.predict(X_test)

# Evaluate the decision tree model.
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1_score = f1_score(y_test, y_pred)

print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('F1 score:', f1_score)
104/21:
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Train the decision tree model.
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate the decision tree model.
y_pred = model.predict(X_test)

# Evaluate the decision tree model.
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='micro')
recall = recall_score(y_test, y_pred, average='micro')
f1 = f1_score(y_test, y_pred, average='micro')

print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('F1 score:', f1)
104/22:
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Train the decision tree model.
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate the decision tree model.
y_pred = model.predict(X_test)

# Evaluate the decision tree model.
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='micro')
recall = recall_score(y_test, y_pred, average='micro')
f1 = f1_score(y_test, y_pred, average='micro')

print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('F1 score:', f1)

# Use the decision tree model to predict drug combinations.
new_drug_combinations = pd.DataFrame({
    'drug_class': ['drug class 1', 'drug class 2'],
    'target_class': ['target class 1', 'target class 2']
})

predicted_synergy = model.predict(new_drug_combinations)

# Print the predicted synergy.
print('Predicted synergy:', predicted_synergy)
104/23:
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Train the decision tree model.
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate the decision tree model.
y_pred = model.predict(X_test)

# Evaluate the decision tree model.
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='micro')
recall = recall_score(y_test, y_pred, average='micro')
f1 = f1_score(y_test, y_pred, average='micro')

print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('F1 score:', f1)

# Use the decision tree model to predict drug combinations.
new_drug_combinations = pd.DataFrame({
    'drug_class': ['3', '5'],
    'target_class': ['11', '20']
})

predicted_synergy = model.predict(new_drug_combinations)

# Print the predicted synergy.
print('Predicted synergy:', predicted_synergy)
104/24:
# Splitting train and test set

X = df [['Drug Name', 'Drug classes', 'Target Class']]
y = df[['Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train: ')
print(X_train.head())
print('X_test: ')
print(X_test.head())
print('y_train: ')
print(y_train.head())
print('y_test: ')
print(y_test.head())
104/25:
import pandas as pd
from itertools import combinations
from sklearn.tree import DecisionTreeClassifier

# Load the drug data
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate all possible drug combinations
drug_combinations = []
for i in range(len(df)):
    for j in range(i + 1, len(df)):
        drug_combinations.append([df.loc[i, 'Drug Name'], df.loc[j, 'Drug Name']])

# Load the decision tree model
model = DecisionTreeClassifier()
model.load('decision_tree_model.pkl')

# Predict the synergy for each drug combination
synergies = []
for drug_combination in drug_combinations:
    synergy = model.predict([drug_combination])
    synergies.append(synergy)

# Select the drug combinations with the highest predicted synergy
top_drug_combinations = []
for i in range(len(drug_combinations)):
    if synergies[i] >= 0.9:
        top_drug_combinations.append(drug_combinations[i])

# Print the top drug combinations
print(top_drug_combinations)
104/26:
import pandas as pd
from itertools import combinations
from sklearn.tree import DecisionTreeClassifier
import pickle

# Load the drug data
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate all possible drug combinations
drug_combinations = []
for i in range(len(df)):
    for j in range(i + 1, len(df)):
        drug_combinations.append([df.loc[i, 'Drug Name'], df.loc[j, 'Drug Name']])

# Load the decision tree model
with open('decision_tree_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Predict the synergy for each drug combination
synergies = []
for drug_combination in drug_combinations:
    synergy = model.predict([drug_combination])
    synergies.append(synergy)

# Select the drug combinations with the highest predicted synergy
top_drug_combinations = []
for i in range(len(drug_combinations)):
    if synergies[i] >= 0.9:
        top_drug_combinations.append(drug_combinations[i])

# Print the top drug combinations
print(top_drug_combinations)
104/27:
import pandas as pd
from itertools import combinations
from sklearn.tree import DecisionTreeClassifier
import pickle

# Load the drug data
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate all possible drug combinations
drug_combinations = []
for i in range(len(df)):
    for j in range(i + 1, len(df)):
        # Remove the invalid character from the drug combination
        drug_combination = remove_non_printable_characters(drug_combination)
        drug_combinations.append([df.loc[i, 'Drug Name'], df.loc[j, 'Drug Name']])

# Load the decision tree model
with open('decision_tree_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Predict the synergy for each drug combination
synergies = []
for drug_combination in drug_combinations:
    synergy = model.predict([drug_combination])
    synergies.append(synergy)

# Select the drug combinations with the highest predicted synergy
top_drug_combinations = []
for i in range(len(drug_combinations)):
    if synergies[i] >= 0.9:
        top_drug_combinations.append(drug_combinations[i])

# Print the top drug combinations
print(top_drug_combinations)
104/28:
import pandas as pd
from itertools import combinations
from sklearn.tree import DecisionTreeClassifier
import pickle

# Load the drug data
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate all possible drug combinations
drug_combinations = []
for i in range(len(df)):
    for j in range(i + 1, len(df)):
        # Remove the invalid character from the drug combination  drug_combination = remove_non_printable_characters(drug_combination)
          drug_combinations.append([df.loc[i, 'Drug Name'], df.loc[j, 'Drug Name']])

# Load the decision tree model
with open('decision_tree_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Predict the synergy for each drug combination
synergies = []
for drug_combination in drug_combinations:
    synergy = model.predict([drug_combination])
    synergies.append(synergy)

# Select the drug combinations with the highest predicted synergy
top_drug_combinations = []
for i in range(len(drug_combinations)):
    if synergies[i] >= 0.9:
        top_drug_combinations.append(drug_combinations[i])

# Print the top drug combinations
print(top_drug_combinations)
104/29:
import pandas as pd
from itertools import combinations
from sklearn.tree import DecisionTreeClassifier
import pickle

# Load the drug data
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate all possible drug combinations
drug_combinations = []
for i in range(len(df)):
    for j in range(i + 1, len(df)):
        # Remove the invalid character from the drug combination  drug_combination = remove_non_printable_characters(drug_combination)
          drug_combinations.append([df.loc[i, 'Drug Name'], df.loc[j, 'Drug Name']])

# Load the decision tree model
with open('decision_tree_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Predict the synergy for each drug combination
synergies = []
for drug_combination in drug_combinations:
    synergy = model.predict([drug_combination])
    synergies.append(synergy)

# Select the drug combinations with the highest predicted synergy
top_drug_combinations = []
for i in range(len(drug_combinations)):
    if synergies[i] >= 0.9:
       top_drug_combinations.append(drug_combinations[i])

# Print the top drug combinations
print(top_drug_combinations)
104/30:
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
import pickle

# Load the drug data
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate all possible drug combinations
drug_combinations = []
for i in range(len(df)):
    for j in range(i + 1, len(df)):
        drug_combinations.append([df.loc[i, 'Drug Name'], df.loc[j, 'Drug Name']])

# Load the decision tree model
with open('decision_tree_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Predict the synergy for each drug combination
synergies = []
for drug_combination in drug_combinations:
    synergy = model.predict([drug_combination])
    synergies.append(synergy)

# Select the drug combinations with the highest predicted synergy
top_drug_combinations = []
for i in range(len(drug_combinations)):
    if synergies[i] >= 0.9:
        top_drug_combinations.append(drug_combinations[i])

# Print the top drug combinations
print(top_drug_combinations)
104/31:
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
import pickle

# Load the drug data
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate all possible drug combinations
drug_combinations = []
for i in range(len(df)):
    for j in range(i + 1, len(df)):
        drug_combinations.append([df.loc[i, 'Drug Name'], df.loc[j, 'Drug Name']])

# Train the decision tree model.
model = DecisionTreeClassifier()
model.fit(X_train, y_train)


# Predict the synergy for each drug combination
synergies = []
for drug_combination in drug_combinations:
    synergy = model.predict([drug_combination])
    synergies.append(synergy)

# Select the drug combinations with the highest predicted synergy
top_drug_combinations = []
for i in range(len(drug_combinations)):
    if synergies[i] >= 0.9:
        top_drug_combinations.append(drug_combinations[i])

# Print the top drug combinations
print(top_drug_combinations)
104/32:
# Preprocess the drug data to convert all strings to floats
def preprocess_drug_data(drug_data):
  """Preprocesses the drug data to convert all strings to floats.

  Args:
    drug_data: A Pandas DataFrame containing the drug data.

  Returns:
    A Pandas DataFrame with all strings converted to floats.
  """

  for column in drug_data.columns:
    if drug_data[column].dtype == 'object':
      drug_data[column] = drug_data[column].astype('float')

  return drug_data

# Preprocess the drug data
drug_data = preprocess_drug_data(drug_data)
104/33:
# Preprocess the drug data to convert all strings to floats
def preprocess_drug_data(Drug Name):
  """Preprocesses the drug data to convert all strings to floats.

  Args:
    drug_data: A Pandas DataFrame containing the drug data.

  Returns:
    A Pandas DataFrame with all strings converted to floats.
  """

  for column in Drug_Name.columns:
    if drug_data[column].dtype == 'object':
      drug_data[column] = drug_data[Drug_Name].astype('float')

  return drug_data

# Preprocess the drug data
drug_data = preprocess_drug_data(Drug_Name)
104/34:
# Train the decision tree model
from sklearn.tree import DecisionTreeClassifier


model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Predict the synergy for each drug combination
synergies = []
for drug_combination in drug_combinations:
  synergy = model.predict([drug_combination])
  synergies.append(synergy)

# Select the drug combinations with the highest predicted synergy
top_drug_combinations = []
for i in range(len(drug_combinations)):
  if synergies[i] >= 0.9:
    top_drug_combinations.append(drug_combinations[i])

# Print the top drug combinations
print(top_drug_combinations)
104/35:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        drug_similarity_matrix[i, j] = Chem.DataStructs.FingerprintSimilarity(Chem.MolFromSmiles(drug_data['chemical structure'].iloc[i]), Chem.MolFromSmiles(drug_data['chemical structure'].iloc[j]))

# Identify drug pairs that have similar structures.
similar_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_similarity_matrix[i, j] > 0.8:
            similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that target the same class of proteins.
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
            same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that belong to different drug classes.
different_drug_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Drug classes'].iloc[i] != drug_data['Drug classes'].iloc[j]:
            different_drug_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Print the results.
print('Similar drug pairs:', similar_drug_pairs)
print('Same target class drug pairs:', same_target_class_drug_pairs)
print('Different drug class drug pairs:', different_drug_class_drug_pairs)
104/36:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        drug_similarity_matrix[i, j] = Chem.DataStructs.FingerprintSimilarity(Chem.MolFromSmiles(drug_data['Chemical structure'].iloc[i]), Chem.MolFromSmiles(drug_data['chemical structure'].iloc[j]))

# Identify drug pairs that have similar structures.
similar_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_similarity_matrix[i, j] > 0.8:
            similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that target the same class of proteins.
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
            same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that belong to different drug classes.
different_drug_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Drug classes'].iloc[i] != drug_data['Drug classes'].iloc[j]:
            different_drug_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Print the results.
print('Similar drug pairs:', similar_drug_pairs)
print('Same target class drug pairs:', same_target_class_drug_pairs)
print('Different drug class drug pairs:', different_drug_class_drug_pairs)
104/37:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        drug_similarity_matrix[i, j] = Chem.DataStructs.FingerprintSimilarity(Chem.MolFromSmiles(drug_data['Chemical structure'].iloc[i]), Chem.MolFromSmiles(drug_data['chemical structure'].iloc[j]))

# Identify drug pairs that have similar structures.
similar_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_similarity_matrix[i, j] > 0.8:
            similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that target the same class of proteins.
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
            same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that belong to different drug classes.
different_drug_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Drug classes'].iloc[i] != drug_data['Drug classes'].iloc[j]:
            different_drug_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Print the results.
print('Similar drug pairs:', similar_drug_pairs)
print('Same target class drug pairs:', same_target_class_drug_pairs)
print('Different drug class drug pairs:', different_drug_class_drug_pairs)
104/38:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        drug_similarity_matrix[i, j] = Chem.DataStructs.FingerprintSimilarity(Chem.MolFromSmiles(drug_data['Chemical structure'].iloc[i]), Chem.MolFromSmiles(drug_data['Chemical structure'].iloc[j]))

# Identify drug pairs that have similar structures.
similar_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_similarity_matrix[i, j] > 0.8:
            similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that target the same class of proteins.
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
            same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that belong to different drug classes.
different_drug_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Drug classes'].iloc[i] != drug_data['Drug classes'].iloc[j]:
            different_drug_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Print the results.
print('Similar drug pairs:', similar_drug_pairs)
print('Same target class drug pairs:', same_target_class_drug_pairs)
print('Different drug class drug pairs:', different_drug_class_drug_pairs)
104/39:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
import numpy as np

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
        drug_similarity_matrix[i, j] = similarity

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        drug_similarity_matrix[i, j] = Chem.DataStructs.FingerprintSimilarity(Chem.MolFromSmiles(drug_data['Chemical structure'].iloc[i]), Chem.MolFromSmiles(drug_data['Chemical structure'].iloc[j]))

# Identify drug pairs that have similar structures.
similar_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_similarity_matrix[i, j] > 0.8:
            similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that target the same class of proteins.
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
            same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that belong to different drug classes.
different_drug_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Drug classes'].iloc[i] != drug_data['Drug classes'].iloc[j]:
            different_drug_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Print the results.
print('Similar drug pairs:', similar_drug_pairs)
print('Same target class drug pairs:', same_target_class_drug_pairs)
print('Different drug class drug pairs:', different_drug_class_drug_pairs)
104/40:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import numpy as np

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
        drug_similarity_matrix[i, j] = similarity

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        drug_similarity_matrix[i, j] = Chem.DataStructs.FingerprintSimilarity(Chem.MolFromSmiles(drug_data['Chemical structure'].iloc[i]), Chem.MolFromSmiles(drug_data['Chemical structure'].iloc[j]))

# Identify drug pairs that have similar structures.
similar_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_similarity_matrix[i, j] > 0.8:
            similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that target the same class of proteins.
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
            same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that belong to different drug classes.
different_drug_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Drug classes'].iloc[i] != drug_data['Drug classes'].iloc[j]:
            different_drug_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Print the results.
print('Similar drug pairs:', similar_drug_pairs)
print('Same target class drug pairs:', same_target_class_drug_pairs)
print('Different drug class drug pairs:', different_drug_class_drug_pairs)
104/41:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import numpy as np

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
        drug_similarity_matrix[i, j] = similarity


# Identify drug pairs that have similar structures.
similar_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_similarity_matrix[i, j] > 0.8:
            similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that target the same class of proteins.
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
            same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that belong to different drug classes.
different_drug_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Drug classes'].iloc[i] != drug_data['Drug classes'].iloc[j]:
            different_drug_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Print the results.
print('Similar drug pairs:', similar_drug_pairs)
print('Same target class drug pairs:', same_target_class_drug_pairs)
print('Different drug class drug pairs:', different_drug_class_drug_pairs)
104/42:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import numpy as np

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
        drug_similarity_matrix[i, j] = similarity


# Identify drug pairs that have similar structures.
similar_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_similarity_matrix[i, j] > 0.8:
            similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that target the same class of proteins.
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
            same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that belong to different drug classes.
different_drug_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Drug classes'].iloc[i] != drug_data['Drug classes'].iloc[j]:
            different_drug_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Print the results.
print('Similar drug pairs:', similar_drug_pairs)
print('Same target class drug pairs:', same_target_class_drug_pairs)
print('Different drug class drug pairs:', different_drug_class_drug_pairs)
104/43:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import numpy as np

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
        drug_similarity_matrix[i, j] = similarity


# Identify drug pairs that have similar structures.
similar_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_similarity_matrix[i, j] > 0.8:
            similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that target the same class of proteins.
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
            same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that belong to different drug classes.
different_drug_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Drug classes'].iloc[i] != drug_data['Drug classes'].iloc[j]:
            different_drug_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Print the results.
print('Similar drug pairs:')
for drug_pair in similar_drug_pairs:
  print(drug_pair)

print('Same target class drug pairs:')
for drug_pair in same_target_class_drug_pairs:
  print(drug_pair)

print('Different drug class drug pairs:')
for drug_pair in different_drug_class_drug_pairs:
  print(drug_pair)
104/44:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import numpy as np

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
        drug_similarity_matrix[i, j] = similarity


# Identify drug pairs that have similar structures.
similar_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_similarity_matrix[i, j] > 0.8:
            similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that target the same class of proteins.
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
            same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Identify drug pairs that belong to different drug classes.
different_drug_class_drug_pairs = []
for i in range(len(drug_data)):
    for j in range(i + 1, len(drug_data)):
        if drug_data['Drug classes'].iloc[i] != drug_data['Drug classes'].iloc[j]:
            different_drug_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Print the results.
print('Similar drug pairs:')
for drug_pair in similar_drug_pairs:
  display(drug_pair)

print('Same target class drug pairs:')
for drug_pair in same_target_class_drug_pairs:
  display(drug_pair)

print('Different drug class drug pairs:')
for drug_pair in different_drug_class_drug_pairs:
  display(drug_pair)
106/1:
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from graphviz import Digraph
106/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')
display(df)
106/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
106/4:
data_types = df.dtypes
display(data_types)

unique_drug_names = df['Drug Name'].drop_duplicates()
print(unique_drug_names)
unique_drug_classes = df['Drug classes'].drop_duplicates()
print(unique_drug_classes)
106/5:
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the Data
large_dataset = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Step 2: Get Unique Drug Names
unique_drugs = large_dataset['Drug Name'].unique()

# Calculate the number of rows and columns needed
num_drugs = len(unique_drugs)
num_rows = (num_drugs + 1) // 2  # Ensure at least 1 row
num_cols = min(2, num_drugs)  # Maximum 2 columns

# Create a grid with num_rows rows and num_cols columns for the pie charts
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))

# Step 3: Generate a Pie Chart for Each Drug
for i, drug_name in enumerate(unique_drugs):
    drug_df = large_dataset[large_dataset['Drug Name'] == drug_name]

    # Get target class counts for the drug
    target_class_counts = drug_df['Target Class'].value_counts()

    # Generate a color map for the pie chart
    colors = plt.cm.Paired(range(len(target_class_counts)))

    # Calculate percentages
    total_count = target_class_counts.sum()
    percentages = (target_class_counts / total_count) * 100

    # Calculate the position in the grid
    row = i // 2
    col = i % 2 if num_drugs > 1 else 0

    # Create Pie Chart
    wedges, texts, autotexts = axes[row, col].pie(
        percentages,
        autopct='%1.1f%%',
        startangle=140,
        colors=colors,
    )
    axes[row, col].axis('equal')
    axes[row, col].set_title(f'{drug_name} - Target Class Distribution')

    # Create legends
    legend_labels = [f'{label} - {percent:.1f}%' for label, percent in zip(target_class_counts.index, percentages)]
    axes[row, col].legend(legend_labels, title="Target Class", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adjust layout
plt.tight_layout()

# Display all Pie Charts
plt.show()
106/6:
# Label Encoding

import pandas as pd
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Drug Name']= label_encoder.fit_transform(df['Drug Name']) 
df['Drug Name'].unique() 
display("Drug name:", df['Drug Name'].unique())
print("Drug name:", df['Drug Name'])


label_encoder = preprocessing.LabelEncoder() 
df['Drug classes']= label_encoder.fit_transform(df['Drug classes']) 
df['Drug classes'].unique() 
display("Drug classes:", df['Drug classes'].unique())
print("Drug classes:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target Class']= label_encoder.fit_transform(df['Target Class']) 
df['Target Class'].unique() 
display("Target Class:", df['Target Class'].unique())
print("Target Class:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Target']= label_encoder.fit_transform(df['Target']) 
df['Target'].unique() 
display("Target:", df['Target'].unique())
print("Target:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Common name']= label_encoder.fit_transform(df['Common name']) 
df['Common name'].unique() 
display("Common name:", df['Common name'].unique())
print("Common name:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Uniprot ID']= label_encoder.fit_transform(df['Uniprot ID']) 
df['Uniprot ID'].unique() 
display("Uniprot ID:", df['Uniprot ID'].unique())
print("Uniprot ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['ChEMBL ID']= label_encoder.fit_transform(df['ChEMBL ID']) 
df['ChEMBL ID'].unique() 
display("ChEMBL ID:", df['ChEMBL ID'].unique())
print("ChEMBL ID:", df['Drug classes'])

label_encoder = preprocessing.LabelEncoder() 
df['Chemical structure']= label_encoder.fit_transform(df['Chemical structure']) 
df['Chemical structure'].unique() 
display("Chemical structure:", df['Chemical structure'].unique())
print("Chemical structure:", df['Chemical structure'])
106/7:
# Splitting train and test set

X = df [['Drug Name', 'Drug classes', 'Target Class']]
y = df[['Chemical structure']]  

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train: ')
print(X_train.head())
print('X_test: ')
print(X_test.head())
print('y_train: ')
print(y_train.head())
print('y_test: ')
print(y_test.head())
106/8:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
106/9:
# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
    for j in range(len(drug_data)):
        drug_similarity_matrix[i, j] = Chem.DataStructs.FingerprintSimilarity(Chem.MolFromSmiles(drug_data['Chemical structure'].iloc[i]), Chem.MolFromSmiles(drug_data['Chemical structure'].iloc[j]))
106/10:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
  for j in range(len(drug_data)):
    similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
    drug_similarity_matrix[i, j] = similarity

# Identify drug pairs that target the same drug target class
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
  for j in range(i + 1, len(drug_data)):
    if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
      same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Calculate the synergistic, antagonistic, and agonism properties of the drug pairs
def calculate_synergistic_property(drug_pair):
  ...

def calculate_antagonistic_property(drug_pair):
  ...

def calculate_agonistic_property(drug_pair):
  ...

synergistic_properties = []
antagonistic_properties = []
agonistic_properties = []
for drug_pair in same_target_class_drug_pairs:
  synergistic_properties.append(calculate_synergistic_property(drug_pair))
  antagonistic_properties.append(calculate_antagonistic_property(drug_pair))
  agonistic_properties.append(calculate_agonistic_property(drug_pair))

# Train a random forest classifier to predict the synergistic, antagonistic, and agonism properties of drug pairs
clf = RandomForestClassifier()
clf.fit(drug_similarity_matrix, np.array([synergistic_properties, antagonistic_properties, agonistic_properties]).T)

# Identify the drug in the pie chart that has high synergistic, antagonism, and agonism properties
drug_in_pie_chart = 'Acetaminophen'

# Calculate the synergistic, antagonistic, and agonism properties of the drug in the pie chart with the other drugs in the dataset
drug_in_pie_chart_properties = clf.predict_proba(drug_similarity_matrix[drug_data['Drug Name'] == drug_in_pie_chart])[:, 1]

# Find the drug in the dataset that has the highest synergistic, antagonistic, and agonism properties with the drug in the pie chart
highest_synergistic_antagonistic_agonism_drug = drug_data.loc[np.argmax(drug_in_pie_chart_properties), 'Drug Name']

# Print the drug with the highest synergistic, antagonistic, and agonism properties
print(highest_synergistic_antagonistic_agonism_drug)
106/11:
from sklearn.compose import ColumnTransformer

# Define a transformer to impute missing values in the first feature
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

# Define a transformer to standardize the second feature
scaler = StandardScaler()

# Create a ColumnTransformer object to apply the transformers to the features
column_transformer = ColumnTransformer([('impute', imputer, [0]), ('scale', scaler, [1])])

# Transform the features
X_transformed = column_transformer.fit_transform(X)

# Train the random forest classifier on the transformed features
clf = RandomForestClassifier()
clf.fit(X_transformed, y)
106/12:
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

# Define a transformer to impute missing values in the first feature
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

# Define a transformer to standardize the second feature
scaler = StandardScaler()

# Create a ColumnTransformer object to apply the transformers to the features
column_transformer = ColumnTransformer([('impute', imputer, [0]), ('scale', scaler, [1])])

# Transform the features
X_transformed = column_transformer.fit_transform(X)

# Train the random forest classifier on the transformed features
clf = RandomForestClassifier()
clf.fit(X_transformed, y)
106/13:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
  for j in range(len(drug_data)):
    similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
    drug_similarity_matrix[i, j] = similarity

# Identify drug pairs that target the same drug target class
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
  for j in range(i + 1, len(drug_data)):
    if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
      same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Calculate the synergistic, antagonistic, and agonism properties of the drug pairs
def calculate_synergistic_property(drug_pair):
  ...

def calculate_antagonistic_property(drug_pair):
  ...

def calculate_agonistic_property(drug_pair):
  ...

synergistic_properties = []
antagonistic_properties = []
agonistic_properties = []
for drug_pair in same_target_class_drug_pairs:
  synergistic_properties.append(calculate_synergistic_property(drug_pair))
  antagonistic_properties.append(calculate_antagonistic_property(drug_pair))
  agonistic_properties.append(calculate_agonistic_property(drug_pair))

# Train a random forest classifier to predict the synergistic, antagonistic, and agonism properties of drug pairs
clf = RandomForestClassifier()
clf.fit(drug_similarity_matrix, np.array([synergistic_properties, antagonistic_properties, agonistic_properties]).T)

# Identify the drug in the pie chart that has high synergistic, antagonism, and agonism properties
drug_in_pie_chart = 'Acetaminophen'

# Calculate the synergistic, antagonistic, and agonism properties of the drug in the pie chart with the other drugs in the dataset
drug_in_pie_chart_properties = clf.predict_proba(drug_similarity_matrix[drug_data['Drug Name'] == drug_in_pie_chart])[:, 1]

# Find the drug in the dataset that has the highest synergistic, antagonistic, and agonism properties with the drug in the pie chart
highest_synergistic_antagonistic_agonism_drug = drug_data.loc[np.argmax(drug_in_pie_chart_properties), 'Drug Name']

# Print the drug with the highest synergistic, antagonistic, and agonism properties
print(highest_synergistic_antagonistic_agonism_drug)
106/14:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import numpy as np
from sklearn.ensemble import RandomForestClassifier



# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
  for j in range(len(drug_data)):
    similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
    drug_similarity_matrix[i, j] = similarity

# Identify drug pairs that target the same drug target class
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
  for j in range(i + 1, len(drug_data)):
    if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
      same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Calculate the synergistic, antagonistic, and agonism properties of the drug pairs
def calculate_synergistic_property(drug_pair):
  ...

def calculate_antagonistic_property(drug_pair):
  ...

def calculate_agonistic_property(drug_pair):
  ...

synergistic_properties = []
antagonistic_properties = []
agonistic_properties = []
for drug_pair in same_target_class_drug_pairs:
  synergistic_properties.append(calculate_synergistic_property(drug_pair))
  antagonistic_properties.append(calculate_antagonistic_property(drug_pair))
  agonistic_properties.append(calculate_agonistic_property(drug_pair))

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

# Define a transformer to impute missing values in the first feature
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

# Define a transformer to standardize the second feature
scaler = StandardScaler()

# Create a ColumnTransformer object to apply the transformers to the features
column_transformer = ColumnTransformer([('impute', imputer, [0]), ('scale', scaler, [1])])

# Transform the features
X_transformed = column_transformer.fit_transform(X)

# Train the random forest classifier on the transformed features
clf = RandomForestClassifier()
clf.fit(X_transformed, y)
clf.fit(drug_similarity_matrix, np.array([synergistic_properties, antagonistic_properties, agonistic_properties]).T)

# Identify the drug in the pie chart that has high synergistic, antagonism, and agonism properties
drug_in_pie_chart = 'Acetaminophen'

# Calculate the synergistic, antagonistic, and agonism properties of the drug in the pie chart with the other drugs in the dataset
drug_in_pie_chart_properties = clf.predict_proba(drug_similarity_matrix[drug_data['Drug Name'] == drug_in_pie_chart])[:, 1]

# Find the drug in the dataset that has the highest synergistic, antagonistic, and agonism properties with the drug in the pie chart
highest_synergistic_antagonistic_agonism_drug = drug_data.loc[np.argmax(drug_in_pie_chart_properties), 'Drug Name']

# Print the drug with the highest synergistic, antagonistic, and agonism properties
print(highest_synergistic_antagonistic_agonism_drug)
106/15:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import numpy as np
from sklearn.ensemble import RandomForestClassifier



# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
  for j in range(len(drug_data)):
    similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
    drug_similarity_matrix[i, j] = similarity

# Identify drug pairs that target the same drug target class
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
  for j in range(i + 1, len(drug_data)):
    if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
      same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Calculate the synergistic, antagonistic, and agonism properties of the drug pairs
def calculate_synergistic_property(drug_pair):
  ...

def calculate_antagonistic_property(drug_pair):
  ...

def calculate_agonistic_property(drug_pair):
  ...

synergistic_properties = []
antagonistic_properties = []
agonistic_properties = []
for drug_pair in same_target_class_drug_pairs:
  synergistic_properties.append(calculate_synergistic_property(drug_pair))
  antagonistic_properties.append(calculate_antagonistic_property(drug_pair))
  agonistic_properties.append(calculate_agonistic_property(drug_pair))

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

# Define a transformer to impute missing values in the first feature
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

# Define a transformer to standardize the second feature
scaler = StandardScaler()

# Create a ColumnTransformer object to apply the transformers to the features
column_transformer = ColumnTransformer([('impute', imputer, [0]), ('scale', scaler, [1])])

# Transform the features
X_transformed = column_transformer.fit_transform(X)

# Train the random forest classifier on the transformed features
clf = RandomForestClassifier()
clf.fit(drug_similarity_matrix, np.array([synergistic_properties, antagonistic_properties, agonistic_properties]).T)


# Identify the drug in the pie chart that has high synergistic, antagonism, and agonism properties
drug_in_pie_chart = 'Acetaminophen'

# Calculate the synergistic, antagonistic, and agonism properties of the drug in the pie chart with the other drugs in the dataset
drug_in_pie_chart_properties = clf.predict_proba(drug_similarity_matrix[drug_data['Drug Name'] == drug_in_pie_chart])[:, 1]

# Find the drug in the dataset that has the highest synergistic, antagonistic, and agonism properties with the drug in the pie chart
highest_synergistic_antagonistic_agonism_drug = drug_data.loc[np.argmax(drug_in_pie_chart_properties), 'Drug Name']

# Print the drug with the highest synergistic, antagonistic, and agonism properties
print(highest_synergistic_antagonistic_agonism_drug)
106/16:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import numpy as np
from sklearn.ensemble import RandomForestClassifier



# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
  for j in range(len(drug_data)):
    similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
    drug_similarity_matrix[i, j] = similarity

# Identify drug pairs that target the same drug target class
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
  for j in range(i + 1, len(drug_data)):
    if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
      same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Calculate the synergistic, antagonistic, and agonism properties of the drug pairs
def calculate_synergistic_property(drug_pair):
  ...

def calculate_antagonistic_property(drug_pair):
  ...

def calculate_agonistic_property(drug_pair):
  ...

synergistic_properties = []
antagonistic_properties = []
agonistic_properties = []
for drug_pair in same_target_class_drug_pairs:
  synergistic_properties.append(calculate_synergistic_property(drug_pair))
  antagonistic_properties.append(calculate_antagonistic_property(drug_pair))
  agonistic_properties.append(calculate_agonistic_property(drug_pair))

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

# Define a transformer to impute missing values in the first feature
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

# Define a transformer to standardize the second feature
scaler = StandardScaler()

# Create a ColumnTransformer object to apply the transformers to the features
column_transformer = ColumnTransformer([('impute', imputer, [0]), ('scale', scaler, [1])])

# Transform the features
X_transformed = column_transformer.fit_transform(X)

# Train the random forest classifier on the transformed features
clf = RandomForestClassifier()
clf.fit(drug_similarity_matrix, np.array([synergistic_properties).T)


# Identify the drug in the pie chart that has high synergistic, antagonism, and agonism properties
drug_in_pie_chart = 'Acetaminophen'

# Calculate the synergistic, antagonistic, and agonism properties of the drug in the pie chart with the other drugs in the dataset
drug_in_pie_chart_properties = clf.predict_proba(drug_similarity_matrix[drug_data['Drug Name'] == drug_in_pie_chart])[:, 1]

# Find the drug in the dataset that has the highest synergistic, antagonistic, and agonism properties with the drug in the pie chart
highest_synergistic_antagonistic_agonism_drug = drug_data.loc[np.argmax(drug_in_pie_chart_properties), 'Drug Name']

# Print the drug with the highest synergistic, antagonistic, and agonism properties
print(highest_synergistic_antagonistic_agonism_drug)
106/17:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import numpy as np
from sklearn.ensemble import RandomForestClassifier



# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the chemical structures of the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
  for j in range(len(drug_data)):
    similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
    drug_similarity_matrix[i, j] = similarity

# Identify drug pairs that target the same drug target class
same_target_class_drug_pairs = []
for i in range(len(drug_data)):
  for j in range(i + 1, len(drug_data)):
    if drug_data['Target Class'].iloc[i] == drug_data['Target Class'].iloc[j]:
      same_target_class_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Calculate the synergistic, antagonistic, and agonism properties of the drug pairs
def calculate_synergistic_property(drug_pair):
  ...

def calculate_antagonistic_property(drug_pair):
  ...

def calculate_agonistic_property(drug_pair):
  ...

synergistic_properties = []
antagonistic_properties = []
agonistic_properties = []
for drug_pair in same_target_class_drug_pairs:
  synergistic_properties.append(calculate_synergistic_property(drug_pair))
  antagonistic_properties.append(calculate_antagonistic_property(drug_pair))
  agonistic_properties.append(calculate_agonistic_property(drug_pair))

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

# Define a transformer to impute missing values in the first feature
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

# Define a transformer to standardize the second feature
scaler = StandardScaler()

# Create a ColumnTransformer object to apply the transformers to the features
column_transformer = ColumnTransformer([('impute', imputer, [0]), ('scale', scaler, [1])])

# Transform the features
X_transformed = column_transformer.fit_transform(X)

# Train the random forest classifier on the transformed features
clf = RandomForestClassifier()
clf.fit(drug_similarity_matrix, np.array([synergistic_properties]).T)



# Identify the drug in the pie chart that has high synergistic, antagonism, and agonism properties
drug_in_pie_chart = 'Acetaminophen'

# Calculate the synergistic, antagonistic, and agonism properties of the drug in the pie chart with the other drugs in the dataset
drug_in_pie_chart_properties = clf.predict_proba(drug_similarity_matrix[drug_data['Drug Name'] == drug_in_pie_chart])[:, 1]

# Find the drug in the dataset that has the highest synergistic, antagonistic, and agonism properties with the drug in the pie chart
highest_synergistic_antagonistic_agonism_drug = drug_data.loc[np.argmax(drug_in_pie_chart_properties), 'Drug Name']

# Print the drug with the highest synergistic, antagonistic, and agonism properties
print(highest_synergistic_antagonistic_agonism_drug)
106/18:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
from sklearn.ensemble import RandomForestClassifier

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs.
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
  for j in range(len(drug_data)):
    similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
    drug_similarity_matrix[i, j] = similarity

# Identify drug pairs that are similar in terms of their function.
similar_drug_pairs = []
for i in range(len(drug_data)):
  for j in range(i + 1, len(drug_data)):
    if drug_similarity_matrix[i, j] > 0.8:
      similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Load the known DDIs.
known_ddis = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\known_ddis.csv')

# Create a label vector for the drug pairs, indicating whether or not there is a known DDI.
labels = []
for drug_pair in similar_drug_pairs:
  if drug_pair in known_ddis['Drug Pair'].tolist():
    labels.append(1)
  else:
    labels.append(0)

# Split the data into training and test sets.
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(drug_similarity_matrix, labels, test_size=0.25, random_state=42)

# Train a random forest classifier to predict DDIs.
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Predict DDIs for the test set.
y_pred = clf.predict(X_test)

# Calculate the accuracy of the model.
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
106/19:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
from sklearn.ensemble import RandomForestClassifier

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs.
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
  for j in range(len(drug_data)):
    similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
    drug_similarity_matrix[i, j] = similarity

# Identify drug pairs that are similar in terms of their function.
similar_drug_pairs = []
for i in range(len(drug_data)):
  for j in range(i + 1, len(drug_data)):
    if drug_similarity_matrix[i, j] > 0.8:
      similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Load the known DDIs.
known_ddis = pd.read_csv(r"C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\drug ddi.xlsx")

# Create a label vector for the drug pairs, indicating whether or not there is a known DDI.
labels = []
for drug_pair in similar_drug_pairs:
  if drug_pair in known_ddis['Drug Pair'].tolist():
    labels.append(1)
  else:
    labels.append(0)

# Split the data into training and test sets.
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(drug_similarity_matrix, labels, test_size=0.25, random_state=42)

# Train a random forest classifier to predict DDIs.
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Predict DDIs for the test set.
y_pred = clf.predict(X_test)

# Calculate the accuracy of the model.
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
106/20:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
from sklearn.ensemble import RandomForestClassifier

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs.
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
  for j in range(len(drug_data)):
    similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
    drug_similarity_matrix[i, j] = similarity

# Identify drug pairs that are similar in terms of their function.
similar_drug_pairs = []
for i in range(len(drug_data)):
  for j in range(i + 1, len(drug_data)):
    if drug_similarity_matrix[i, j] > 0.8:
      similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Load the known DDIs.
known_ddis = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\drug ddi.xlsx')

# Create a label vector for the drug pairs, indicating whether or not there is a known DDI.
labels = []
for drug_pair in similar_drug_pairs:
  if drug_pair in known_ddis['Drug Pair'].tolist():
    labels.append(1)
  else:
    labels.append(0)

# Split the data into training and test sets.
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(drug_similarity_matrix, labels, test_size=0.25, random_state=42)

# Train a random forest classifier to predict DDIs.
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Predict DDIs for the test set.
y_pred = clf.predict(X_test)

# Calculate the accuracy of the model.
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
106/21:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
from sklearn.ensemble import RandomForestClassifier

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs.
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
  for j in range(len(drug_data)):
    similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
    drug_similarity_matrix[i, j] = similarity

# Identify drug pairs that are similar in terms of their function.
similar_drug_pairs = []
for i in range(len(drug_data)):
  for j in range(i + 1, len(drug_data)):
    if drug_similarity_matrix[i, j] > 0.8:
      similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Load the known DDIs.
known_ddis = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\drug ddi.xlsx"C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\drug ddi.csv')

# Create a label vector for the drug pairs, indicating whether or not there is a known DDI.
labels = []
for drug_pair in similar_drug_pairs:
  if drug_pair in known_ddis['Drug Pair'].tolist():
    labels.append(1)
  else:
    labels.append(0)

# Split the data into training and test sets.
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(drug_similarity_matrix, labels, test_size=0.25, random_state=42)

# Train a random forest classifier to predict DDIs.
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Predict DDIs for the test set.
y_pred = clf.predict(X_test)

# Calculate the accuracy of the model.
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
106/22:
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
from sklearn.ensemble import RandomForestClassifier

# Load the drug data into a Pandas DataFrame.
drug_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\20 drug dataset for combination.csv')

# Generate fingerprints for the drugs.
fingerprints = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in drug_data['Chemical structure']]

# Calculate the similarity between the drugs.
drug_similarity_matrix = np.zeros((len(drug_data), len(drug_data)))
for i in range(len(drug_data)):
  for j in range(len(drug_data)):
    similarity = DataStructs.DiceSimilarity(fingerprints[i], fingerprints[j])
    drug_similarity_matrix[i, j] = similarity

# Identify drug pairs that are similar in terms of their function.
similar_drug_pairs = []
for i in range(len(drug_data)):
  for j in range(i + 1, len(drug_data)):
    if drug_similarity_matrix[i, j] > 0.8:
      similar_drug_pairs.append((drug_data['Drug Name'].iloc[i], drug_data['Drug Name'].iloc[j]))

# Load the known DDIs.
known_ddis = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\drug ddi.xlsx"C:\Users\sound\OneDrive\Desktop\main project\python project main\drug combination project\drug ddi.csv')

# Create a label vector for the drug pairs, indicating whether or not there is a known DDI.
labels = []
for drug_pair in similar_drug_pairs:
  if drug_pair in known_ddis['Drug Pair'].tolist():
    labels.append(1)
  else:
    labels.append(0)

# Split the data into training and test sets.
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(drug_similarity_matrix, labels, test_size=0.25, random_state=42)

# Train a random forest classifier to predict DDIs.
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Predict DDIs for the test set.
y_pred = clf.predict(X_test)

# Calculate the accuracy of the model.
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
106/23:
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Load the SMILES strings
smiles_strings = df['Chemical structure']

# Extract features from the SMILES strings
vectorizer = TfidfVectorizer(tokenizer=re.compile(r"[\w]+"))
X = vectorizer.fit_transform(smiles_strings)

# Load the biological activity labels
biological_activity_labels = [1, 0, 0]

# Train a machine learning model
model = LogisticRegression()
model.fit(X, biological_activity_labels)

# Make predictions
predictions = model.predict_proba(X)

# Print the predictions
for i in range(len(smiles_strings)):
    print(f"SMILES string: {smiles_strings[i]}")
    print(f"Predicted biological activity: {predictions[i][1]}")
106/24:
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Load the SMILES strings
smiles_strings = df['Chemical structure']

# Define a function to tokenize the SMILES strings
def tokenize_smiles(smiles_string):
    return smiles_string.split()

# Create a TfidfVectorizer object using the custom tokenizer
vectorizer = TfidfVectorizer(tokenizer=tokenize_smiles)

# Extract features from the SMILES strings
X = vectorizer.fit_transform(smiles_strings)
106/25:
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Load the SMILES strings
smiles_strings = df['Chemical structure']

# Create a TfidfVectorizer object without passing in a tokenizer
vectorizer = TfidfVectorizer(tokenizer=None)

# Extract features from the SMILES strings
X = vectorizer.fit_transform(smiles_strings)
106/26:
import re
from sklearn.feature_extraction.text import TfidfVectorizer

# Filter out non-string elements
smiles_strings = [s for s in df['Chemical structure'] if isinstance(s, str)]

# Create a TfidfVectorizer object without passing in a tokenizer
vectorizer = TfidfVectorizer(tokenizer=None)

# Extract features from the SMILES strings
X = vectorizer.fit_transform(smiles_strings)
106/27:
import re
from sklearn.feature_extraction.text import TfidfVectorizer

# Filter out non-string elements
smiles_strings = [s for s in df['Chemical structure'] if isinstance(s, str)]

def custom_tokenizer(text):
    return text.split()  # Modify this based on your specific requirements

vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)
X = vectorizer.fit_transform(smiles_strings)
106/28:
import rdkit
import numpy as np

def extract_features_from_smiles(smiles_string):
  """Extracts features from a SMILES string in chiral SMILES format.

  Args:
    smiles_string: A SMILES string in chiral SMILES format.

  Returns:
    A numpy array containing the extracted features.
  """

  # Convert the SMILES string to a RDKit molecule object.
  mol = rdkit.Chem.MolFromSmiles(smiles_string)

  # Extract the types of atoms and bonds in the molecule.
  atom_types = []
  bond_types = []
  for atom in mol.GetAtoms():
    atom_types.append(atom.GetSymbol())
  for bond in mol.GetBonds():
    bond_types.append(bond.GetType())

  # Extract the branching of the molecule.
  branching = rdkit.Chem.GraphDescriptors.GetBranchingFactor(mol)

  # Extract the presence of functional groups in the molecule.
  functional_groups = []
  for functional_group in ['ester', 'hydroxyl']:
    functional_groups.append(rdkit.Chem.rdchem.HasSubstructMatch(mol, rdkit.Chem.MolFromSmiles(functional_group)))

  # Convert the extracted features to a numpy array.
  features = np.array([atom_types, bond_types, branching, functional_groups])

  return features
106/29:
import rdkit
import numpy as np

def extract_features_from_smiles(smiles_string):
  """Extracts features from a SMILES string in chiral SMILES format.

  Args:
    smiles_string: A SMILES string in chiral SMILES format.

  Returns:
    A numpy array containing the extracted features.
  """

  # Convert the SMILES string to a RDKit molecule object.
  mol = rdkit.Chem.MolFromSmiles(smiles_string)

  # Extract the types of atoms and bonds in the molecule.
  atom_types = []
  bond_types = []
  for atom in mol.GetAtoms():
    atom_types.append(atom.GetSymbol())
  for bond in mol.GetBonds():
    bond_types.append(bond.GetType())

  # Extract the branching of the molecule.
  branching = rdkit.Chem.GraphDescriptors.GetBranchingFactor(mol)

  # Extract the presence of functional groups in the molecule.
  functional_groups = []
  for functional_group in ['ester', 'hydroxyl']:
    functional_groups.append(rdkit.Chem.rdchem.HasSubstructMatch(mol, rdkit.Chem.MolFromSmiles(functional_group)))

  # Convert the extracted features to a numpy array.
  features = np.array([atom_types, bond_types, branching, functional_groups])

  return features


# Example usage:

smiles_string = df['Chemical structure']
features = extract_features_from_smiles(smiles_string)

print(features)
106/30:
from termios import NCC
import rdkit
import numpy as np

def extract_features_from_smiles(smiles_string):
  """Extracts features from a SMILES string in chiral SMILES format.

  Args:
    smiles_string: A SMILES string in chiral SMILES format.

  Returns:
    A numpy array containing the extracted features.
  """

  # Convert the SMILES string to a RDKit molecule object.
  mol = rdkit.Chem.MolFromSmiles(smiles_string)

  # Extract the types of atoms and bonds in the molecule.
  atom_types = []
  bond_types = []
  for atom in mol.GetAtoms():
    atom_types.append(atom.GetSymbol())
  for bond in mol.GetBonds():
    bond_types.append(bond.GetType())

  # Extract the branching of the molecule.
  branching = rdkit.Chem.GraphDescriptors.GetBranchingFactor(mol)

  # Extract the presence of functional groups in the molecule.
  functional_groups = []
  for functional_group in ['ester', 'hydroxyl']:
    functional_groups.append(rdkit.Chem.rdchem.HasSubstructMatch(mol, rdkit.Chem.MolFromSmiles(functional_group)))

  # Convert the extracted features to a numpy array.
  features = np.array([atom_types, bond_types, branching, functional_groups])

  return features

smiles_string = CC(C)NCC(O)COC1=CC=C(CC(N)=O)C=C1
features = extract_features_from_smiles(smiles_string)

print(features)
106/31:
import termios
import rdkit
import numpy as np

from termios import NCC

def extract_features_from_smiles(smiles_string):
  """Extracts features from a SMILES string in chiral SMILES format.

  Args:
    smiles_string: A SMILES string in chiral SMILES format.

  Returns:
    A numpy array containing the extracted features.
  """

  # Convert the SMILES string to a RDKit molecule object.
  mol = rdkit.Chem.MolFromSmiles(smiles_string)

  # Extract the types of atoms and bonds in the molecule.
  atom_types = []
  bond_types = []
  for atom in mol.GetAtoms():
    atom_types.append(atom.GetSymbol())
  for bond in mol.GetBonds():
    bond_types.append(bond.GetType())

  # Extract the branching of the molecule.
  branching = rdkit.Chem.GraphDescriptors.GetBranchingFactor(mol)

  # Extract the presence of functional groups in the molecule.
  functional_groups = []
  for functional_group in ['ester', 'hydroxyl']:
    functional_groups.append(rdkit.Chem.rdchem.HasSubstructMatch(mol, rdkit.Chem.MolFromSmiles(functional_group)))

  # Convert the extracted features to a numpy array.
  features = np.array([atom_types, bond_types, branching, functional_groups])

  return features

# Example usage:

smiles_string = CC(C)NCC(O)COC1=CC=C(CC(N)=O)C=C1
features = extract_features_from_smiles(smiles_string)

print(features)
106/32:
import termios
import rdkit
import numpy as np

from termios import NCC

def extract_features_from_smiles(smiles_string):
  """Extracts features from a SMILES string in chiral SMILES format.

  Args:
    smiles_string: A SMILES string in chiral SMILES format.

  Returns:
    A numpy array containing the extracted features.
  """

  # Convert the SMILES string to a RDKit molecule object.
  mol = rdkit.Chem.MolFromSmiles(smiles_string)

  # Extract the types of atoms and bonds in the molecule.
  atom_types = ()
  bond_types = ()
  for atom in mol.GetAtoms():
    atom_types.append(atom.GetSymbol())
  for bond in mol.GetBonds():
    bond_types.append(bond.GetType())

  # Extract the branching of the molecule.
  branching = rdkit.Chem.GraphDescriptors.GetBranchingFactor(mol)

  # Extract the presence of functional groups in the molecule.
  functional_groups = []
  for functional_group in ['ester', 'hydroxyl']:
    functional_groups.append(rdkit.Chem.rdchem.HasSubstructMatch(mol, rdkit.Chem.MolFromSmiles(functional_group)))

  # Convert the extracted features to a numpy array.
  features = np.array([atom_types, bond_types, branching, functional_groups])

  return features

# Example usage:

smiles_string = CC(C)NCC(O)COC1=CC=C(CC(N)=O)C=C1
features = extract_features_from_smiles(smiles_string)

print(features)
106/33:
import rdkit
import numpy as np

def extract_features_from_smiles(smiles_string):
  """Extracts features from a SMILES string in chiral SMILES format.

  Args:
    smiles_string: A SMILES string in chiral SMILES format.

  Returns:
    A numpy array containing the extracted features.
  """

  # Convert the SMILES string to a RDKit molecule object.
  mol = rdkit.Chem.MolFromSmiles(smiles_string)

  # Extract the types of atoms and bonds in the molecule.
  atom_types = []
  bond_types = []
  for atom in mol.GetAtoms():
    atom_types.append(atom.GetSymbol())
  for bond in mol.GetBonds():
    bond_types.append(bond.GetType())

  # Extract the branching of the molecule.
  branching = rdkit.Chem.GraphDescriptors.GetBranchingFactor(mol)

  # Extract the presence of functional groups in the molecule.
  functional_groups = []
  for functional_group in ['ester', 'hydroxyl']:
    functional_groups.append(rdkit.Chem.rdchem.HasSubstructMatch(mol, rdkit.Chem.MolFromSmiles(functional_group)))

  # Convert the extracted features to a numpy array.
  features = np.array([atom_types, bond_types, branching, functional_groups])

  return features

# Example usage:

smiles_string = '[H][C@@]12CC[C@@]3(CCC(=O)O3)[C@@]1(C)C[C@H]1O'
features = extract_features_from_smiles(smiles_string)

print(features)
106/34:
import rdkit
import numpy as np

def extract_features_from_smiles(smiles_string):
  """Extracts features from a SMILES string in chiral SMILES format.

  Args:
    smiles_string: A SMILES string in chiral SMILES format.

  Returns:
    A numpy array containing the extracted features.
  """

  # Convert the SMILES string to a RDKit molecule object.
  mol = rdkit.Chem.MolFromSmiles(smiles_string)

  # Extract the types of atoms and bonds in the molecule.
  atom_types = []
  bond_types = []
  for atom in mol.GetAtoms():
    atom_types.append(atom.GetSymbol())
  for bond in mol.GetBonds():
    bond_types.append(bond.GetType())

  # Extract the branching of the molecule.
  branching = rdkit.Chem.GraphDescriptors.GetBranchingFactor(mol)

  # Extract the presence of functional groups in the molecule.
  functional_groups = []
  for functional_group in ['ester', 'hydroxyl']:
    functional_groups.append(rdkit.Chem.rdchem.HasSubstructMatch(mol, rdkit.Chem.MolFromSmiles(functional_group)))

  # Convert the extracted features to a numpy array.
  features = np.array([atom_types, bond_types, branching, functional_groups])

  return features

# Example usage:

smiles_string = ('[H][C@@]12CC[C@@]3(CCC(=O)O3)[C@@]1(C)C[C@H]1O')
features = extract_features_from_smiles(smiles_string)

print(features)
106/35:
import rdkit

def is_valid_smiles(smiles_string):
  """Returns True if the SMILES string is valid, False otherwise."""

  try:
    rdkit.Chem.MolFromSmiles(smiles_string)
    return True
  except:
    return False

# Example usage:

smiles_string = '[H][C@@]12CC[C@@]3(CCC(=O)O3)[C@@]1(C)C[C@H]1O'

if is_valid_smiles(smiles_string):
  features = extract_features_from_smiles(smiles_string)
else:
  print("The SMILES string is invalid.")
106/36:
import rdkit

def is_valid_smiles(smiles_string):
  """Returns True if the SMILES string is valid, False otherwise."""

  try:
    rdkit.Chem.MolFromSmiles(smiles_string)
    return True
  except:
    return False

# Example usage:

smiles_string = ('[H]C12CCC3(CCC(=O)O3)[C@@]1(C)CC[C@H]2O')
mol = rdkit.Chem.MolFromSmiles(smiles_string)
if mol is None:
    print(f"Failed to parse SMILES string: {smiles_string}")
else:
    features = extract_features_from_smiles(smiles_string)
    print(features)
106/37:
import rdkit
import numpy as np

def extract_features_from_smiles(smiles_string):
    """Extracts features from a SMILES string in chiral SMILES format."""
    
    # Convert the SMILES string to a RDKit molecule object.
    mol = rdkit.Chem.MolFromSmiles(smiles_string)

    if mol is None:
        return None

    # Extract the types of atoms and bonds in the molecule.
    atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]
    bond_types = [bond.GetBondTypeAsDouble() for bond in mol.GetBonds()]

    # Extract the branching of the molecule.
    branching = rdkit.Chem.GraphDescriptors.GetBranchingFactor(mol)

    # Extract the presence of functional groups in the molecule.
    functional_groups = []
    for functional_group in ['ester', 'hydroxyl']:
        functional_groups.append(rdkit.Chem.rdchem.HasSubstructMatch(mol, rdkit.Chem.MolFromSmiles(functional_group)))

    # Convert the extracted features to a numpy array.
    features = np.array([atom_types, bond_types, branching, functional_groups])

    return features

# Example usage:

smiles_string = ('[H]C12CCC3(CCC(=O)O3)[C@@]1(C)CC[C@H]2O')
mol = rdkit.Chem.MolFromSmiles(smiles_string)

if mol is None:
    print(f"Failed to parse SMILES string: {smiles_string}")
else:
    features = extract_features_from_smiles(smiles_string)
    print(features)
106/38:
import rdkit
import numpy as np

def extract_features_from_smiles(smiles_string):
    """Extracts features from a SMILES string in chiral SMILES format."""
    
    # Convert the SMILES string to a RDKit molecule object.
    mol = rdkit.Chem.MolFromSmiles(smiles_string)

    if mol is None:
        return None

    # Extract the types of atoms and bonds in the molecule.
    atom_types = [atom.GetSymbol() for atom in mol.GetAtoms()]
    bond_types = [bond.GetBondTypeAsDouble() for bond in mol.GetBonds()]

    # Calculate the branching factor of the molecule.
    num_branches = sum(1 for atom in mol.GetAtoms() if atom.GetDegree() > 1)
    branching = num_branches / len(atom_types)

    # Extract the presence of functional groups in the molecule.
    functional_groups = []
    for functional_group in ['ester', 'hydroxyl']:
        functional_groups.append(rdkit.Chem.rdchem.HasSubstructMatch(mol, rdkit.Chem.MolFromSmiles(functional_group)))

    # Convert the extracted features to a numpy array.
    features = np.array([atom_types, bond_types, branching, functional_groups])

    return features

# Example usage:

smiles_string = ('[H]C12CCC3(CCC(=O)O3)[C@@]1(C)CC[C@H]2O')
mol = rdkit.Chem.MolFromSmiles(smiles_string)

if mol is None:
    print(f"Failed to parse SMILES string: {smiles_string}")
else:
    features = extract_features_from_smiles(smiles_string)
    print(features)
107/1:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
107/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
107/3:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
107/4:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
# display(Name_df)
# display(Age_df)
display(Churn_df)
107/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
107/6: df.describe(include="all")
107/7: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
107/8:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
107/9:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
107/10:
ax = df['Subscription_Length_Months'].value_counts().plot(kind = 'bar',rot = 0, width = 0.3)
ax.set_ylabel('# of Customers')
ax.set_title('# of Customers by Subscription_Length_Months')
107/11:
import matplotlib.ticker as mtick

colors = ['#4D3425','#E4512B']
ax = (df['Churn'].value_counts()*100.0 /len(df)).plot(kind='bar',
                                                   stacked = True,
                                                   rot = 0,
                                                   color = colors,
                                                   figsize = (8,6))
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
ax.set_ylabel('% Customers', size = 14)
ax.set_xlabel('Churn', size = 14)
ax.set_title('Churn Rate', size = 14)

# create a list to collect the plt.patches data
totals = []

# find the values and append to list
for i in ax.patches:
    totals.append(i.get_width())

# set individual bar labels using above list
total = sum(totals)
for i in ax.patches:
    # get_width pulls left or right; get_y pushes up or down
    ax.text(i.get_x() + 0.15, i.get_height() - 4.0, \
            str(round((i.get_height() / total), 1)) + '%',
            fontsize=12,
            color='white',
            weight='bold')

plt.show()
107/12:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
107/13:
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Assuming you have a DataFrame 'df' with features and target variable 'churn'

# Step 1: Prepare the data
# Assuming 'df' has features (X) and target variable 'churn' (y)
X = df[['CustomerID', 'Name', 'Subscription_Length_Months']]
y = df['Churn']

# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Create and train the Decision Tree model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

# Step 5: Evaluate the model
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)

print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")
107/14:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
107/15:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
107/16:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Assuming you have X_train, y_train defined from previous code

# Define the hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the Grid Search object
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,  # Number of cross-validation folds
    scoring='accuracy',  # Evaluation metric
    verbose=1  # Output progress
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)

print(f"Testing Accuracy with Best Model: {train_accuracy}")
print(f"Testing Accuracy with Best Model: {test_accuracy}")
107/17:
# Running logistic regression model
from asyncio import log
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error


lr = LogisticRegression()
result = lr.fit(X_train, y_train)
from sklearn import metrics
prediction_test = lr.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(lr, X, y, cv=kf, scoring='accuracy')

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = lr.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = lr.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)


print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
107/18:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Create and train the model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate on training set
train_preds = model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_preds)

# Evaluate on testing set
test_preds = model.predict(X_test)
test_accuracy = accuracy_score(y_test, test_preds)

print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

feature_importance = model.feature_importances_
# Assign the feature names to the importance values
feature_names = df.columns
feature_importance_dict = dict(zip(feature_names, feature_importance))
print(feature_importance_dict)
107/19:
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)
)

train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)

plt.plot(train_sizes, train_scores_mean, label='Training Accuracy')
plt.plot(train_sizes, test_scores_mean, label='Testing Accuracy')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
107/20:
from sklearn.ensemble import RandomForestClassifier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)
rf = RandomForestClassifier(n_estimators=1000, oob_score=True, n_jobs=-1,
                                  random_state=50, max_features="sqrt",
                                  max_leaf_nodes=30)

rf.fit(X_train, y_train)

# Make predictions
prediction_test = rf.predict(X_test)
print (metrics.accuracy_score(y_test, prediction_test))


# Perform k-fold cross-validation
cv_scores = cross_val_score(rf, X, y, cv=kf, scoring='accuracy')  # Use an appropriate scoring metric

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = rf.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = rf.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
107/21:
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Step 1: Define the base models
base_models = [
    ('lr', LogisticRegression(max_iter=1000)),
    ('dt', DecisionTreeClassifier()),
    ('rf', RandomForestClassifier(n_estimators=100))
]

# Step 2: Define the meta-model (Gradient Boosting)
meta_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Step 3: Create the StackingClassifier
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)

# Step 4: Train the StackingClassifier
stacking_model.fit(X_train, y_train)

# Step 5: Evaluate the model
y_pred = stacking_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
107/22:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Assuming you have X_train, y_train defined from previous code

# Define the hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the Grid Search object
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,  # Number of cross-validation folds
    scoring='accuracy',  # Evaluation metric
    verbose=1  # Output progress
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_test = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Testing Accuracy with Best Model: {test_accuracy}")
107/23:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Assuming you have X_train, y_train defined from previous code

# Define the hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the Grid Search object
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,  # Number of cross-validation folds
    scoring='accuracy',  # Evaluation metric
    verbose=1  # Output progress
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_train = best_model.predict(X_train)
train_accuracy = accuracy_score(y_test, y_pred_train)
y_pred_test = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Testing Accuracy with Best Model: {test_accuracy}")
107/24:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Assuming you have X_train, y_train defined from previous code

# Define the hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the Grid Search object
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,  # Number of cross-validation folds
    scoring='accuracy',  # Evaluation metric
    verbose=1  # Output progress
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_train = best_model.predict(X_train)
train_accuracy = accuracy_score(y_train, y_pred_train)
y_pred_test = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Testing Accuracy with Best Model: {test_accuracy}")
107/25:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Assuming you have X_train, y_train defined from previous code

# Define the hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the Grid Search object
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,  # Number of cross-validation folds
    scoring='accuracy',  # Evaluation metric
    verbose=1  # Output progress
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_train = best_model.predict(X_train)
train_accuracy = accuracy_score(y_train, y_pred_train)
print(f"Training Accuracy with Best Model: {train_accuracy}")
y_pred_test = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Testing Accuracy with Best Model: {test_accuracy}")
107/26:
from sklearn.ensemble import RandomForestClassifier

# Create and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate the model
y_pred_train_rf = rf_model.predict(X_train)
y_pred_test_rf = rf_model.predict(X_test)

train_accuracy_rf = accuracy_score(y_train, y_pred_train_rf)
test_accuracy_rf = accuracy_score(y_test, y_pred_test_rf)

print(f"Random Forest Training Accuracy: {train_accuracy_rf}")
print(f"Random Forest Testing Accuracy: {test_accuracy_rf}")
107/27:
import numpy as np
from sklearn.tree import DecisionTreeClassifier

# Create the decision tree classifier
clf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=1, min_samples_split=2)

# Fit the decision tree classifier to the training data
clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = clf.predict(X_test)

# Calculate the accuracy of the predictions
accuracy = np.mean(y_pred == y_test)

print('Testing Accuracy:', accuracy)
107/28:
import numpy as np
from sklearn.tree import DecisionTreeClassifier

# Create the decision tree classifier
clf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=1, min_samples_split=2)

# Fit the decision tree classifier to the training data
clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = clf.predict(X_test)
yt_pred = clf.predict(X_train)
# Calculate the accuracy of the predictions
accuracy = np.mean(y_pred == y_test)
accuracyt = cnp.mean(yt_pred == X_train)

print('Testing Accuracy:', accuracy)
print('Training Accuracy:', accuracy)
107/29:
import numpy as np
from sklearn.tree import DecisionTreeClassifier

# Create the decision tree classifier
clf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=1, min_samples_split=2)

# Fit the decision tree classifier to the training data
clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = clf.predict(X_test)
yt_pred = clf.predict(X_train)
# Calculate the accuracy of the predictions
accuracy = np.mean(y_pred == y_test)
accuracyt = cnp.mean(yt_pred == X_train)

print('Testing Accuracy:', accuracy)
print('Training Accuracy:', accuracyt)
107/30:
import numpy as np
from sklearn.tree import DecisionTreeClassifier

# Create the decision tree classifier
clf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=1, min_samples_split=2)

# Fit the decision tree classifier to the training data
clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = clf.predict(X_test)
yt_pred = clf.predict(X_train)
# Calculate the accuracy of the predictions
accuracy = np.mean(y_pred == y_test)
accuracyt = np.mean(yt_pred == X_train)

print('Testing Accuracy:', accuracy)
print('Training Accuracy:', accuracyt)
107/31:
import numpy as np
from sklearn.tree import DecisionTreeClassifier

# Create the decision tree classifier
clf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=1, min_samples_split=2)

# Fit the decision tree classifier to the training data
clf.fit(X_train, y_train)

# Make predictions on the training data
y_pred_train = clf.predict(X_train)

# Calculate the training accuracy
accuracy_train = np.mean(y_pred_train == y_train)

# Make predictions on the test data
y_pred_test = clf.predict(X_test)

# Calculate the testing accuracy
accuracy_test = np.mean(y_pred_test == y_test)

print('Training Accuracy:', accuracy_train)
print('Testing Accuracy:', accuracy_test)
107/32:
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Define the hyperparameters and their possible values
param_dist = {
    'max_depth': np.arange(1, 10),
    'min_samples_split': np.arange(2, 10),
    'min_samples_leaf': np.arange(1, 10)
}

# Create a base model
base_model = DecisionTreeClassifier()

# Create a RandomizedSearchCV object
random_search = RandomizedSearchCV(base_model, param_distributions=param_dist, n_iter=100, cv=5, verbose=1, n_jobs=-1)

# Fit the RandomizedSearchCV object to the data
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the training accuracy with the best model
best_model = random_search.best_estimator_
y_pred_train = best_model.predict(X_train)
accuracy_train = np.mean(y_pred_train == y_train)
print("Training Accuracy with Best Model:", accuracy_train)

# Get the testing accuracy with the best model
y_pred_test = best_model.predict(X_test)
accuracy_test = np.mean(y_pred_test == y_test)
print("Testing Accuracy with Best Model:", accuracy_test)
107/33:
from sklearn.ensemble import RandomForestClassifier

# Create and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=5, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate the model
y_pred_train_rf = rf_model.predict(X_train)
y_pred_test_rf = rf_model.predict(X_test)

train_accuracy_rf = accuracy_score(y_train, y_pred_train_rf)
test_accuracy_rf = accuracy_score(y_test, y_pred_test_rf)

print(f"Random Forest Training Accuracy: {train_accuracy_rf}")
print(f"Random Forest Testing Accuracy: {test_accuracy_rf}")
107/34:
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Define the hyperparameters and their possible values
param_dist = {
    'max_depth': np.arange(1, 10),
    'min_samples_split': np.arange(2, 10),
    'min_samples_leaf': np.arange(1, 10)
}

selected_features = ['CustomerID', 'Subscription_Length_Months', 'Name', 'Location', 'Gender', 'Age']

# Assuming X_train and X_test are your training and testing feature matrices
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# Create a base model
base_model = DecisionTreeClassifier()

# Create a RandomizedSearchCV object
random_search = RandomizedSearchCV(base_model, param_distributions=param_dist, n_iter=100, cv=5, verbose=1, n_jobs=-1)

# Fit the RandomizedSearchCV object to the data
random_search.fit(X_train[selected_features], y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the training accuracy with the best model
best_model = random_search.best_estimator_
y_pred_train = best_model.predict(X_train[selected_features])
accuracy_train = np.mean(y_pred_train == y_train)
print("Training Accuracy with Best Model:", accuracy_train)

# Get the testing accuracy with the best model
y_pred_test = best_model.predict(X_test[selected_features])
accuracy_test = np.mean(y_pred_test == y_test)
print("Testing Accuracy with Best Model:", accuracy_test)
107/35:
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Define the hyperparameters and their possible values
param_dist = {
    'max_depth': np.arange(1, 10),
    'min_samples_split': np.arange(2, 10),
    'min_samples_leaf': np.arange(1, 10)
}

selected_features = ['Name', 'Subscription_Length_Months', 'Name', 'Location', 'Gender', 'Age']

# Assuming X_train and X_test are your training and testing feature matrices
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# Create a base model
base_model = DecisionTreeClassifier()

# Create a RandomizedSearchCV object
random_search = RandomizedSearchCV(base_model, param_distributions=param_dist, n_iter=100, cv=5, verbose=1, n_jobs=-1)

# Fit the RandomizedSearchCV object to the data
random_search.fit(X_train[selected_features], y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the training accuracy with the best model
best_model = random_search.best_estimator_
y_pred_train = best_model.predict(X_train[selected_features])
accuracy_train = np.mean(y_pred_train == y_train)
print("Training Accuracy with Best Model:", accuracy_train)

# Get the testing accuracy with the best model
y_pred_test = best_model.predict(X_test[selected_features])
accuracy_test = np.mean(y_pred_test == y_test)
print("Testing Accuracy with Best Model:", accuracy_test)
107/36:
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Define the hyperparameters and their possible values
param_dist = {
    'max_depth': np.arange(1, 10),
    'min_samples_split': np.arange(2, 10),
    'min_samples_leaf': np.arange(1, 10)
}

# Create a base model
base_model = DecisionTreeClassifier()

# Create a RandomizedSearchCV object
random_search = RandomizedSearchCV(base_model, param_distributions=param_dist, n_iter=100, cv=5, verbose=1, n_jobs=-1)

# Fit the RandomizedSearchCV object to the data
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the training accuracy with the best model
best_model = random_search.best_estimator_
y_pred_train = best_model.predict(X_train)
accuracy_train = np.mean(y_pred_train == y_train)
print("Training Accuracy with Best Model:", accuracy_train)

# Get the testing accuracy with the best model
y_pred_test = best_model.predict(X_test)
accuracy_test = np.mean(y_pred_test == y_test)
print("Testing Accuracy with Best Model:", accuracy_test)
107/37:
from sklearn.ensemble import RandomForestClassifier

selected_features = ['CustomerID', 'Subscription_Length_Months', 'Name', 'Location', 'Gender', 'Age']

# Assuming X_train and X_test are your training and testing feature matrices
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# Create and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=5, random_state=42)
rf_model.fit(X_train[selected_features], y_train)

# Evaluate the model
y_pred_train_rf = rf_model.predict(X_train[selected_features])
y_pred_test_rf = rf_model.predict(X_test[selected_features])

train_accuracy_rf = accuracy_score(y_train, y_pred_train_rf)
test_accuracy_rf = accuracy_score(y_test, y_pred_test_rf)

print(f"Random Forest Training Accuracy: {train_accuracy_rf}")
print(f"Random Forest Testing Accuracy: {test_accuracy_rf}")
107/38:
from sklearn.ensemble import RandomForestClassifier

selected_features = ['Name', 'Subscription_Length_Months', 'Name', 'Location', 'Gender', 'Age']

# Assuming X_train and X_test are your training and testing feature matrices
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# Create and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=5, random_state=42)
rf_model.fit(X_train[selected_features], y_train)

# Evaluate the model
y_pred_train_rf = rf_model.predict(X_train[selected_features])
y_pred_test_rf = rf_model.predict(X_test[selected_features])

train_accuracy_rf = accuracy_score(y_train, y_pred_train_rf)
test_accuracy_rf = accuracy_score(y_test, y_pred_test_rf)

print(f"Random Forest Training Accuracy: {train_accuracy_rf}")
print(f"Random Forest Testing Accuracy: {test_accuracy_rf}")
107/39:
# Example: Binning age into categories
bins = [0, 18, 30, 50, 100]
labels = ['young', 'adult', 'middle-aged', 'senior']
X_train['age_category'] = pd.cut(X_train['age'], bins=bins, labels=labels)
X_test['age_category'] = pd.cut(X_test['age'], bins=bins, labels=labels)
107/40:
# Example: Binning age into categories
bins = [0, 18, 30, 50, 100]
labels = ['young', 'adult', 'middle-aged', 'senior']
X_train['age_category'] = pd.cut(X_train['Age'], bins=bins, labels=labels)
X_test['age_category'] = pd.cut(X_test['Age'], bins=bins, labels=labels)
107/41:
# Example: Binning age into categories
bins = [0, 18, 30, 50, 100]
labels = ['young', 'adult', 'middle-aged', 'senior']
X_train['age_category'] = pd.cut(X_train['Age'], bins=bins, labels=labels)
X_test['age_category'] = pd.cut(X_test['Age'], bins=bins, labels=labels)
print(X_train['age_category'])
107/42:
from sklearn.ensemble import RandomForestClassifier

# Assuming you have already applied feature engineering and have X_train and X_test ready

# Create and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=5, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate the model
y_pred_train_rf = rf_model.predict(X_train)
y_pred_test_rf = rf_model.predict(X_test)

train_accuracy_rf = accuracy_score(y_train, y_pred_train_rf)
test_accuracy_rf = accuracy_score(y_test, y_pred_test_rf)

print(f"Random Forest Training Accuracy: {train_accuracy_rf}")
print(f"Random Forest Testing Accuracy: {test_accuracy_rf}")
107/43:
# Example: Binning age into categories
bins = [0, 18, 30, 50, 100]
labels = ['young', 'adult', 'middle-aged', 'senior']
X_train['age_category'] = pd.cut(X_train['Age'], bins=bins, labels=labels)
X_test['age_category'] = pd.cut(X_test['Age'], bins=bins, labels=labels)
107/44:
# Example: Binning age into categories
bins = [0, 18, 30, 50, 100]
labels = ['young', 'adult', 'middle-aged', 'senior']
107/45:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
107/46:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
107/47:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
107/48:
# Example: Binning age into categories
bins = [0, 18, 30, 50, 100]
labels = ['young', 'adult', 'middle-aged', 'senior']
labels['age_category'] = pd.cut(['Age'], bins=bins, labels=labels)
107/49:
import pandas as pd

# Assuming 'df' is your DataFrame and 'Age' is a column in it
bins = [0, 18, 30, 50, 100]
labels = ['young', 'adult', 'middle-aged', 'senior']

df['age_category'] = pd.cut(df['Age'], bins=bins, labels=labels)
107/50:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['age_category'] = label_encoder.fit_transform(df['age_category'])
df['age_category'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
107/51:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
107/52:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
107/53:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['age_category'] = label_encoder.fit_transform(df['age_category'])
df['age_category'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
107/54:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
107/55:
import pandas as pd

# Assuming 'df' is your DataFrame and 'Age' is a column in it
bins = [0, 18, 30, 50, 100]
labels = ['young', 'adult', 'middle-aged', 'senior']

df['age_category'] = pd.cut(df['Age'], bins=bins, labels=labels)
107/56:
import pandas as pd

# Assuming 'df' is your DataFrame and 'Age' is a column in it
bins = [0, 18, 30, 50, 100]
labels = ['young', 'adult', 'middle-aged', 'senior']

df['age_category'] = pd.cut(df['Age'], bins=bins, labels=labels)
print(df['age_category'])
107/57:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['age_category'] = label_encoder.fit_transform(df['age_category'])
df['age_category'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
107/58:
# Splitting train and test data

X = df[['Name', 'Age', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
107/59:
# Splitting train and test data

X = df[['Name', 'age_category', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
107/60:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
107/61:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Assuming you have X_train, y_train defined from previous code

# Define the hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the Grid Search object
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,  # Number of cross-validation folds
    scoring='accuracy',  # Evaluation metric
    verbose=1  # Output progress
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)

print(f"Testing Accuracy with Best Model: {train_accuracy}")
print(f"Testing Accuracy with Best Model: {test_accuracy}")
107/62:
# Running logistic regression model
from asyncio import log
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error


lr = LogisticRegression()
result = lr.fit(X_train, y_train)
from sklearn import metrics
prediction_test = lr.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(lr, X, y, cv=kf, scoring='accuracy')

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = lr.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = lr.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)


print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
107/63:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Create and train the model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate on training set
train_preds = model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_preds)

# Evaluate on testing set
test_preds = model.predict(X_test)
test_accuracy = accuracy_score(y_test, test_preds)

print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")
107/64:
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)
)

train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)

plt.plot(train_sizes, train_scores_mean, label='Training Accuracy')
plt.plot(train_sizes, test_scores_mean, label='Testing Accuracy')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
107/65:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Assuming you have X_train, y_train defined from previous code

# Define the hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the Grid Search object
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,  # Number of cross-validation folds
    scoring='accuracy',  # Evaluation metric
    verbose=1  # Output progress
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_train = best_model.predict(X_train)
train_accuracy = accuracy_score(y_train, y_pred_train)
print(f"Training Accuracy with Best Model: {train_accuracy}")
y_pred_test = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Testing Accuracy with Best Model: {test_accuracy}")
107/66:
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)
)

train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)

plt.plot(train_sizes, train_scores_mean, label='Training Accuracy')
plt.plot(train_sizes, test_scores_mean, label='Testing Accuracy')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
107/67:
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Define the hyperparameters and their possible values
param_dist = {
    'max_depth': np.arange(1, 10),
    'min_samples_split': np.arange(2, 10),
    'min_samples_leaf': np.arange(1, 10)
}

# Create a base model
base_model = DecisionTreeClassifier()

# Create a RandomizedSearchCV object
random_search = RandomizedSearchCV(base_model, param_distributions=param_dist, n_iter=100, cv=5, verbose=1, n_jobs=-1)

# Fit the RandomizedSearchCV object to the data
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the training accuracy with the best model
best_model = random_search.best_estimator_
y_pred_train = best_model.predict(X_train)
accuracy_train = np.mean(y_pred_train == y_train)
print("Training Accuracy with Best Model:", accuracy_train)

# Get the testing accuracy with the best model
y_pred_test = best_model.predict(X_test)
accuracy_test = np.mean(y_pred_test == y_test)
print("Testing Accuracy with Best Model:", accuracy_test)
107/68:
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Define the hyperparameters and their possible values
param_dist = {
    'max_depth': np.arange(1, 10),
    'min_samples_split': np.arange(2, 10),
    'min_samples_leaf': np.arange(1, 10)
}

# Create a base model
base_model = DecisionTreeClassifier()

# Create a RandomizedSearchCV object
random_search = RandomizedSearchCV(base_model, param_distributions=param_dist, n_iter=100, cv=5, verbose=1, n_jobs=-1)

# Fit the RandomizedSearchCV object to the data
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the training accuracy with the best model
best_model = random_search.best_estimator_
y_pred_train = best_model.predict(X_train)
accuracy_train = np.mean(y_pred_train == y_train)
print("Training Accuracy with Best Model:", accuracy_train)

# Get the testing accuracy with the best model
y_pred_test = best_model.predict(X_test)
accuracy_test = np.mean(y_pred_test == y_test)
print("Testing Accuracy with Best Model:", accuracy_test)
107/69:
from sklearn.ensemble import RandomForestClassifier

# Create and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=5, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate the model
y_pred_train_rf = rf_model.predict(X_train)
y_pred_test_rf = rf_model.predict(X_test)

train_accuracy_rf = accuracy_score(y_train, y_pred_train_rf)
test_accuracy_rf = accuracy_score(y_test, y_pred_test_rf)

print(f"Random Forest Training Accuracy: {train_accuracy_rf}")
print(f"Random Forest Testing Accuracy: {test_accuracy_rf}")
108/1:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')
display(df)
108/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')
display(df)

import pandas as pd

df = pd.DataFrame({'fixed acidity;"volatile acidity";"citric acid";"residual sugar";"chlorides";"free sulfur dioxide";"total sulfur dioxide";"density";"pH";"sulphates";"alcohol";"quality"': ['7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5', '7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5', '7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;9.9;5', '11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58;9.8;5', '7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5', '6.2;0.6;0.08;2;0.09;32;44;0.9949;3.45;0.58;10.5;5', '5.9;0.55;0.1;2.2;0.062;39;51;0.99512;3.52;0.76;12.8;6', '6.3;0.51;0.13;2.3;0.076;29;40;0.99574;3.42;0.75;11.1;6', '5.9;0.645;0.12;2;0.075;32;44;0.99547;3.57;0.71;10.5;5', '6;0.31;0.47;3.6;0.067;18;42;0.99549;3.39;0.66;11.0;6']})

# Transpose the DataFrame
df_t = df.T

# Print the transposed DataFrame
print(df_t)
108/3:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')
display(df)

import pandas as pd

df = pd.DataFrame({'fixed acidity;"volatile acidity";"citric acid";"residual sugar";"chlorides";"free sulfur dioxide";"total sulfur dioxide";"density";"pH";"sulphates";"alcohol";"quality"': ['7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5', '7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5', '7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;9.9;5', '11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58;9.8;5', '7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5', '6.2;0.6;0.08;2;0.09;32;44;0.9949;3.45;0.58;10.5;5', '5.9;0.55;0.1;2.2;0.062;39;51;0.99512;3.52;0.76;12.8;6', '6.3;0.51;0.13;2.3;0.076;29;40;0.99574;3.42;0.75;11.1;6', '5.9;0.645;0.12;2;0.075;32;44;0.99547;3.57;0.71;10.5;5', '6;0.31;0.47;3.6;0.067;18;42;0.99549;3.39;0.66;11.0;6']})

# Transpose the DataFrame
df_t = df.T

# Print the transposed DataFrame
display(df_t)
108/4:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')
display(df)
108/5:
from mysqlx import Column
import pandas as pd

# Transpose the columns
df = pd.DataFrame(Column).T

# Split each row into a list of values
for index, row in df.iterrows():
    row = row.split(' ')
    df.loc[index] = row

# Print the DataFrame
print(df)
109/1:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')
display(df)

df_t = df.T
109/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')
display(df)

df_t = df.T
print(df_t)
109/3:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

df_t = df.T
display(df_t)
109/4:
from numpy import column_stack
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

df_t = df(column_stack).T
display(df_t)
109/5:
from numpy import column_stack
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

df_t = df.T
109/6:
from numpy import column_stack
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

df_t = df.T
# Save the transposed DataFrame to a new CSV file
df_t.to_csv('csv_file_column_wise.csv', index=False)
108/6:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')
print(df)
108/7:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

# Display the DataFrame with the column headers on the left
display(df.style.set_properties(**{'text-align': 'left'}))
108/8:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

# Display the DataFrame with the column headers above the data
display(df.style.set_properties(**{'caption-side': 'top'}))
108/9:
import pandas as pd

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

# Separate the data in the DataFrame as a tab
df_tab = df.to_string(sep='\t')

# Save the tab-separated DataFrame to a new CSV file
with open('winequality-red_tab.csv', 'w') as f:
    f.write(df_tab)
108/10:
import pandas as pd

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

# Separate the data in the DataFrame as a tab
df_tab = df.to_string(sep='\t')

# Display the DataFrame
print(df_tab)

# Save the tab-separated DataFrame to a new CSV file
with open('winequality-red_tab.csv', 'w') as f:
    f.write(df_tab)
108/11:
import pandas as pd

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

# Save the DataFrame to a CSV file with the data separated by tabs
df.to_csv('winequality-red_tab.csv', sep='\t')
108/12:
import pandas as pd

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

# Get the column names
column_names = df.columns.tolist()

# Create a list of subplots
subplots = []
for i in range(len(column_names)):
    subplots.append(plt.subplot(1, len(column_names), i + 1))

# Plot the data in each column
for i in range(len(column_names)):
    subplots[i].hist(df[column_names[i]])
    subplots[i].set_title(column_names[i])

# Adjust the layout of the subplots
plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.3, hspace=0.3)

# Show the plot
plt.show()
108/13:
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

# Get the column names
column_names = df.columns.tolist()

# Create a list of subplots
subplots = []
for i in range(len(column_names)):
    subplots.append(plt.subplot(1, len(column_names), i + 1))

# Plot the data in each column
for i in range(len(column_names)):
    subplots[i].hist(df[column_names[i]])
    subplots[i].set_title(column_names[i])

# Adjust the layout of the subplots
plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.3, hspace=0.3)

# Show the plot
plt.show()
108/14:
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('input.csv')

# Transpose the DataFrame
df = df.T

# Split each row into a list of values
for index, row in df.iterrows():
    row = row[0].split(' ')
    df.loc[index] = row

# Save the DataFrame as a new CSV file
df.to_csv('output.csv', index=False)
108/15:
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv')

# Transpose the DataFrame
df = df.T

# Split each row into a list of values
for index, row in df.iterrows():
    row = row[0].split(' ')
    df.loc[index] = row

# Save the DataFrame as a new CSV file
df.to_csv('output.csv', index=False)
108/16:
import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')

# Save the modified DataFrame to a new CSV file
df.to_csv('edited.csv', index=False)
108/17:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
print(df)
108/18:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
110/1:
# Check for missing values
missing_values = df.isna().sum()
print(missing_values)
110/2:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
110/3:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
110/4:
import pandas as pd

# Define a function to detect outliers using IQR
def find_outliers_iqr(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return (series < lower_bound) | (series > upper_bound)

# Apply the function to each numerical column
outliers = df.select_dtypes(include=['float64', 'int64']).apply(find_outliers_iqr)

# Print columns with outliers
print(outliers.any())
110/5:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
110/6:
duplicate_count = df.duplicated().sum()

print(f'Total duplicate rows: {duplicate_count}')
110/7:
duplicate_count = df.duplicated().sum()

print(f'Total duplicate rows: {duplicate_count}')

df = df.drop_duplicates()
110/8:
duplicate_count = df.duplicated().sum()

print(f'Total duplicate rows: {duplicate_count}')

data = df.drop_duplicates()
110/9:
duplicate_count = df.duplicated().sum()

print(f'Total duplicate rows: {duplicate_count}')
110/10:
duplicate_count = df.duplicated().sum()

print(f'Total duplicate rows: {duplicate_count}')
110/11:
duplicate_count = df.duplicated().sum()

print(f'Total duplicate rows: {duplicate_count}')
110/12:
duplicate_count = df.duplicated().sum()

print(f'Total duplicate rows: {duplicate_count}')
110/13:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
110/14:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
110/15:
import pandas as pd

# Define a function to detect outliers using IQR
def find_outliers_iqr(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return (series < lower_bound) | (series > upper_bound)

# Apply the function to each numerical column
outliers = df.select_dtypes(include=['float64', 'int64']).apply(find_outliers_iqr)

# Print columns with outliers
print(outliers.any())
110/16:
duplicate_count = df.duplicated().sum()

print(f'Total duplicate rows: {duplicate_count}')
110/17:
# Find duplicate rows
duplicate_rows = df[df.duplicated()]

# Print duplicate rows
print(duplicate_rows)
110/18:
# Find duplicate rows
duplicate_rows = df[df.duplicated()]

# Print duplicate rows
print(duplicate_rows)

# Remove duplicate rows
data = df.drop_duplicates()
print(data)
110/19:
# Find duplicate rows
duplicate_rows = df[df.duplicated()]

# Remove duplicate rows
data = df.drop_duplicates()
print(data)
110/20: data.describe(include="all")
110/21: plotCorrelationMatrix(data, 8)
110/22:
from matplotlib import pyplot as plt


plt.CorrelationMatrix(data, 8)
110/23:
import matplotlib.pyplot as plt
import numpy as np

# Assuming 'data' is your correlation matrix (replace it with your actual data)
data = np.random.rand(8, 8)
plt.matshow(data)
plt.colorbar()
plt.show()
110/24:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
110/25:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
110/26:
import matplotlib.pyplot as plt
import pandas as pd

# Get the list of feature names
features = data.columns.tolist()
features.remove('quality')  # Remove 'quality' from the list of features

# Create subplots
fig, axes = plt.subplots(3, 4, figsize=(15, 10))  # 3 rows, 4 columns of subplots

for i, feature in enumerate(features):
    row = i // 4
    col = i % 4

    # Scatter plot quality vs feature
    axes[row, col].scatter(df[feature], df['quality'], alpha=0.5)
    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('quality')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/27: data.describe(include="all")
110/28:
# Find duplicate rows
duplicate_rows = df[df.duplicated()]

# Remove duplicate rows
data = df.drop_duplicates()
print(data)
110/29: data.describe(include="all")
110/30:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
110/31:
import matplotlib.pyplot as plt
import pandas as pd

# Get the list of feature names
features = data.columns.tolist()
features.remove('quality')  # Remove 'quality' from the list of features

# Create subplots
fig, axes = plt.subplots(3, 4, figsize=(15, 10))  # 3 rows, 4 columns of subplots

for i, feature in enumerate(features):
    row = i // 4
    col = i % 4

    # Scatter plot quality vs feature
    axes[row, col].scatter(df[feature], df['quality'], alpha=0.5)
    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('quality')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/32:
import matplotlib.pyplot as plt
import pandas as pd


# Get the list of feature names
features = data.columns.tolist()
features.remove('quality')  # Remove 'quality' from the list of features

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, feature in enumerate(features):
    row = i // 4
    col = i % 4

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/33:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names
features = data.columns.tolist()
features.remove('quality')  # Remove 'quality' from the list of features

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(3, 4, figsize=(15, 10))  # 3 rows, 4 columns of subplots

for i, feature in enumerate(features):
    row = i // 4
    col = i % 4

    # Create bar plots with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        avg_value = data[data['quality'] == q][feature].mean()
        axes[row, col].bar(f'Quality {q}', avg_value, color=color)

    axes[row, col].set_xlabel('Quality Level')
    axes[row, col].set_ylabel(f'Average {feature}')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/34:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names
features = data.columns.tolist()
features.remove('density', 'quality')  # Remove 'quality' from the list of features

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(3, 4, figsize=(15, 10))  # 3 rows, 4 columns of subplots

for i, feature in enumerate(features):
    row = i // 4
    col = i % 4

    # Create bar plots with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        avg_value = data[data['quality'] == q][feature].mean()
        axes[row, col].bar(f'Quality {q}', avg_value, color=color)

    axes[row, col].set_xlabel('Quality Level')
    axes[row, col].set_ylabel(f'Average {feature}')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/35:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density' and 'quality'
features = [col for col in data.columns if col not in ['density', 'quality']]

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(3, 4, figsize=(15, 10))  # 3 rows, 4 columns of subplots

for i, feature in enumerate(features):
    row = i // 4
    col = i % 4

    # Create bar plots with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        avg_value = data[data['quality'] == q][feature].mean()
        axes[row, col].bar('', avg_value, color=color, label=f'Quality {q}')

    axes[row, col].set_ylabel(f'Average {feature}')

# Add legends
axes[0, 0].legend(loc='upper right')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/36:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density' and 'quality'
features = [col for col in data.columns if col not in ['density', 'quality']]

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(3, 4, figsize=(15, 10))  # 3 rows, 4 columns of subplots

for i, feature in enumerate(features):
    row = i // 4
    col = i % 4

    # Create bar plots with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        avg_value = data[data['quality'] == q][feature].mean()
        axes[row, col].bar('', avg_value, color=color, label=f'Quality {q}')

    axes[row, col].set_ylabel(f'Average {feature}')

# Add legends
axes[0, 0].legend(loc='upper right')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/37:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/38:
# Splitting train and test data

from sklearn.model_selection import train_test_split


X = df['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']
y = df['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
110/39:
# Splitting train and test data

from sklearn.model_selection import train_test_split


X = data['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
110/40:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
110/41:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
110/42:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Assuming X_train and y_train are your training data
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
110/43:
from sklearn.svm import SVR

# Assuming X_train and y_train are your training data
model = SVR()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
110/44:
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Assuming X_train and y_train are your training data
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
110/45:
from sklearn.neighbors import KNeighborsClassifier

# Assuming X_train and y_train are your training data
model = KNeighborsClassifier()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
110/46:
from sklearn.ensemble import RandomForestClassifier

# Assuming X_train and y_train are your training data
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
110/47:
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Assuming X_train and y_train are your training data
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

# Get feature importances (coefficients)
importance = model.coef_[0]

# Print feature importances
for i, imp in enumerate(importance):
    print(f'Feature {i}: {imp}')

# Optionally, you can also sort the features by importance
# sorted_indices = importance.argsort()
# for i in sorted_indices:
#     print(f'Feature {i}: {importance[i]}')
110/48:
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Assuming X_train and y_train are your training data
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
110/49:
from sklearn.svm import SVC

# Assuming X_train and y_train are your training data
model = SVC()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
110/50:
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Assuming you have your data in a variable called 'X' and labels in a variable called 'y'

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the hyperparameters and their values to search
param_grid = {
    'RandomForestClassifier': {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
    },
    'SVM': {
        'C': [0.1, 1, 10],
        'kernel': ['linear', 'rbf'],
    },
    'LogisticRegression': {
        'C': [0.1, 1, 10],
        'penalty': ['l1', 'l2'],
    }
}

# Create a dictionary of models to be tuned
models = {
    'RandomForestClassifier': RandomForestClassifier(),
    'SVM': SVC(),
    'LogisticRegression': LogisticRegression()
}

# Perform Grid Search and print the best parameters and corresponding accuracy
for model_name, model in models.items():
    grid_search = GridSearchCV(model, param_grid[model_name], cv=5)
    grid_search.fit(X_train, y_train)
    best_params = grid_search.best_params_
    best_model = grid_search.best_estimator_
    
    # Evaluate on test set
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"Best parameters for {model_name}: {best_params}")
    print(f"Accuracy: {accuracy:.4f}\n")
110/51:
from collections import Counter


def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5)
110/52:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(data.loc[detect_outliers(data,data.columns[:-1])]))
data.loc[detect_outliers(data,data.columns[:-1])]
110/53:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(data.loc[detect_outliers(data,data.columns[:-1])]))
data.loc[detect_outliers(data,data.columns[:-1])]

data = data.drop(detect_outliers(data,data.columns[:-1]),axis = 0).reset_index(drop = True)
110/54:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(data.loc[detect_outliers(data,data.columns[:-1])]))
data.loc[detect_outliers(data,data.columns[:-1])]
110/55: data = data.drop(detect_outliers(data,data.columns[:-1]),axis = 0).reset_index(drop = True)
110/56:
data = data.drop(detect_outliers(data,data.columns[:-1]),axis = 0).reset_index(drop = True)
print(data)
110/57: data = data.drop(detect_outliers(data,data.columns[:-1]),axis = 0).reset_index(drop = True)
110/58:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[["Fixed acidity", "Volatile acidity", "Citric acid", "Residual sugar", 
               "Chlorides", "Free sulfur dioxide", "Total sulfur dioxide", "Density", 
               "pH", "Sulphates", "Alcohol content", "quality"]].groupby(["quality"], as_index=False).mean().sort_values(by="quality")

# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/59:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
110/60:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
110/61:
import pandas as pd

# Define a function to detect outliers using IQR
def find_outliers_iqr(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return (series < lower_bound) | (series > upper_bound)

# Apply the function to each numerical column
outliers = df.select_dtypes(include=['float64', 'int64']).apply(find_outliers_iqr)

# Print columns with outliers
print(outliers.any())
110/62:
# Find duplicate rows
duplicate_rows = df[df.duplicated()]

# Remove duplicate rows
data = df.drop_duplicates()
print(data)
110/63:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(data.loc[detect_outliers(data,data.columns[:-1])]))
data.loc[detect_outliers(data,data.columns[:-1])]
110/64: data = data.drop(detect_outliers(data,data.columns[:-1]),axis = 0).reset_index(drop = True)
110/65: data.describe(include="all")
110/66:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
110/67:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/68:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[["Fixed acidity", "Volatile acidity", "Citric acid", "Residual sugar", 
               "Chlorides", "Free sulfur dioxide", "Total sulfur dioxide", "Density", 
               "pH", "Sulphates", "Alcohol content", "quality"]].groupby(["quality"], as_index=False).mean().sort_values(by="quality")

# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/69:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")

# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/70:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']].groupby(['quality'], as_index=False).mean().sort_values(by="quality")

# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/71:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']].groupby(['quality'], as_index=False).mean().sort_values(by="quality")

# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/72:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']].groupby(['quality'], as_index=False).mean().sort_values(by='quality')

# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/73:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']].groupby(['quality'], as_index=False).mean().sort_values(by='quality')

# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/74:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/75:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']].groupby(['quality'], as_index=False).mean().sort_values(by='quality')

# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/76:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']].groupby(['quality'], as_index=False).mean().sort_values(by='quality')

# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/77:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']].groupby(['class'], as_index=False).mean().sort_values(by='class')


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/78:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']].groupby(['quality'], as_index=False).mean().sort_values(by='quality')


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/79: data[["fixed acidity","quality"]].groupby(["quality"], as_index = False).mean().sort_values(by = "quality").style.background_gradient("Reds")
110/80:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/81:
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
110/82:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
110/83:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=14)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
110/84:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
110/85:
scaler = StandardScaler()
X_train_sm = scaler.fit_transform(X_train_sm) 
X_test = scaler.transform(X_test)
110/86:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
110/87:
from sklearn.base import accuracy_score
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier


knn = KNeighborsClassifier(n_neighbors = 2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc)

print("Score : ", score)
print("KNeighborsClassifier Acc : ", acc)

pair_confusion_matrix(knn, X_test, y_test, cmap= "Greens")  
plt.show()
110/88:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix


knn = KNeighborsClassifier(n_neighbors = 2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc)

print("Score : ", score)
print("KNeighborsClassifier Acc : ", acc)

pair_confusion_matrix(knn, X_test, y_test, cmap= "Greens")  
plt.show()
110/89:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix


knn = KNeighborsClassifier(n_neighbors = 2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc)

print("Score : ", score)
print("KNeighborsClassifier Acc : ", acc)

pair_confusion_matrix(knn, X_test, y_test, cmap= "Greens")  
plt.show()
110/90: data['quality'].value_counts()
110/91:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
110/92:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
110/93:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
110/94:
import pandas as pd

# Define a function to detect outliers using IQR
def find_outliers_iqr(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return (series < lower_bound) | (series > upper_bound)

# Apply the function to each numerical column
outliers = df.select_dtypes(include=['float64', 'int64']).apply(find_outliers_iqr)

# Print columns with outliers
print(outliers.any())
110/95:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(data.loc[detect_outliers(data,data.columns[:-1])]))
data.loc[detect_outliers(data,data.columns[:-1])]
110/96: data = data.drop(detect_outliers(data,data.columns[:-1]),axis = 0).reset_index(drop = True)
110/97: data.describe(include="all")
110/98:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
110/99:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/100:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/101:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
110/102:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
110/103:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
110/104:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
110/105:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
110/106:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
110/107:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(data.loc[detect_outliers(data,data.columns[:-1])]))
data.loc[detect_outliers(data,data.columns[:-1])]
110/108:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(data.loc[detect_outliers(data,data.columns[:-1])]))
data.loc[detect_outliers(data,data.columns[:-1])]
110/109: data = data.drop(detect_outliers(data,data.columns[:-1]),axis = 0).reset_index(drop = True)
110/110:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(data.loc[detect_outliers(data,data.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
110/111:
import pandas as pd

# Define a function to detect outliers using IQR
def find_outliers_iqr(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return (series < lower_bound) | (series > upper_bound)

# Apply the function to each numerical column
outliers = df.select_dtypes(include=['float64', 'int64']).apply(find_outliers_iqr)

# Print columns with outliers
print(outliers.any())
110/112:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
110/113: data = df.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
110/114: data.describe(include="all")
110/115:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
110/116:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/117:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/118:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
110/119:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
110/120:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
110/121:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
110/122:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
110/123:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix


knn = KNeighborsClassifier(n_neighbors = 2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc)

print("Score : ", score)
print("KNeighborsClassifier Acc : ", acc)

pair_confusion_matrix(knn, X_test, y_test, cmap= "Greens")  
plt.show()
110/124:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix


knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc)

print("Score: ", score)
print("KNeighborsClassifier Acc: ", acc)

# Use plot_confusion_matrix instead of pair_confusion_matrix
plot_confusion_matrix(knn, X_test, y_test, cmap="Greens")  
plt.show()
110/125:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix


knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc)

print("Score: ", score)
print("KNeighborsClassifier Acc: ", acc)

# Use plot_confusion_matrix instead of pair_confusion_matrix
plt.confusion_matrix(knn, X_test, y_test, cmap="Greens")  
plt.show()
110/126:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix


knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc)

print("Score: ", score)
print("KNeighborsClassifier Acc: ", acc)
110/127:
from sklearn.metrics import classification_report


report = classification_report(y_test, y_pred)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Print the classification report
print(report)
110/128:
from sklearn.metrics import classification_report


report = classification_report(y_test, y_pred)
print(report)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/129:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc = accuracy_score(y_test, y_pred_gbc)
score = gbc.score(X_test, y_test)
results.append(acc)

print("Score : ", score)
print("GradientBoostingClassifier Acc : ", acc)
110/130:
from sklearn.svm import SVC


svc = SVC()
svc.fit(X_resampled, y_resampled)
pred_svc = svc.predict(X_test)

cm_svc = confusion_matrix(y_test, pred_svc)
acc = accuracy_score(y_test, pred_svc)
score = svc.score(X_test, y_test)
results.append(acc)

print("Score : ", score)
print("SVC Acc : ", acc)
110/131:
from sklearn.metrics import classification_report


report = classification_report(y_test, y_pred)
print(report)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=svc.classes_, yticklabels=svc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/132:
report_svc = classification_report(y_test, pred_svc)
print(report_svc)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svc, annot=True, fmt='d', cmap='Greens', xticklabels=svc.classes_, yticklabels=svc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/133:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_dt = dt_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_dt = confusion_matrix(y_test, pred_dt)
acc = accuracy_score(y_test, pred_dt)

# Print accuracy
print("Decision Tree Classifier Accuracy: ", acc)

# Generate the classification report
report = classification_report(y_test, pred_dt)
print(report)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blue', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/134:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_dt = dt_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_dt = confusion_matrix(y_test, pred_dt)
acc = accuracy_score(y_test, pred_dt)

# Print accuracy
print("Decision Tree Classifier Accuracy: ", acc)

# Generate the classification report
report = classification_report(y_test, pred_dt)
print(report)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/135:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_rf = rf_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_rf = confusion_matrix(y_test, pred_rf)
acc = accuracy_score(y_test, pred_rf)

# Print accuracy
print("Random Forest Classifier Accuracy: ", acc)

# Generate the classification report
report = classification_report(y_test, pred_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Print the classification report
print(report)
110/136:
from sklearn.ensemble import GradientBoostingClassifier

# Initialize and train the GradientBoostingClassifier
gb_classifier = GradientBoostingClassifier()
gb_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_gb = gb_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_gb = confusion_matrix(y_test, pred_gb)
acc_gb = accuracy_score(y_test, pred_gb)

# Print accuracy
print("Gradient Boosting Classifier Accuracy: ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, pred_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Purples', xticklabels=gb_classifier.classes_, yticklabels=gb_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

# Print the classification report
print(report_gb)
110/137:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc = accuracy_score(y_test, y_pred_gbc)
score = gbc.score(X_test, y_test)
results.append(acc)

print("Score : ", score)
print("GradientBoostingClassifier Acc : ", acc)
110/138:
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc = accuracy_score(y_test, y_pred_gbc)
score = gbc.score(X_test, y_test)
results.append(acc)

print("Score : ", score)
print("GradientBoostingClassifier Acc : ", acc)

# Generate the classification report
report_gb = classification_report(y_test, pred_gb)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Purples', xticklabels=gb_classifier.classes_, yticklabels=gb_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()
110/139:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
y_pred_rf = rf_classifier.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

acc = accuracy_score(y_test, y_pred_rf)
score = rf_classifier.score(X_test, y_test)
results.append(acc)

print("Score : ", score)
print("RandomForestClassifier Acc : ", acc)

# Print accuracy
print("Random Forest Classifier Accuracy: ", acc)

# Generate the classification report
report = classification_report(y_test, y_pred_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Print the classification report
print(report)
110/140:
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc = accuracy_score(y_test, y_pred_gbc)
score = gbc.score(X_test, y_test)
results.append(acc)

print("Score : ", score)
print("GradientBoostingClassifier Acc : ", acc)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()
110/141:
import numpy as np


plt.figure(figsize=(12,6))
sns.scatterplot(x="citric acid", y="fixed acidity", hue="Y_test", data=y_test, palette=["Black","darkgreen"])
plt.title("Classifications We Made Wrong", fontsize = 13, fontweight = "bold", color = "darkred")

diff = np.where(y_pred_rf!=y_test)[0]
plt.scatter(y_test.iloc[diff,0],y_test.iloc[diff,1],label = "Wrong Classified", marker="x",alpha = 0.7, color = "Red",s = 300)
plt.legend()
plt.show()
110/142:
import numpy as np

plt.figure(figsize=(12,6))

# Create a scatter plot with actual labels
sns.scatterplot(x=X_test["citric acid"], y=X_test["fixed acidity"], hue=y_test, palette=["black", "darkgreen"])
plt.title("Classifications We Made Wrong", fontsize=13, fontweight="bold", color="darkred")

# Find indices where predictions were wrong
diff = np.where(y_pred_rf != y_test)[0]

# Scatter plot the wrong classified points
plt.scatter(X_test.iloc[diff, X_test.columns.get_loc("citric acid")], 
            X_test.iloc[diff, X_test.columns.get_loc("fixed acidity")], 
            label="Wrong Classified", marker="x", alpha=0.7, color="red", s=300)

plt.legend()
plt.show()
110/143:
import numpy as np

plt.figure(figsize=(12,6))

# Create a scatter plot with actual labels
sns.scatterplot(x=X_test["citric acid"], y=X_test["fixed acidity"], hue=y_test, palette=["black", "darkgreen"])
plt.title("Classifications We Made Wrong", fontsize=13, fontweight="bold", color="darkred")

# Find indices where predictions were wrong
diff = np.where(y_pred_rf != y_test)[0]

# Scatter plot the wrong classified points
plt.scatter(X_test.iloc[diff, X_test.columns.get_loc("citric acid")], 
            X_test.iloc[diff, X_test.columns.get_loc("fixed acidity")], 
            label="Wrong Classified", marker="x", alpha=0.7, color="red", s=300)

plt.legend()
plt.show()
110/144:
from sklearn.ensemble import AdaBoostClassifier

# Initialize and train the AdaBoostClassifier
ab_classifier = AdaBoostClassifier()
ab_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_ab = ab_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_ab = confusion_matrix(y_test, pred_ab)
acc_ab = accuracy_score(y_test, pred_ab)

# Print accuracy
print("AdaBoost Classifier Accuracy: ", acc_ab)

# Generate the classification report
report_ab = classification_report(y_test, pred_ab)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_ab, annot=True, fmt='d', cmap='browns', xticklabels=ab_classifier.classes_, yticklabels=ab_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (AdaBoost)')
plt.show()

# Print the classification report
print(report_ab)
110/145:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("Score : ", score)
print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()
110/146:
from sklearn.ensemble import AdaBoostClassifier

# Initialize and train the AdaBoostClassifier
ab_classifier = AdaBoostClassifier()
ab_classifier.fit(X_resampled, y_resampled)
pred_ab = ab_classifier.predict(X_test)
cm_ab = confusion_matrix(y_test, pred_ab)
acc_ab = accuracy_score(y_test, pred_ab)
results.append(acc_ab)
print("AdaBoost Classifier Accuracy: ", acc_ab)

# Generate the classification report
report_ab = classification_report(y_test, pred_ab)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_ab, annot=True, fmt='d', cmap='browns', xticklabels=ab_classifier.classes_, yticklabels=ab_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (AdaBoost)')
plt.show()

# Print the classification report
print(report_ab)
110/147:
from sklearn.ensemble import AdaBoostClassifier

# Initialize and train the AdaBoostClassifier
ab_classifier = AdaBoostClassifier()
ab_classifier.fit(X_resampled, y_resampled)
pred_ab = ab_classifier.predict(X_test)
cm_ab = confusion_matrix(y_test, pred_ab)
acc_ab = accuracy_score(y_test, pred_ab)
results.append(acc_ab)
print("AdaBoost Classifier Accuracy: ", acc_ab)

# Generate the classification report
report_ab = classification_report(y_test, pred_ab)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_ab, annot=True, fmt='d', cmap='YlOrBr', xticklabels=ab_classifier.classes_, yticklabels=ab_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (AdaBoost)')
plt.show()

# Print the classification report
print(report_ab)
110/148:
from xgboost import XGBClassifier

# Initialize and train the XGBClassifier
xgb_classifier = XGBClassifier()
xgb_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_xgb = xgb_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_xgb = confusion_matrix(y_test, pred_xgb)
acc_xgb = accuracy_score(y_test, pred_xgb)

# Print accuracy
print("XGBoost Classifier Accuracy: ", acc_xgb)

# Generate the classification report
report_xgb = classification_report(y_test, pred_xgb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='coolwarm', xticklabels=xgb_classifier.classes_, yticklabels=xgb_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (XGBoost)')
plt.show()

# Print the classification report
print(report_xgb)
110/149:
from xgboost import XGBClassifier

# Initialize and train the XGBClassifier
xgb_classifier = XGBClassifier()
xgb_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_xgb = xgb_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_xgb = confusion_matrix(y_test, pred_xgb)
acc_xgb = accuracy_score(y_test, pred_xgb)

# Print accuracy
print("XGBoost Classifier Accuracy: ", acc_xgb)

# Generate the classification report
report_xgb = classification_report(y_test, pred_xgb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Pinks', xticklabels=xgb_classifier.classes_, yticklabels=xgb_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (XGBoost)')
plt.show()

# Print the classification report
print(report_xgb)
110/150:
from xgboost import XGBClassifier

# Initialize and train the XGBClassifier
xgb_classifier = XGBClassifier()
xgb_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_xgb = xgb_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_xgb = confusion_matrix(y_test, pred_xgb)
acc_xgb = accuracy_score(y_test, pred_xgb)

# Print accuracy
print("XGBoost Classifier Accuracy: ", acc_xgb)

# Generate the classification report
report_xgb = classification_report(y_test, pred_xgb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='pink', xticklabels=xgb_classifier.classes_, yticklabels=xgb_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (XGBoost)')
plt.show()

# Print the classification report
print(report_xgb)
110/151:
df_result = pd.DataFrame({"Score":results, "ML Models":["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier"
             ,"XGBClassifier"]})
110/152:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()
110/153:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
y_pred_rf = rf_classifier.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

acc = accuracy_score(y_test, y_pred_rf)
score = rf_classifier.score(X_test, y_test)
results.append(acc)

print("RandomForestClassifier Acc : ", acc)

# Print accuracy
print("Random Forest Classifier Accuracy: ", acc)

# Generate the classification report
report = classification_report(y_test, y_pred_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Print the classification report
print(report)
110/154:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
y_pred_rf = rf_classifier.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

acc_rf = accuracy_score(y_test, y_pred_rf)
score = rf_classifier.score(X_test, y_test)
results.append(acc_rf)

print("RandomForestClassifier Acc : ", acc_rf)


# Generate the classification report
report = classification_report(y_test, y_pred_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Print the classification report
print(report)
110/155:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_dt = dt_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_dt = confusion_matrix(y_test, pred_dt)
acc_dt = accuracy_score(y_test, pred_dt)

# Print accuracy
print("Decision Tree Classifier Accuracy: ", acc_dt)

# Generate the classification report
report = classification_report(y_test, pred_dt)
print(report)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/156:
from sklearn.svm import SVC


svc = SVC()
svc.fit(X_resampled, y_resampled)
pred_svc = svc.predict(X_test)

cm_svc = confusion_matrix(y_test, pred_svc)
acc_svc = accuracy_score(y_test, pred_svc)
score = svc.score(X_test, y_test)
results.append(acc)

print("SVC Acc : ", acc_svc)
110/157:
from sklearn.svm import SVC


svc = SVC()
svc.fit(X_resampled, y_resampled)
pred_svc = svc.predict(X_test)

cm_svc = confusion_matrix(y_test, pred_svc)
acc_svc = accuracy_score(y_test, pred_svc)
score = svc.score(X_test, y_test)
results.append(acc)

print("SVC Acc : ", acc_svc)

report_svc = classification_report(y_test, pred_svc)
print(report_svc)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svc, annot=True, fmt='d', cmap='Greens', xticklabels=svc.classes_, yticklabels=svc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/158:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix


knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc_knn = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc_knn)

print("Score: ", score)
print("KNeighborsClassifier Acc: ", acc_knn)

from sklearn.metrics import classification_report


report = classification_report(y_test, y_pred)
print(report)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/159:
from sklearn.svm import SVC


svc = SVC()
svc.fit(X_resampled, y_resampled)
pred_svc = svc.predict(X_test)

cm_svc = confusion_matrix(y_test, pred_svc)
acc_svc = accuracy_score(y_test, pred_svc)
score = svc.score(X_test, y_test)
results.append(acc_svc)

print("SVC Acc : ", acc_svc)

report_svc = classification_report(y_test, pred_svc)
print(report_svc)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svc, annot=True, fmt='d', cmap='Greens', xticklabels=svc.classes_, yticklabels=svc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/160:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_dt = dt_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_dt = confusion_matrix(y_test, pred_dt)
acc_dt = accuracy_score(y_test, pred_dt)

# Print accuracy
print("Decision Tree Classifier Accuracy: ", acc_dt)

# Generate the classification report
report = classification_report(y_test, pred_dt)
print(report)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/161:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
y_pred_rf = rf_classifier.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

acc_rf = accuracy_score(y_test, y_pred_rf)
score = rf_classifier.score(X_test, y_test)
results.append(acc_rf)

print("RandomForestClassifier Acc : ", acc_rf)


# Generate the classification report
report = classification_report(y_test, y_pred_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Print the classification report
print(report)
110/162:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()
110/163:
from sklearn.ensemble import AdaBoostClassifier

# Initialize and train the AdaBoostClassifier
ab_classifier = AdaBoostClassifier()
ab_classifier.fit(X_resampled, y_resampled)
pred_ab = ab_classifier.predict(X_test)
cm_ab = confusion_matrix(y_test, pred_ab)
acc_ab = accuracy_score(y_test, pred_ab)
results.append(acc_ab)
print("AdaBoost Classifier Accuracy: ", acc_ab)

# Generate the classification report
report_ab = classification_report(y_test, pred_ab)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_ab, annot=True, fmt='d', cmap='YlOrBr', xticklabels=ab_classifier.classes_, yticklabels=ab_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (AdaBoost)')
plt.show()

# Print the classification report
print(report_ab)
110/164:
df_result = pd.DataFrame({"Score":results, "ML Models":["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier"
             ,"XGBClassifier"]})
110/165:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix


knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc_knn = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc_knn)

print("KNeighborsClassifier Acc: ", acc_knn)

from sklearn.metrics import classification_report


report = classification_report(y_test, y_pred)
print(report)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/166:
from sklearn.svm import SVC


svc = SVC()
svc.fit(X_resampled, y_resampled)
pred_svc = svc.predict(X_test)

cm_svc = confusion_matrix(y_test, pred_svc)
acc_svc = accuracy_score(y_test, pred_svc)
score = svc.score(X_test, y_test)
results.append(acc_svc)

print("SVC Acc : ", acc_svc)

report_svc = classification_report(y_test, pred_svc)
print(report_svc)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svc, annot=True, fmt='d', cmap='Greens', xticklabels=svc.classes_, yticklabels=svc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/167:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_dt = dt_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_dt = confusion_matrix(y_test, pred_dt)
acc_dt = accuracy_score(y_test, pred_dt)

# Print accuracy
print("Decision Tree Classifier Accuracy: ", acc_dt)

# Generate the classification report
report = classification_report(y_test, pred_dt)
print(report)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/168:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
y_pred_rf = rf_classifier.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

acc_rf = accuracy_score(y_test, y_pred_rf)
score = rf_classifier.score(X_test, y_test)
results.append(acc_rf)

print("RandomForestClassifier Acc : ", acc_rf)


# Generate the classification report
report = classification_report(y_test, y_pred_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Print the classification report
print(report)
110/169:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()
110/170:
from sklearn.ensemble import AdaBoostClassifier

# Initialize and train the AdaBoostClassifier
ab_classifier = AdaBoostClassifier()
ab_classifier.fit(X_resampled, y_resampled)
pred_ab = ab_classifier.predict(X_test)
cm_ab = confusion_matrix(y_test, pred_ab)
acc_ab = accuracy_score(y_test, pred_ab)
results.append(acc_ab)
print("AdaBoost Classifier Accuracy: ", acc_ab)

# Generate the classification report
report_ab = classification_report(y_test, pred_ab)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_ab, annot=True, fmt='d', cmap='YlOrBr', xticklabels=ab_classifier.classes_, yticklabels=ab_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (AdaBoost)')
plt.show()

# Print the classification report
print(report_ab)
110/171:
from xgboost import XGBClassifier

# Initialize and train the XGBClassifier
xgb_classifier = XGBClassifier()
xgb_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_xgb = xgb_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_xgb = confusion_matrix(y_test, pred_xgb)
acc_xgb = accuracy_score(y_test, pred_xgb)

# Print accuracy
print("XGBoost Classifier Accuracy: ", acc_xgb)

# Generate the classification report
report_xgb = classification_report(y_test, pred_xgb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='pink', xticklabels=xgb_classifier.classes_, yticklabels=xgb_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (XGBoost)')
plt.show()

# Print the classification report
print(report_xgb)
110/172:
df_result = pd.DataFrame({"Score":results, "ML Models":["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier"
             ,"XGBClassifier"]})
110/173:
df_result = pd.DataFrame({"Score":results, "ML Models":["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier"
             ,"XGBClassifier"]})
110/174:
df_result_sorted = df_result.sort_values(by='Score', ascending=False)

# Print the sorted DataFrame
print(df_result_sorted)
110/175:
df_result = pd.DataFrame({"Score":results, "ML Models":["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier"
             ,"XGBClassifier"]}) 

df_result_sorted = df_result.sort_values(by='Score', ascending=False)
110/176:
df_result = pd.DataFrame({"Score":results, "ML Models":["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier"
             ,"XGBClassifier"]}) 

df_result_sorted = df_result.sort_values(by='Score', ascending=False)
110/177:
from sklearn.ensemble import AdaBoostClassifier

# Initialize and train the AdaBoostClassifier
ab_classifier = AdaBoostClassifier()
ab_classifier.fit(X_resampled, y_resampled)
pred_ab = ab_classifier.predict(X_test)
cm_ab = confusion_matrix(y_test, pred_ab)
acc_ab = accuracy_score(y_test, pred_ab)
results.append(acc_ab)
print("AdaBoost Classifier Accuracy: ", acc_ab)

# Generate the classification report
report_ab = classification_report(y_test, pred_ab)
print(report_ab)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_ab, annot=True, fmt='d', cmap='YlOrBr', xticklabels=ab_classifier.classes_, yticklabels=ab_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (AdaBoost)')
plt.show()
110/178:
from xgboost import XGBClassifier

# Initialize and train the XGBClassifier
xgb_classifier = XGBClassifier()
xgb_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_xgb = xgb_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_xgb = confusion_matrix(y_test, pred_xgb)
acc_xgb = accuracy_score(y_test, pred_xgb)

# Print accuracy
print("XGBoost Classifier Accuracy: ", acc_xgb)

# Generate the classification report
report_xgb = classification_report(y_test, pred_xgb)
print(report_xgb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='pink', xticklabels=xgb_classifier.classes_, yticklabels=xgb_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (XGBoost)')
plt.show()
110/179:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
y_pred_rf = rf_classifier.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

acc_rf = accuracy_score(y_test, y_pred_rf)
score = rf_classifier.score(X_test, y_test)
results.append(acc_rf)

print("RandomForestClassifier Acc : ", acc_rf)


# Generate the classification report
report = classification_report(y_test, y_pred_rf)
print(report)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/180:
df_result = pd.DataFrame({"Score":results, "ML Models":["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier"
             ,"XGBClassifier"]}) 

df_result_sorted = df_result.sort_values(by='Score', ascending=False)
110/181:
df_result = pd.DataFrame({"Score":results, "ML Models":["KNN","GradientBoostingClassifier",
             "SVC","XGBClassifier","CatBoostClassifier","RandomForestClassifier"]})
110/182:
import pandas as pd

# Define the list of accuracy scores and model names
results = [0.8445945945945946, 0.793918918918919, 0.8175675675675675, 0.8344594594594594, 0.8513513513513513, 0.7837837837837838, 0.847972972972973]
model_names = ["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier", "XGBoost Classifier"]

# Create the DataFrame
df_result = pd.DataFrame({"Score": results, "ML Models": model_names})

# Sort the DataFrame by 'Score' column in descending order
df_result_sorted = df_result.sort_values(by='Score', ascending=False)

# Print the sorted DataFrame
print(df_result_sorted)
110/183:
# Test with a new dataset with Gradient boosting classifier

from sklearn.ensemble import GradientBoostingClassifier

# Define the input data for prediction
new_data = {
    'Alcohol': 13.50,
    'Fixed acidity': 7.0,
    'Volatile acidity': 0.35,
    'Citric acid': 0.35,
    'Residual sugar': 2.0,
    'Chlorides': 0.023,
    'Free sulfur dioxide': 30,
    'Total sulfur dioxide': 150,
    'Density': 0.997,
    'pH': 3.3,
    'Sulfates': 0.7
}

# Create a DataFrame from the new data
new_data_df = pd.DataFrame([new_data])

# Use the Gradient Boosting model for prediction
gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
predicted_quality = gbc.predict(new_data_df)

# Print the predicted quality
print(f"The predicted quality is: {predicted_quality[0]}")
110/184:
# Test with a new dataset with Gradient boosting classifier

from sklearn.ensemble import GradientBoostingClassifier

# Define the input data for prediction
new_data = {
    'Alcohol': 13.50,
    'Fixed acidity': 7.0,
    'Volatile acidity': 0.35,
    'Citric acid': 0.35,
    'Residual sugar': 2.0,
    'Chlorides': 0.023,
    'Free sulfur dioxide': 30,
    'Total sulfur dioxide': 150,
    'Density': 0.997,
    'pH': 3.3,
    'Sulfates': 0.7
}

# Create a DataFrame from the new data
new_data_df = pd.DataFrame([new_data])

# Assuming X_train and y_train are your training data
gbc = GradientBoostingClassifier(max_depth=6, random_state=2)
gbc.fit(new_data_df)

# Now you can use the model for predictions
predicted_quality = gbc.predict(new_data_df)

# Print the predicted quality
print(f"The predicted quality is: {predicted_quality[0]}")
110/185:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)
110/186:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Example: Assuming 'new_data' contains the features for the new wine sample
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Predict quality for the new data
predicted_quality = gbc.predict(new_data)

# Print the predicted quality
print(f"The predicted quality is: {predicted_quality[0]}")
110/187:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Example: Assuming 'new_data' contains the features for the new wine sample
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Drop rows with missing values
new_data_cleaned = new_data.dropna()

# Predict quality for the cleaned data
predicted_quality = gbc.predict(new_data_cleaned)

# Print the predicted quality
print(f"The predicted quality is: {predicted_quality[0]}")
110/188:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Example: Assuming 'new_data' contains the features for the new wine sample
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Drop rows with missing values
new_data_cleaned = new_data.dropna()

# Predict quality for the cleaned data
predicted_quality = gbc.predict(new_data_cleaned)

# Print the predicted quality
print(f"The predicted quality is: {predicted_quality}")
110/189:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Example: Assuming 'new_data' contains the features for the new wine sample
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

from sklearn.impute import SimpleImputer

# Assuming 'new_data' is the DataFrame containing the new data
# Create an imputer object
imputer = SimpleImputer(strategy='mean')

# Apply the imputer to fill missing values
new_data_imputed = imputer.fit_transform(new_data)

# Create a DataFrame with the imputed values
new_data_imputed_df = pd.DataFrame(new_data_imputed, columns=new_data.columns)

# Predict quality for the new data
predicted_quality = gbc.predict(new_data_imputed_df)

# Print the predicted quality
print(f"The predicted quality is:", predicted_quality)
110/190:
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from skopt import gbrt_minimize


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Predict the quality of a new wine sample
new_wine = np.array([6.8, 0.32, 0.35, 2.2, 0.021, 33, 160, 0.997, 3.3, 0.65])
y_pred_new = gbc.predict(new_wine)

# Print the predicted quality
print('Predicted quality:', y_pred_new)
110/191:
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Predict the quality of a new wine sample
new_wine = np.array([6.8, 0.32, 0.35, 2.2, 0.021, 33, 160, 0.997, 3.3, 0.65])
y_pred_new = gbc.predict(new_wine)

# Print the predicted quality
print('Predicted quality:', y_pred_new)
110/192:
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Predict the quality of a new wine sample
# Predict the quality of a new wine sample
new_wine = np.array([6.8, 0.32, 0.35, 2.2, 0.021, 33, 160, 0.997, 3.3, 0.65])
new_wine = new_wine.reshape(1, -1)  # Reshape to make it a 2D array

# Print the reshaped array
print(new_wine)

# Predict quality for the new wine sample
y_pred_new = gbc.predict(new_wine)

# Print the predicted quality
print('Predicted quality:', y_pred_new[0])
110/193:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
110/194:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
110/195:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
110/196: data = df.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
110/197: data.describe(include="all")
110/198:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
110/199:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
110/200:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
110/201:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
110/202:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
110/203:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
110/204:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
110/205:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
110/206:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix


knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc_knn = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc_knn)

print("KNeighborsClassifier Acc: ", acc_knn)

from sklearn.metrics import classification_report


report = classification_report(y_test, y_pred)
print(report)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/207:
from sklearn.svm import SVC


svc = SVC()
svc.fit(X_resampled, y_resampled)
pred_svc = svc.predict(X_test)

cm_svc = confusion_matrix(y_test, pred_svc)
acc_svc = accuracy_score(y_test, pred_svc)
score = svc.score(X_test, y_test)
results.append(acc_svc)

print("SVC Acc : ", acc_svc)

report_svc = classification_report(y_test, pred_svc)
print(report_svc)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svc, annot=True, fmt='d', cmap='Greens', xticklabels=svc.classes_, yticklabels=svc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/208:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_dt = dt_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_dt = confusion_matrix(y_test, pred_dt)
acc_dt = accuracy_score(y_test, pred_dt)

# Print accuracy
print("Decision Tree Classifier Accuracy: ", acc_dt)

# Generate the classification report
report = classification_report(y_test, pred_dt)
print(report)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/209:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
y_pred_rf = rf_classifier.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

acc_rf = accuracy_score(y_test, y_pred_rf)
score = rf_classifier.score(X_test, y_test)
results.append(acc_rf)

print("RandomForestClassifier Acc : ", acc_rf)


# Generate the classification report
report = classification_report(y_test, y_pred_rf)
print(report)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
110/210:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()
110/211:
from sklearn.ensemble import AdaBoostClassifier

# Initialize and train the AdaBoostClassifier
ab_classifier = AdaBoostClassifier()
ab_classifier.fit(X_resampled, y_resampled)
pred_ab = ab_classifier.predict(X_test)
cm_ab = confusion_matrix(y_test, pred_ab)
acc_ab = accuracy_score(y_test, pred_ab)
results.append(acc_ab)
print("AdaBoost Classifier Accuracy: ", acc_ab)

# Generate the classification report
report_ab = classification_report(y_test, pred_ab)
print(report_ab)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_ab, annot=True, fmt='d', cmap='YlOrBr', xticklabels=ab_classifier.classes_, yticklabels=ab_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (AdaBoost)')
plt.show()
110/212:
from xgboost import XGBClassifier

# Initialize and train the XGBClassifier
xgb_classifier = XGBClassifier()
xgb_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_xgb = xgb_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_xgb = confusion_matrix(y_test, pred_xgb)
acc_xgb = accuracy_score(y_test, pred_xgb)

# Print accuracy
print("XGBoost Classifier Accuracy: ", acc_xgb)

# Generate the classification report
report_xgb = classification_report(y_test, pred_xgb)
print(report_xgb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='pink', xticklabels=xgb_classifier.classes_, yticklabels=xgb_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (XGBoost)')
plt.show()
110/213:
import pandas as pd

# Define the list of accuracy scores and model names
results = [0.8445945945945946, 0.793918918918919, 0.8175675675675675, 0.8344594594594594, 0.8513513513513513, 0.7837837837837838, 0.847972972972973]
model_names = ["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier", "XGBoost Classifier"]

# Create the DataFrame
df_result = pd.DataFrame({"Score": results, "ML Models": model_names})

# Sort the DataFrame by 'Score' column in descending order
df_result_sorted = df_result.sort_values(by='Score', ascending=False)

# Print the sorted DataFrame
print(df_result_sorted)
110/214:
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Predict the quality of a new wine sample
# Predict the quality of a new wine sample
new_wine = np.array([6.8, 0.32, 0.35, 2.2, 0.021, 33, 160, 0.997, 3.3, 0.65])
new_wine = new_wine.reshape(1, -1)  # Reshape to make it a 2D array

# Print the reshaped array
print(new_wine)

# Predict quality for the new wine sample
y_pred_new = gbc.predict(new_wine)

# Print the predicted quality
print('Predicted quality:', y_pred_new[0])
110/215:
from sklearn.linear_model import LinearRegression

# Assuming X_resampled, y_resampled are your training data
regressor = LinearRegression()
regressor.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = regressor.predict(X_test)

# Now you have continuous predictions. You can evaluate the performance using regression metrics.
# For example, you can use Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2), etc.
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')
print(f'R-squared (R2): {r2}')
110/216:
from sklearn.svm import SVR

# Assuming X_resampled, y_resampled are your training data
svr = SVR(kernel='rbf')  # You can experiment with different kernels (e.g., 'linear', 'poly')
svr.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = svr.predict(X_test)

# Now you have continuous predictions. You can evaluate the performance using regression metrics.
# For example, you can use Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2), etc.
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')
print(f'R-squared (R2): {r2}')
110/217:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeRegressor
dt_regressor = DecisionTreeRegressor()
dt_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = dt_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')
print(f'R-squared (R2): {r2}')

# If you want to visualize the predictions, you can create scatter plots
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_regression, alpha=0.5)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs. Predicted')
plt.show()
110/218:
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestRegressor
rf_regressor = RandomForestRegressor()
rf_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = rf_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')
print(f'R-squared (R2): {r2}')
110/219:
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the GradientBoostingRegressor
gbr = GradientBoostingRegressor(max_depth=6, random_state=2)
gbr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = gbr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')
print(f'R-squared (R2): {r2}')
110/220:
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the AdaBoostRegressor
abr = AdaBoostRegressor()
abr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = abr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')
print(f'R-squared (R2): {r2}')
110/221:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')
print(f'R-squared (R2): {r2}')
110/222:
model_names = ["Linear Regression", "SVR", "DecisionTreeRegressor", "RandomForestRegressor", "GradientBoostingRegressor", "AdaBoostRegressor", "XGBoost Regressor"]

# Create the DataFrame
df_result = pd.DataFrame({"Score": results, "ML Models": model_names})

# Sort the DataFrame by 'Score' column in descending order
df_result_sorted = df_result.sort_values(by='Score', ascending=False)

# Print the sorted DataFrame
print(df_result_sorted)
110/223:
from sklearn.linear_model import LinearRegression

# Assuming X_resampled, y_resampled are your training data
regressor = LinearRegression()
regressor.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = regressor.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'LinearRegression Mean Absolute Error: {mae}')
print(f'LinearRegression Mean Squared Error: {mse}')
print(f'LinearRegression R-squared (R2): {r2}')
110/224:
from sklearn.svm import SVR

# Assuming X_resampled, y_resampled are your training data
svr = SVR(kernel='rbf')  # You can experiment with different kernels (e.g., 'linear', 'poly')
svr.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = svr.predict(X_test)

# Now you have continuous predictions. You can evaluate the performance using regression metrics.
# For example, you can use Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2), etc.
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'SVR Mean Absolute Error: {mae}')
print(f'SVR Mean Squared Error: {mse}')
print(f'SVR R-squared (R2): {r2}')
110/225:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeRegressor
dt_regressor = DecisionTreeRegressor()
dt_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = dt_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'DecisionTreeRegressor Mean Absolute Error: {mae}')
print(f'DecisionTreeRegressor Mean Squared Error: {mse}')
print(f'DecisionTreeRegressor R-squared (R2): {r2}')
110/226:
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestRegressor
rf_regressor = RandomForestRegressor()
rf_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = rf_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'RandomForestRegressor Mean Absolute Error: {mae}')
print(f'RandomForestRegressor Mean Squared Error: {mse}')
print(f'RandomForestRegressor R-squared (R2): {r2}')
110/227:
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the GradientBoostingRegressor
gbr = GradientBoostingRegressor(max_depth=6, random_state=2)
gbr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = gbr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'GradientBoostingRegressor Mean Absolute Error: {mae}')
print(f'GradientBoostingRegressor Mean Squared Error: {mse}')
print(f'GradientBoostingRegressor R-squared (R2): {r2}')
110/228:
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the AdaBoostRegressor
abr = AdaBoostRegressor()
abr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = abr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'AdaBoostRegressor Mean Absolute Error: {mae}')
print(f'AdaBoostRegressor Mean Squared Error: {mse}')
print(f'AdaBoostRegressor R-squared (R2): {r2}')
110/229:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'XGBRegressor Mean Absolute Error: {mae}')
print(f'XGBRegressor Mean Squared Error: {mse}')
print(f'XGBRegressor R-squared (R2): {r2}')
110/230:
from sklearn.metrics import r2_score

# Define the regression models and their results
regression_models = ["Linear Regression", "SVR", "DecisionTreeRegressor", "RandomForestRegressor", "GradientBoostingRegressor", "AdaBoostRegressor", "XGBRegressor"]
regression_results = [-0.5396689746717418, -0.2887939459942046, -0.6303549571603424, -0.07790616075071366, -0.06021923946097263, -0.3544676952580543, 0.01844820285381421]

# Create a DataFrame to store the results
regression_df = pd.DataFrame({"Model": regression_models, "R-squared (R2)": regression_results})

# Sort the DataFrame by R-squared (higher R2 means better accuracy)
regression_df_sorted = regression_df.sort_values(by="R-squared (R2)", ascending=False)

# Print the sorted DataFrame
print(regression_df_sorted)
110/231:
import pandas as pd
from xgboost import XGBRegressor

# Load and preprocess the new dataset (replace 'new_data.csv' with your dataset)
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')


# Assuming 'X_new' contains the features for prediction
X_new = new_data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 
                 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']]

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict quality for the new dataset
predicted_quality = xgb_regressor.predict(X_new)
111/1:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
111/2:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
111/3:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
111/4:
import pandas as pd
from xgboost import XGBRegressor

# Load and preprocess the new dataset (replace 'new_data.csv' with your dataset)
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')


# Assuming 'X_new' contains the features for prediction
X_new = new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 
                 'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates']]

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict quality for the new dataset
predicted_quality = xgb_regressor.predict(X_new)
111/5:
import numpy as np
import pandas as pd
from xgboost import XGBRegressor

# Load the data
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 
                 'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates']], new_data['quality'], test_size=0.25)

# Initialize and train the XGBRegressor model
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_regressor.predict(X_test)

# Evaluate the performance of the model on the test set
mse = mean_squared_error(y_test, y_pred)
print('Mean squared error:', mse)
111/6:
import numpy as np
import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split

# Load the data
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 
                       'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates']], new_data['quality'], test_size=0.25)

# Initialize and train the XGBRegressor model
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_regressor.predict(X_test)

# Evaluate the performance of the model on the test set
mse = mean_squared_error(y_test, y_pred)
print('Mean squared error:', mse)
111/7:
import numpy as np
import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split

# Load the data
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 
                       'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates']], new_data['Quality'], test_size=0.25)

# Initialize and train the XGBRegressor model
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_regressor.predict(X_test)

# Evaluate the performance of the model on the test set
mse = mean_squared_error(y_test, y_pred)
print('Mean squared error:', mse)
111/8:
import numpy as np
import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split

# Load the data
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 
                       'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates']], new_data['Quality'], test_size=0.25)

# Initialize and train the XGBRegressor model
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_regressor.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

# Calculate the MSE
mse = mean_squared_error(y_test, y_pred)

# Calculate the RMSE
rmse = np.sqrt(mse)

# Calculate the R2 score
r2 = r2_score(y_test, y_pred)

# Print the performance metrics
print('Mean squared error:', mse)
print('Root mean squared error:', rmse)
print('R-squared:', r2)
111/9:
import numpy as np
import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split

# Load the data
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 
                       'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates']], new_data['Quality'], test_size=0.25)

# Initialize and train the XGBRegressor model
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_regressor.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

# Calculate the MSE
mse = mean_squared_error(y_test, y_pred)

# Calculate the RMSE
rmse = np.sqrt(mse)

# Calculate the R2 score
r2 = r2_score(y_test, y_pred)

# Print the performance metrics
print('Mean squared error:', mse)
print('Root mean squared error:', rmse)
print('R-squared:', r2)
111/10:
print("Missing values in y_train:", y_train.isnull().sum())
print("Missing values in y_test:", y_test.isnull().sum())
111/11:
import numpy as np
import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split

# Load the data
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 
                       'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates']], new_data['Quality'], test_size=0.25)

# Assuming y_train and y_test are pandas Series
y_train.fillna(y_train.mean(), inplace=True)
y_test.fillna(y_test.mean(), inplace=True)

# Initialize and train the XGBRegressor model
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_regressor.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

# Calculate the MSE
mse = mean_squared_error(y_test, y_pred)

# Calculate the RMSE
rmse = np.sqrt(mse)

# Calculate the R2 score
r2 = r2_score(y_test, y_pred)

# Print the performance metrics
print('Mean squared error:', mse)
print('Root mean squared error:', rmse)
print('R-squared:', r2)
111/12:
print("Missing values in y_train:", y_train.isnull().sum())
print("Missing values in y_test:", y_test.isnull().sum())

# Assuming y_train and y_test are pandas Series
y_train.fillna(y_train.mean(), inplace=True)
y_test.fillna(y_test.mean(), inplace=True)
111/13:
import numpy as np
import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split

# Load the data
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 
                       'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates']], new_data['Quality'], test_size=0.25)

# Assuming y_train and y_test are pandas Series
y_train.fillna(y_train.mean(), inplace=True)
y_test.fillna(y_test.mean(), inplace=True)

# Initialize and train the XGBRegressor model
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_regressor.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

# Calculate the MSE
mse = mean_squared_error(y_test, y_pred)

# Calculate the RMSE
rmse = np.sqrt(mse)

# Calculate the R2 score
r2 = r2_score(y_test, y_pred)

# Print the performance metrics
print('Mean squared error:', mse)
print('Root mean squared error:', rmse)
print('R-squared:', r2)
111/14:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Define the new data
new_data = [
    [7.3, 0.54, 0.10, 2.5, 0.075, 15.0, 30.0, 0.998, 3.4, 0.55, 9.5],
    [6.8, 0.41, 0.29, 1.8, 0.068, 18.0, 38.0, 0.996, 3.2, 0.56, 9.5],
    # Add the rest of the rows here in the same format
]

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)  # Assuming X_resampled and y_resampled are from your original dataset

# Predict using the trained model
y_pred_regression = xgb_regressor.predict(new_data)

# Print the predictions
print('Predicted Wine Quality:', y_pred_regression)
111/15:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
111/16:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
111/17:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
111/18: data = df.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
111/19: data.describe(include="all")
111/20:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
111/21:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
111/22:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
111/23:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
111/24:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
111/25:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
111/26:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
111/27:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
111/28:
from sklearn.linear_model import LinearRegression

# Assuming X_resampled, y_resampled are your training data
regressor = LinearRegression()
regressor.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = regressor.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'LinearRegression Mean Absolute Error: {mae}')
print(f'LinearRegression Mean Squared Error: {mse}')
print(f'LinearRegression R-squared (R2): {r2}')
111/29:
from sklearn.svm import SVR

# Assuming X_resampled, y_resampled are your training data
svr = SVR(kernel='rbf')  # You can experiment with different kernels (e.g., 'linear', 'poly')
svr.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = svr.predict(X_test)

# Now you have continuous predictions. You can evaluate the performance using regression metrics.
# For example, you can use Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2), etc.
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'SVR Mean Absolute Error: {mae}')
print(f'SVR Mean Squared Error: {mse}')
print(f'SVR R-squared (R2): {r2}')
111/30:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeRegressor
dt_regressor = DecisionTreeRegressor()
dt_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = dt_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'DecisionTreeRegressor Mean Absolute Error: {mae}')
print(f'DecisionTreeRegressor Mean Squared Error: {mse}')
print(f'DecisionTreeRegressor R-squared (R2): {r2}')
111/31:
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestRegressor
rf_regressor = RandomForestRegressor()
rf_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = rf_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'RandomForestRegressor Mean Absolute Error: {mae}')
print(f'RandomForestRegressor Mean Squared Error: {mse}')
print(f'RandomForestRegressor R-squared (R2): {r2}')
111/32:
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the GradientBoostingRegressor
gbr = GradientBoostingRegressor(max_depth=6, random_state=2)
gbr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = gbr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'GradientBoostingRegressor Mean Absolute Error: {mae}')
print(f'GradientBoostingRegressor Mean Squared Error: {mse}')
print(f'GradientBoostingRegressor R-squared (R2): {r2}')
111/33:
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the AdaBoostRegressor
abr = AdaBoostRegressor()
abr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = abr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'AdaBoostRegressor Mean Absolute Error: {mae}')
print(f'AdaBoostRegressor Mean Squared Error: {mse}')
print(f'AdaBoostRegressor R-squared (R2): {r2}')
111/34:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Define the new data
new_data = [
    [7.3, 0.54, 0.10, 2.5, 0.075, 15.0, 30.0, 0.998, 3.4, 0.55, 9.5],
    [6.8, 0.41, 0.29, 1.8, 0.068, 18.0, 38.0, 0.996, 3.2, 0.56, 9.5],
    # Add the rest of the rows here in the same format
]

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)  # Assuming X_resampled and y_resampled are from your original dataset

# Predict using the trained model
y_pred_regression = xgb_regressor.predict(new_data)

# Print the predictions
print('Predicted Wine Quality:', y_pred_regression)
111/35:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Define the new data
new_data = [
    [7.3, 0.54, 0.10, 2.5, 0.075, 15.0, 30.0, 0.998, 3.4, 0.55, 9.5],
    [6.8, 0.41, 0.29, 1.8, 0.068, 18.0, 38.0, 0.996, 3.2, 0.56, 9.5],
    # Add the rest of the rows here in the same format
]
# Assuming 'new_data' is your new dataset
# Remove the 'quality' column if it exists
if 'quality' in new_data.columns:
    new_data = new_data.drop('quality', axis=1)

# Make sure the columns are in the same order as the original dataset
new_data = new_data[X_train.columns]


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)  # Assuming X_resampled and y_resampled are from your original dataset

# Predict using the trained model
y_pred_regression = xgb_regressor.predict(new_data)

# Print the predictions
print('Predicted Wine Quality:', y_pred_regression)
111/36:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Define the new data
new_data = [
    [7.3, 0.54, 0.10, 2.5, 0.075, 15.0, 30.0, 0.998, 3.4, 0.55, 9.5],
    [6.8, 0.41, 0.29, 1.8, 0.068, 18.0, 38.0, 0.996, 3.2, 0.56, 9.5],
    # Add the rest of the rows here in the same format
]
# Assuming 'new_data' is your new dataset
# Remove the 'quality' column if it exists
if 'quality' in new_data.columns:
    new_data = new_data.drop('quality', axis=1)


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)  # Assuming X_resampled and y_resampled are from your original dataset

# Predict using the trained model
y_pred_regression = xgb_regressor.predict(new_data)

# Print the predictions
print('Predicted Wine Quality:', y_pred_regression)
111/37:
import pandas as pd
from xgboost import XGBRegressor

# Create a DataFrame with the features (excluding alcohol)
data = {
    'fixed acidity': [7.3],
    'volatile acidity': [0.54],
    'citric acid': [0.10],
    'residual sugar': [2.5],
    'chlorides': [0.075],
    'free sulfur dioxide': [15.0],
    'total sulfur dioxide': [30.0],
    'density': [0.998],
    'pH': [3.4],
    'sulphates': [0.55],
    'alcohol': [9.5]  # Set alcohol to 9.5
}

# Create a DataFrame
new_data = pd.DataFrame(data)

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = xgb_regressor.predict(new_data)

# Print the predicted wine quality
print('Predicted Wine Quality:', y_pred_regression[0])
111/38:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
111/39:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
111/40:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
111/41: data = df.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
111/42: data.describe(include="all")
111/43:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
111/44:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
111/45:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
111/46:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
111/47:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
111/48:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
111/49:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
111/50:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
111/51:
from sklearn.linear_model import LinearRegression

# Assuming X_resampled, y_resampled are your training data
regressor = LinearRegression()
regressor.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = regressor.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'LinearRegression Mean Absolute Error: {mae}')
print(f'LinearRegression Mean Squared Error: {mse}')
print(f'LinearRegression R-squared (R2): {r2}')
111/52:
from sklearn.svm import SVR

# Assuming X_resampled, y_resampled are your training data
svr = SVR(kernel='rbf')  # You can experiment with different kernels (e.g., 'linear', 'poly')
svr.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = svr.predict(X_test)

# Now you have continuous predictions. You can evaluate the performance using regression metrics.
# For example, you can use Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2), etc.
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'SVR Mean Absolute Error: {mae}')
print(f'SVR Mean Squared Error: {mse}')
print(f'SVR R-squared (R2): {r2}')
111/53:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeRegressor
dt_regressor = DecisionTreeRegressor()
dt_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = dt_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'DecisionTreeRegressor Mean Absolute Error: {mae}')
print(f'DecisionTreeRegressor Mean Squared Error: {mse}')
print(f'DecisionTreeRegressor R-squared (R2): {r2}')
111/54:
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestRegressor
rf_regressor = RandomForestRegressor()
rf_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = rf_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'RandomForestRegressor Mean Absolute Error: {mae}')
print(f'RandomForestRegressor Mean Squared Error: {mse}')
print(f'RandomForestRegressor R-squared (R2): {r2}')
111/55:
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the GradientBoostingRegressor
gbr = GradientBoostingRegressor(max_depth=6, random_state=2)
gbr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = gbr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'GradientBoostingRegressor Mean Absolute Error: {mae}')
print(f'GradientBoostingRegressor Mean Squared Error: {mse}')
print(f'GradientBoostingRegressor R-squared (R2): {r2}')
111/56:
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the AdaBoostRegressor
abr = AdaBoostRegressor()
abr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = abr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'AdaBoostRegressor Mean Absolute Error: {mae}')
print(f'AdaBoostRegressor Mean Squared Error: {mse}')
print(f'AdaBoostRegressor R-squared (R2): {r2}')
111/57:
import pandas as pd
from xgboost import XGBRegressor

# Create a DataFrame with the features (excluding alcohol)
data = {
    'fixed acidity': [7.3],
    'volatile acidity': [0.54],
    'citric acid': [0.10],
    'residual sugar': [2.5],
    'chlorides': [0.075],
    'free sulfur dioxide': [15.0],
    'total sulfur dioxide': [30.0],
    'density': [0.998],
    'pH': [3.4],
    'sulphates': [0.55],
    'alcohol': [9.5]  # Set alcohol to 9.5
}

# Create a DataFrame
new_data = pd.DataFrame(data)

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = xgb_regressor.predict(new_data)

# Print the predicted wine quality
print('Predicted Wine Quality:', y_pred_regression[0])
111/58:
import pandas as pd
from xgboost import XGBRegressor

# Create a DataFrame with the features (excluding alcohol)
data = {
data = {
    'fixed acidity': [7.3],
    'volatile acidity': [0.54],
    'citric acid': [0.10],
    'residual sugar': [2.5],
    'chlorides': [0.075],
    'free sulfur dioxide': [15.0],
    'total sulfur dioxide': [30.0],
    'density': [0.998],
    'pH': [3.4],
    'sulphates': [0.55],
    'alcohol': [9.5],  # Set alcohol to 9.5
    'quality': [0]  # Add a placeholder for quality (you can replace 0 with any value)
}
  # Set alcohol to 9.5
}

from xgboost import XGBRegressor

# Assuming you have already trained xgb_regressor

# Predict quality for the new dataset
X_new = pd.DataFrame(data)
predicted_quality = xgb_regressor.predict(X_new)

# Print the predicted wine quality
print('Predicted Wine Quality:', predicted_quality[0])
111/59:
import pandas as pd
from xgboost import XGBRegressor

# Create a DataFrame with the features (excluding alcohol)
data = {
    'fixed acidity': [7.3],
    'volatile acidity': [0.54],
    'citric acid': [0.10],
    'residual sugar': [2.5],
    'chlorides': [0.075],
    'free sulfur dioxide': [15.0],
    'total sulfur dioxide': [30.0],
    'density': [0.998],
    'pH': [3.4],
    'sulphates': [0.55],
    'alcohol': [9.5],  # Set alcohol to 9.5
    'quality': [0]  # Add a placeholder for quality (you can replace 0 with any value)
}
  # Set alcohol to 9.5
}

from xgboost import XGBRegressor

# Assuming you have already trained xgb_regressor

# Predict quality for the new dataset
X_new = pd.DataFrame(data)
predicted_quality = xgb_regressor.predict(X_new)

# Print the predicted wine quality
print('Predicted Wine Quality:', predicted_quality[0])
111/60:
import pandas as pd
from xgboost import XGBRegressor

# Create a DataFrame with the features (excluding alcohol)
data = {
    'fixed acidity': [7.3],
    'volatile acidity': [0.54],
    'citric acid': [0.10],
    'residual sugar': [2.5],
    'chlorides': [0.075],
    'free sulfur dioxide': [15.0],
    'total sulfur dioxide': [30.0],
    'density': [0.998],
    'pH': [3.4],
    'sulphates': [0.55],
    'alcohol': [9.5],  # Set alcohol to 9.5
    'quality': [0]  # Add a placeholder for quality (you can replace 0 with any value)
}


from xgboost import XGBRegressor

# Assuming you have already trained xgb_regressor

# Predict quality for the new dataset
X_new = pd.DataFrame(data)
predicted_quality = xgb_regressor.predict(X_new)

# Print the predicted wine quality
print('Predicted Wine Quality:', predicted_quality[0])
111/61:
from lightgbm import LGBMRegressor
import pandas as pd
from xgboost import XGBRegressor

# Create a DataFrame with the features (excluding alcohol)
data = {
    'fixed acidity': [7.3],
    'volatile acidity': [0.54],
    'citric acid': [0.10],
    'residual sugar': [2.5],
    'chlorides': [0.075],
    'free sulfur dioxide': [15.0],
    'total sulfur dioxide': [30.0],
    'density': [0.998],
    'pH': [3.4],
    'sulphates': [0.55],
    'alcohol': [9.5],  # Set alcohol to 9.5
    'quality': [0]  # Add a placeholder for quality (you can replace 0 with any value)
}


from xgboost import XGBRegressor

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict quality for the new dataset
X_new = pd.DataFrame(data)
predicted_quality = XGBRegressor.predict(X_new)

# Print the predicted wine quality
print('Predicted Wine Quality:', predicted_quality[0])
111/62:
from lightgbm import LGBMRegressor
import pandas as pd
from xgboost import XGBRegressor

# Create a DataFrame with the features (excluding alcohol)
data = {
    'fixed acidity': [7.3],
    'volatile acidity': [0.54],
    'citric acid': [0.10],
    'residual sugar': [2.5],
    'chlorides': [0.075],
    'free sulfur dioxide': [15.0],
    'total sulfur dioxide': [30.0],
    'density': [0.998],
    'pH': [3.4],
    'sulphates': [0.55],
    'alcohol': [9.5],  # Set alcohol to 9.5
    'quality': [0]  # Add a placeholder for quality (you can replace 0 with any value)
}

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict quality for the new dataset
X_new = pd.DataFrame(data)
predicted_quality = xgb_regressor.predict(X_new)

# Print the predicted wine quality
print('Predicted Wine Quality:', predicted_quality[0])
112/1:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
112/2:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
112/3:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
112/4:
from lightgbm import LGBMRegressor
import pandas as pd
from xgboost import XGBRegressor

# Create a DataFrame with the features (excluding alcohol)
data = {
    'fixed acidity': [7.3],
    'volatile acidity': [0.54],
    'citric acid': [0.10],
    'residual sugar': [2.5],
    'chlorides': [0.075],
    'free sulfur dioxide': [15.0],
    'total sulfur dioxide': [30.0],
    'density': [0.998],
    'pH': [3.4],
    'sulphates': [0.55],
    'alcohol': [9.5],  # Set alcohol to 9.5
    'quality': [0]  # Add a placeholder for quality (you can replace 0 with any value)
}

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict quality for the new dataset
X_new = pd.DataFrame(data)
predicted_quality = xgb_regressor.predict(X_new)

import xgboost as xgb
import pandas as pd

# Load the new data
new_data = pd.read_csv('new_data.csv')

# Check if the new data contains all of the required columns
required_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']

for column in required_columns:
    if column not in new_data.columns:
        raise ValueError('The new data must contain all of the following columns: {}'.format(required_columns))

# Make predictions on the new data
predictions = xgb_regressor.predict(new_data)
112/5:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
112/6:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
112/7:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
112/8:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
112/9:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
112/10:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
112/11:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
112/12:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
112/13:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
112/14:
from collections import Counter

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
112/15:
from collections import Counter

import numpy as np

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
112/16: data = df.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
112/17: data.describe(include="all")
112/18:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
112/19:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
112/20:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
112/21:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
112/22:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
112/23:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
112/24:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
112/25:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
112/26:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
112/27:
from sklearn.linear_model import LinearRegression

# Assuming X_resampled, y_resampled are your training data
regressor = LinearRegression()
regressor.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = regressor.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'LinearRegression Mean Absolute Error: {mae}')
print(f'LinearRegression Mean Squared Error: {mse}')
print(f'LinearRegression R-squared (R2): {r2}')
112/28:
from sklearn.svm import SVR

# Assuming X_resampled, y_resampled are your training data
svr = SVR(kernel='rbf')  # You can experiment with different kernels (e.g., 'linear', 'poly')
svr.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = svr.predict(X_test)

# Now you have continuous predictions. You can evaluate the performance using regression metrics.
# For example, you can use Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2), etc.
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'SVR Mean Absolute Error: {mae}')
print(f'SVR Mean Squared Error: {mse}')
print(f'SVR R-squared (R2): {r2}')
112/29:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeRegressor
dt_regressor = DecisionTreeRegressor()
dt_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = dt_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'DecisionTreeRegressor Mean Absolute Error: {mae}')
print(f'DecisionTreeRegressor Mean Squared Error: {mse}')
print(f'DecisionTreeRegressor R-squared (R2): {r2}')
112/30:
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestRegressor
rf_regressor = RandomForestRegressor()
rf_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = rf_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'RandomForestRegressor Mean Absolute Error: {mae}')
print(f'RandomForestRegressor Mean Squared Error: {mse}')
print(f'RandomForestRegressor R-squared (R2): {r2}')
112/31:
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the GradientBoostingRegressor
gbr = GradientBoostingRegressor(max_depth=6, random_state=2)
gbr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = gbr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'GradientBoostingRegressor Mean Absolute Error: {mae}')
print(f'GradientBoostingRegressor Mean Squared Error: {mse}')
print(f'GradientBoostingRegressor R-squared (R2): {r2}')
112/32:
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the AdaBoostRegressor
abr = AdaBoostRegressor()
abr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = abr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'AdaBoostRegressor Mean Absolute Error: {mae}')
print(f'AdaBoostRegressor Mean Squared Error: {mse}')
print(f'AdaBoostRegressor R-squared (R2): {r2}')
112/33:
from lightgbm import LGBMRegressor
import pandas as pd
from xgboost import XGBRegressor

# Create a DataFrame with the features (excluding alcohol)
data = {
    'fixed acidity': [7.3],
    'volatile acidity': [0.54],
    'citric acid': [0.10],
    'residual sugar': [2.5],
    'chlorides': [0.075],
    'free sulfur dioxide': [15.0],
    'total sulfur dioxide': [30.0],
    'density': [0.998],
    'pH': [3.4],
    'sulphates': [0.55],
    'alcohol': [9.5],  # Set alcohol to 9.5
    'quality': [0]  # Add a placeholder for quality (you can replace 0 with any value)
}

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict quality for the new dataset
X_new = pd.DataFrame(data)
predicted_quality = xgb_regressor.predict(X_new)

import xgboost as xgb
import pandas as pd

# Load the new data
new_data = pd.read_csv('new_data.csv')

# Check if the new data contains all of the required columns
required_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']

for column in required_columns:
    if column not in new_data.columns:
        raise ValueError('The new data must contain all of the following columns: {}'.format(required_columns))

# Make predictions on the new data
predictions = xgb_regressor.predict(new_data)
112/34:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')
112/35:
from lightgbm import LGBMRegressor
import pandas as pd
from xgboost import XGBRegressor

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict quality for the new dataset
X_new = pd.DataFrame(data)
predicted_quality = xgb_regressor.predict(X_new)

import xgboost as xgb
import pandas as pd

# Load the new data
new_data = pd.read_csv('new_data.csv')

# Check if the new data contains all of the required columns
required_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']

for column in required_columns:
    if column not in new_data.columns:
        raise ValueError('The new data must contain all of the following columns: {}'.format(required_columns))

# Make predictions on the new data
predictions = xgb_regressor.predict(new_data)
112/36:
from sklearn.metrics import r2_score

# Define the regression models and their results
regression_models = ["Linear Regression", "SVR", "DecisionTreeRegressor", "RandomForestRegressor", "GradientBoostingRegressor", "AdaBoostRegressor", "XGBRegressor"]
regression_results = [-0.5396689746717418, -0.2887939459942046, -0.6303549571603424, -0.07790616075071366, -0.06021923946097263, -0.3544676952580543, 0.01844820285381421]

# Create a DataFrame to store the results
regression_df = pd.DataFrame({"Model": regression_models, "R-squared (R2)": regression_results})

# Sort the DataFrame by R-squared (higher R2 means better accuracy)
regression_df_sorted = regression_df.sort_values(by="R-squared (R2)", ascending=False)

# Print the sorted DataFrame
print(regression_df_sorted)
112/37:
from sklearn.metrics import r2_score

# Define the regression models and their results
regression_models = ["Linear Regression", "SVR", "DecisionTreeRegressor", "RandomForestRegressor", "GradientBoostingRegressor", "AdaBoostRegressor", "XGBRegressor"]
regression_results = [-0.5396689746717418, -0.2887939459942046, -0.6303549571603424, -0.07790616075071366, -0.06021923946097263, -0.3544676952580543, 0.01844820285381421]

# Create a DataFrame to store the results
regression_df = pd.DataFrame({"Model": regression_models, "R-squared (R2)": regression_results})

# Sort the DataFrame by R-squared (higher R2 means better accuracy)
regression_df_sorted = regression_df.sort_values(by="R-squared (R2)", ascending=False)

# Print the sorted DataFrame
print(regression_df_sorted)
112/38:
import numpy as np
import pandas as pd
from xgboost import XGBRegressor

# Load the data
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 
                             'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates']], new_data['Quality'], test_size=0.25)

# Initialize and train the XGBRegressor model
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_regressor.predict(X_test)

# Print the predicted quality for each sample
for i in range(len(X_test)):
    print('Predicted quality for sample {}: {}'.format(i + 1, y_pred[i]))
112/39: print(y_train.isna().sum())
112/40:
import numpy as np
import pandas as pd
from xgboost import XGBRegressor

# Load the data
new_data = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 
                             'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates']], new_data['Quality'], test_size=0.25)

# Initialize and train the XGBRegressor model
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_regressor.predict(X_test)

# Print the predicted quality for each sample
for i in range(len(X_test)):
    print('Predicted quality for sample {}: {}'.format(i + 1, y_pred[i]))
112/41:
# Assuming 'new_data' is a DataFrame containing the new data
new_data = pd.DataFrame({
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'Residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'Chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
})

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data)

# Add the predicted quality to the new_data DataFrame
new_data['Predicted Quality'] = predicted_quality

# Display the DataFrame with predicted quality
print(new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates', 'Predicted Quality']])
112/42:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')

# Assuming 'new_data' is a DataFrame containing the new data
new_data = pd.DataFrame({
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'Residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'Chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
})

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data)

# Add the predicted quality to the new_data DataFrame
new_data['Predicted Quality'] = predicted_quality

# Display the DataFrame with predicted quality
print(new_data[['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides', 'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates', 'Predicted Quality']])
112/43:
# Import necessary libraries
import pandas as pd
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Assuming you have already loaded and preprocessed your data into X_resampled and y_resampled

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Define new data for prediction
new_data = pd.DataFrame({
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [2.2, 2.4, 2.6, 2.8, 3.0, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 2.2, 2.4, 2.6],
    'Residual sugar': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Chlorides': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Free sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Total sulfur dioxide': [0.997, 0.998, 0.999, 1.0, 1.001, 0.996, 0.997, 0.998, 0.999, 1.0, 1.001, 0.997, 0.998, 0.999],
    'Density': [3.3, 3.2, 3.1, 3.0, 2.9, 3.4, 3.3, 3.2, 3.1, 3.0, 2.9, 3.3, 3.2, 3.1],
    'pH': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
})

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data)

# Add the predicted quality to the new_data DataFrame
new_data['Predicted Quality'] = predicted_quality

# Print the new_data DataFrame with predicted quality
print(new_data)
112/44:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')

new_data = pd.DataFrame({
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [2.2, 2.4, 2.6, 2.8, 3.0, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 2.2, 2.4, 2.6],
    'Residual sugar': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Chlorides': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Free sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Total sulfur dioxide': [0.997, 0.998, 0.999, 1.0, 1.001, 0.996, 0.997, 0.998, 0.999, 1.0, 1.001, 0.997, 0.998, 0.999],
    'Density': [3.3, 3.2, 3.1, 3.0, 2.9, 3.4, 3.3, 3.2, 3.1, 3.0, 2.9, 3.3, 3.2, 3.1],
    'pH': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
})

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data)

# Add the predicted quality to the new_data DataFrame
new_data['Predicted Quality'] = predicted_quality

# Print the new_data DataFrame with predicted quality
print(new_data)
112/45:
# Import necessary libraries
import pandas as pd
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Assuming you have already loaded and preprocessed your data into X_resampled and y_resampled

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Define new data for prediction
# Import necessary libraries
import pandas as pd
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Assuming you have already loaded and preprocessed your data into X_resampled and y_resampled

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Define new data for prediction
new_data = pd.DataFrame({
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [2.2, 2.4, 2.6, 2.8, 3.0, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 2.2, 2.4, 2.6],
    'Residual sugar': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Chlorides': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Free sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Total sulfur dioxide': [0.997, 0.998, 0.999, 1.0, 1.001, 0.996, 0.997, 0.998, 0.999, 1.0, 1.001, 0.997, 0.998, 0.999],
    'Density': [3.3, 3.2, 3.1, 3.0, 2.9, 3.4, 3.3, 3.2, 3.1, 3.0, 2.9, 3.3, 3.2, 3.1],
    'pH': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
})

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data)

# Add the predicted quality to the new_data DataFrame
new_data['Predicted Quality'] = predicted_quality

# Print the new_data DataFrame with predicted quality
print(new_data)


# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data)

# Add the predicted quality to the new_data DataFrame
new_data['Predicted Quality'] = predicted_quality

# Print the new_data DataFrame with predicted quality
print(new_data)
112/46:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')
112/47:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')
112/48:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')
112/49:
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the AdaBoostRegressor
abr = AdaBoostRegressor()
abr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = abr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'AdaBoostRegressor Mean Absolute Error: {mae}')
print(f'AdaBoostRegressor Mean Squared Error: {mse}')
print(f'AdaBoostRegressor R-squared (R2): {r2}')
112/50:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
112/51:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
112/52:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
112/53:
from collections import Counter

import numpy as np

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
112/54: data = df.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
112/55: data.describe(include="all")
112/56:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
112/57:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
112/58:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
112/59:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
112/60:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
112/61:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
112/62:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
112/63:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
112/64:
from sklearn.linear_model import LinearRegression

# Assuming X_resampled, y_resampled are your training data
regressor = LinearRegression()
regressor.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = regressor.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'LinearRegression Mean Absolute Error: {mae}')
print(f'LinearRegression Mean Squared Error: {mse}')
print(f'LinearRegression R-squared (R2): {r2}')
112/65:
from sklearn.svm import SVR

# Assuming X_resampled, y_resampled are your training data
svr = SVR(kernel='rbf')  # You can experiment with different kernels (e.g., 'linear', 'poly')
svr.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = svr.predict(X_test)

# Now you have continuous predictions. You can evaluate the performance using regression metrics.
# For example, you can use Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2), etc.
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'SVR Mean Absolute Error: {mae}')
print(f'SVR Mean Squared Error: {mse}')
print(f'SVR R-squared (R2): {r2}')
112/66:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeRegressor
dt_regressor = DecisionTreeRegressor()
dt_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = dt_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'DecisionTreeRegressor Mean Absolute Error: {mae}')
print(f'DecisionTreeRegressor Mean Squared Error: {mse}')
print(f'DecisionTreeRegressor R-squared (R2): {r2}')
112/67:
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestRegressor
rf_regressor = RandomForestRegressor()
rf_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = rf_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'RandomForestRegressor Mean Absolute Error: {mae}')
print(f'RandomForestRegressor Mean Squared Error: {mse}')
print(f'RandomForestRegressor R-squared (R2): {r2}')
112/68:
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the GradientBoostingRegressor
gbr = GradientBoostingRegressor(max_depth=6, random_state=2)
gbr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = gbr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'GradientBoostingRegressor Mean Absolute Error: {mae}')
print(f'GradientBoostingRegressor Mean Squared Error: {mse}')
print(f'GradientBoostingRegressor R-squared (R2): {r2}')
112/69:
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the AdaBoostRegressor
abr = AdaBoostRegressor()
abr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = abr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'AdaBoostRegressor Mean Absolute Error: {mae}')
print(f'AdaBoostRegressor Mean Squared Error: {mse}')
print(f'AdaBoostRegressor R-squared (R2): {r2}')
112/70:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')
112/71:
from sklearn.metrics import r2_score

# Define the regression models and their results
regression_models = ["Linear Regression", "SVR", "DecisionTreeRegressor", "RandomForestRegressor", "GradientBoostingRegressor", "AdaBoostRegressor", "XGBRegressor"]
regression_results = [-0.5396689746717418, -0.2887939459942046, -0.6303549571603424, -0.07790616075071366, -0.06021923946097263, -0.3544676952580543, 0.01844820285381421]

# Create a DataFrame to store the results
regression_df = pd.DataFrame({"Model": regression_models, "R-squared (R2)": regression_results})

# Sort the DataFrame by R-squared (higher R2 means better accuracy)
regression_df_sorted = regression_df.sort_values(by="R-squared (R2)", ascending=False)

# Print the sorted DataFrame
print(regression_df_sorted)
112/72:
# Import necessary libraries
import pandas as pd
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Assuming you have already loaded and preprocessed your data into X_resampled and y_resampled

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Define new data for prediction
# Import necessary libraries
import pandas as pd
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Assuming you have already loaded and preprocessed your data into X_resampled and y_resampled

# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Define new data for prediction
new_data = pd.DataFrame({
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [2.2, 2.4, 2.6, 2.8, 3.0, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 2.2, 2.4, 2.6],
    'Residual sugar': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Chlorides': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Free sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Total sulfur dioxide': [0.997, 0.998, 0.999, 1.0, 1.001, 0.996, 0.997, 0.998, 0.999, 1.0, 1.001, 0.997, 0.998, 0.999],
    'Density': [3.3, 3.2, 3.1, 3.0, 2.9, 3.4, 3.3, 3.2, 3.1, 3.0, 2.9, 3.3, 3.2, 3.1],
    'pH': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
})

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data)

# Add the predicted quality to the new_data DataFrame
new_data['Predicted Quality'] = predicted_quality

# Print the new_data DataFrame with predicted quality
print(new_data)


# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data)

# Add the predicted quality to the new_data DataFrame
new_data['Predicted Quality'] = predicted_quality

# Print the new_data DataFrame with predicted quality
print(new_data)
112/73:
new_data = {
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'Residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'Chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}
112/74:
from sklearn.preprocessing import StandardScaler

new_data = {
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'Residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'Chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

scaler = StandardScaler()
new_data_scaled = scaler.fit_transform(new_data)
112/75:
from sklearn.preprocessing import StandardScaler
import numpy as np

new_data = {
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'Residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'Chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

new_data_array = np.array(new_data)
new_data_scaled = scaler.fit_transform(new_data_array)
112/76:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Define the StandardScaler object
scaler = StandardScaler()

new_data = {
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'Residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'Chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

new_data_array = np.array([list(row.values()) for row in new_data.values()])
new_data_scaled = scaler.fit_transform(new_data_array)

print(new_data_scaled)
112/77:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Define the StandardScaler object
scaler = StandardScaler()

new_data = {
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'Residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'Chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Convert the dictionary values into a NumPy array
new_data_array = np.array(list(new_data.values()))

# Scale the data
new_data_scaled = scaler.fit_transform(new_data_array)

print(new_data_scaled)
112/78:
import xgboost as xgb

xgb_regressor = XGBRegressor()
xgb_regressor.fit(new_data_scaled, np.ones(len(new_data_scaled)))
112/79:
import xgboost as xgb

xgb_regressor = XGBRegressor()
xgb_regressor.fit(new_data_scaled, np.ones(len(new_data_scaled)))

y_pred_xgb = xgb_regressor.predict(new_data_scaled)
112/80:
import xgboost as xgb

xgb_regressor = XGBRegressor()
xgb_regressor.fit(new_data_scaled, np.ones(len(new_data_scaled)))

y_pred_xgb = xgb_regressor.predict(new_data_scaled)

y_pred_xgb_scaled = scaler.inverse_transform(y_pred_xgb)
112/81:
import xgboost as xgb

xgb_regressor = XGBRegressor()
xgb_regressor.fit(new_data_scaled, np.ones(len(new_data_scaled)))

y_pred_xgb = xgb_regressor.predict(new_data_scaled)
112/82:
import xgboost as xgb

xgb_regressor = XGBRegressor()
xgb_regressor.fit(new_data_scaled, np.ones(len(new_data_scaled)))

y_pred_xgb = xgb_regressor.predict(new_data_scaled)

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data_scaled)

# Add the predicted quality to the new_data DataFrame
new_data['Predicted Quality'] = predicted_quality

# Print the new_data DataFrame with predicted quality
print(new_data)
112/83:
import xgboost as xgb

xgb_regressor = XGBRegressor()
xgb_regressor.fit(new_data_scaled, np.ones(len(new_data_scaled)))

y_pred_xgb = xgb_regressor.predict(new_data_scaled)

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data_scaled)

# Add the predicted quality to the new_data DataFrame
new_data['Predicted Quality'] = predicted_quality

# Print the new_data DataFrame with predicted quality
display(new_data)
112/84:
from sklearn.metrics import r2_score

# Define the regression models and their results
regression_models = ["Linear Regression", "SVR", "DecisionTreeRegressor", "RandomForestRegressor", "GradientBoostingRegressor", "AdaBoostRegressor", "XGBRegressor"]
regression_results = [-0.5396689746717418, -0.2887939459942046, -0.6303549571603424, -0.07790616075071366, -0.06021923946097263, -0.3544676952580543, 0.01844820285381421]

# Create a DataFrame to store the results
regression_df = pd.DataFrame({"Model": regression_models, "R-squared (R2)": regression_results})

# Sort the DataFrame by R-squared (higher R2 means better accuracy)
regression_df_sorted = regression_df.sort_values(by="R-squared (R2)", ascending=False)

# Print the sorted DataFrame
print(regression_df_sorted)
112/85:
from sklearn.metrics import r2_score

# Define the regression models and their results
regression_models = ["Linear Regression", "SVR", "DecisionTreeRegressor", "RandomForestRegressor", "GradientBoostingRegressor", "AdaBoostRegressor", "XGBRegressor"]
regression_results = [-0.5396689746717418, -0.2887939459942046, -0.6303549571603424, -0.07790616075071366, -0.06021923946097263, -0.3544676952580543, 0.01844820285381421]

# Create a DataFrame to store the results
regression_df = pd.DataFrame({"Model": regression_models, "R-squared (R2)": regression_results})

# Sort the DataFrame by R-squared (higher R2 means better accuracy)
regression_df_sorted = regression_df.sort_values(by="R-squared (R2)", ascending=False)

# Print the sorted DataFrame
print(regression_df_sorted)
112/86:
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestRegressor
rf_regressor = RandomForestRegressor()
rf_regressor.fit(new_data_scaled, np.ones(len(new_data_scaled)))

y_pred_rf_regressor = rf_regressor.predict(new_data_scaled)

# Predict the quality using the trained XGBRegressor
predicted_quality = rf_regressor.predict(new_data_scaled)

# Add the predicted quality to the new_data DataFrame
new_data['Predicted Quality'] = predicted_quality

# Print the new_data DataFrame with predicted quality
display(new_data)
112/87:
import numpy as np

# Assuming y_pred_xgb contains the predicted values
y_pred_xgb = np.array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])  # Example values

# Apply sigmoid transformation
predicted_quality = 10 / (1 + np.exp(-y_pred_xgb))

# Round to integers for quality scores
predicted_quality = np.round(predicted_quality)

# Ensure values are in the range of 0 to 10
predicted_quality = np.clip(predicted_quality, 0, 10)

# Print the predicted quality
print(predicted_quality)
112/88:
from sklearn.preprocessing import StandardScaler
import numpy as np
import xgboost as xgb

# Define the StandardScaler object
scaler = StandardScaler()

# Assuming you have your training data and labels
# X_train, y_train = ...

# Assuming you have trained your XGBoost Regressor
# xgb_regressor = ...

# Assuming you have new data
new_data = {
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'Residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'Chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Convert the dictionary values into a NumPy array
new_data_array = np.array(list(new_data.values()))

# Scale the data using the same scaler you used for training
new_data_scaled = scaler.transform(new_data_array)

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data_scaled)

# Apply transformation to ensure the output is in the range of 0 to 10
predicted_quality = 10 / (1 + np.exp(-predicted_quality))
predicted_quality = np.round(predicted_quality)
predicted_quality = np.clip(predicted_quality, 0, 10)

# Print the predicted quality
print(predicted_quality)
112/89:
from sklearn.preprocessing import StandardScaler
import numpy as np
import xgboost as xgb

# Define the StandardScaler object
scaler = StandardScaler()

# Assuming you have your training data and labels
# X_train, y_train = ...

# Assuming you have trained your XGBoost Regressor
# xgb_regressor = ...

# Assuming you have new data
new_data = {
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'Residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'Chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Convert the dictionary values into a NumPy array
new_data_array = np.array(list(new_data.values()))

# Scale the data using the same scaler you used for training
new_data_scaled = scaler.transform(new_data_array)

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data_scaled)

# Apply transformation to ensure the output is in the range of 0 to 10
predicted_quality = 10 / (1 + np.exp(-predicted_quality))
predicted_quality = np.round(predicted_quality)
predicted_quality = np.clip(predicted_quality, 0, 10)

# Print the predicted quality
print(predicted_quality)
112/90:
import xgboost as xgb

xgb_regressor = XGBRegressor()
xgb_regressor.fit(new_data_scaled, np.ones(len(new_data_scaled)))

# Convert the dictionary values into a NumPy array
new_data_array = np.array(list(new_data.values()))

# Scale the data using the fitted scaler
new_data_scaled = scaler.transform(new_data_array)

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data_scaled)

# Apply transformation to ensure the output is in the range of 0 to 10
predicted_quality = 10 / (1 + np.exp(-predicted_quality))
predicted_quality = np.round(predicted_quality)
predicted_quality = np.clip(predicted_quality, 0, 10)


# Print the predicted quality
print(predicted_quality)
112/91:
from sklearn.preprocessing import StandardScaler
import numpy as np

# Define the StandardScaler object
scaler = StandardScaler()

new_data = {
    'Fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'Volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'Citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'Residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'Chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'Free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'Total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'Density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'Sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Convert the dictionary values into a NumPy array
new_data_array = np.array(list(new_data.values()))

# Scale the data
new_data_scaled = scaler.fit_transform(new_data_array)

print(new_data_scaled)
112/92:
import xgboost as xgb

xgb_regressor = XGBRegressor()
xgb_regressor.fit(new_data_scaled, np.ones(len(new_data_scaled)))

# Convert the dictionary values into a NumPy array
new_data_array = np.array(list(new_data.values()))

# Scale the data using the fitted scaler
new_data_scaled = scaler.transform(new_data_array)

# Predict the quality using the trained XGBRegressor
predicted_quality = xgb_regressor.predict(new_data_scaled)

# Apply transformation to ensure the output is in the range of 0 to 10
predicted_quality = 10 / (1 + np.exp(-predicted_quality))
predicted_quality = np.round(predicted_quality)
predicted_quality = np.clip(predicted_quality, 0, 10)


# Print the predicted quality
print(predicted_quality)
112/93:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
print(df)
112/94:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')
112/95:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')

new_data = {
    'fixed acidity': [7.1, 7.9, 6.3, 8.5, 5.6],
    'volatile acidity': [0.45, 0.35, 0.29, 0.23, 0.65],
    'citric acid': [0.33, 0.44, 0.21, 0.5, 0.8],
    'residual sugar': [2.1, 2.9, 1.8, 2.6, 3.0],
    'chlorides': [0.067, 0.095, 0.051, 0.046, 0.089],
    'free sulfur dioxide': [15, 30, 10, 20, 35],
    'total sulfur dioxide': [45, 70, 30, 50, 80],
    'density': [0.9955, 0.9968, 0.9936, 0.9952, 0.998],
    'pH': [3.38, 3.2, 3.3, 3.15, 3.45],
    'sulphates': [0.68, 0.72, 0.6, 0.58, 0.75],
    'alcohol': [10.2, 10.5, 11.0, 9.8, 11.5]
}

new_df = pd.DataFrame(new_data)

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
112/96:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')

new_data = {
    'fixed acidity': [7.1, 7.9, 6.3, 8.5, 5.6],
    'volatile acidity': [0.45, 0.35, 0.29, 0.23, 0.65],
    'citric acid': [0.33, 0.44, 0.21, 0.5, 0.8],
    'residual sugar': [2.1, 2.9, 1.8, 2.6, 3.0],
    'chlorides': [0.067, 0.095, 0.051, 0.046, 0.089],
    'free sulfur dioxide': [15, 30, 10, 20, 35],
    'total sulfur dioxide': [45, 70, 30, 50, 80],
    'density': [0.9955, 0.9968, 0.9936, 0.9952, 0.998],
    'pH': [3.38, 3.2, 3.3, 3.15, 3.45],
    'sulphates': [0.68, 0.72, 0.6, 0.58, 0.75],
    'alcohol': [10.2, 10.5, 11.0, 9.8, 11.5]
}

new_df = pd.DataFrame(new_data)

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
112/97:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')
112/98:
import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Assuming 'quality' is the target variable
X = new_df.drop('quality', axis=1)
y = new_df['quality']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train a new XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
112/99:
import pandas as pd
from xgboost import XGBRegressor

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Assuming 'quality' is the target variable in your original model
# If 'quality' is not available, this step is not needed
# X = new_df.drop('quality', axis=1)
# y = new_df['quality']

# Load the trained model
xgb_regressor = XGBRegressor()
xgb_regressor.load_model('wine_quality_model.json')

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
112/100:
import pandas as pd
from xgboost import XGBRegressor

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Assuming 'quality' is the target variable in your original model
# If 'quality' is not available, this step is not needed
# X = new_df.drop('quality', axis=1)
# y = new_df['quality']

# Load the trained model
xgb_regressor = XGBRegressor()
xgb_regressor.load_model(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\new red wine data.csv')

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
112/101:
import pandas as pd
from xgboost import XGBRegressor

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Assuming 'quality' is the target variable in your original model
# If 'quality' is not available, this step is not needed
# X = new_df.drop('quality', axis=1)
# y = new_df['quality']

# Load the trained model
xgb_regressor = XGBRegressor()
xgb_regressor.load_model(r'C:\path\to\wine_quality_model.json')


# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
112/102:
import pandas as pd
from xgboost import XGBRegressor

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Assuming 'quality' is the target variable in your original model
# If 'quality' is not available, this step is not needed
# X = new_df.drop('quality', axis=1)
# y = new_df['quality']

# Load the trained model
xgb_regressor = XGBRegressor()
xgb_regressor.load_model(r'C:\\Users\\sound\\OneDrive\\Desktop\\main project\\python project main\\supervised classification type dataset\\wine_quality_model.json')



# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
112/103:
import pandas as pd
from xgboost import XGBRegressor

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Assuming 'quality' is the target variable in your original model
# If 'quality' is not available, this step is not needed
# X = new_df.drop('quality', axis=1)
# y = new_df['quality']

# Train a new model with the new data
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X, y)

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
112/104:
from sklearn.metrics import r2_score

# Define the regression models and their results
regression_models = ["Linear Regression", "SVR", "DecisionTreeRegressor", "RandomForestRegressor", "GradientBoostingRegressor", "AdaBoostRegressor", "XGBRegressor"]
regression_results = [-0.5396689746717418, -0.2887939459942046, -0.6303549571603424, -0.07790616075071366, -0.06021923946097263, -0.3544676952580543, 0.01844820285381421]

# Create a DataFrame to store the results
regression_df = pd.DataFrame({"Model": regression_models, "R-squared (R2)": regression_results})

# Sort the DataFrame by R-squared (higher R2 means better accuracy)
regression_df_sorted = regression_df.sort_values(by="R-squared (R2)", ascending=False)

# Print the sorted DataFrame
print(regression_df_sorted)
112/105:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')

import pandas as pd
from xgboost import XGBRegressor

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Train a new model with the new data
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X, y)

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
112/106:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
print(df)
112/107:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
112/108:
from collections import Counter

import numpy as np

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
112/109: data = df.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
112/110: data.describe(include="all")
112/111:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
112/112:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
112/113:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
112/114:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
112/115:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
112/116:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
112/117:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
112/118:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
112/119:
from sklearn.linear_model import LinearRegression

# Assuming X_resampled, y_resampled are your training data
regressor = LinearRegression()
regressor.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = regressor.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'LinearRegression Mean Absolute Error: {mae}')
print(f'LinearRegression Mean Squared Error: {mse}')
print(f'LinearRegression R-squared (R2): {r2}')
112/120:
from sklearn.svm import SVR

# Assuming X_resampled, y_resampled are your training data
svr = SVR(kernel='rbf')  # You can experiment with different kernels (e.g., 'linear', 'poly')
svr.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = svr.predict(X_test)

# Now you have continuous predictions. You can evaluate the performance using regression metrics.
# For example, you can use Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2), etc.
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'SVR Mean Absolute Error: {mae}')
print(f'SVR Mean Squared Error: {mse}')
print(f'SVR R-squared (R2): {r2}')
112/121:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeRegressor
dt_regressor = DecisionTreeRegressor()
dt_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = dt_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'DecisionTreeRegressor Mean Absolute Error: {mae}')
print(f'DecisionTreeRegressor Mean Squared Error: {mse}')
print(f'DecisionTreeRegressor R-squared (R2): {r2}')
112/122:
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestRegressor
rf_regressor = RandomForestRegressor()
rf_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = rf_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'RandomForestRegressor Mean Absolute Error: {mae}')
print(f'RandomForestRegressor Mean Squared Error: {mse}')
print(f'RandomForestRegressor R-squared (R2): {r2}')
112/123:
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the GradientBoostingRegressor
gbr = GradientBoostingRegressor(max_depth=6, random_state=2)
gbr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = gbr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'GradientBoostingRegressor Mean Absolute Error: {mae}')
print(f'GradientBoostingRegressor Mean Squared Error: {mse}')
print(f'GradientBoostingRegressor R-squared (R2): {r2}')
112/124:
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the AdaBoostRegressor
abr = AdaBoostRegressor()
abr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = abr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'AdaBoostRegressor Mean Absolute Error: {mae}')
print(f'AdaBoostRegressor Mean Squared Error: {mse}')
print(f'AdaBoostRegressor R-squared (R2): {r2}')
112/125:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')

import pandas as pd
from xgboost import XGBRegressor

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Train a new model with the new data
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X, y)

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
112/126:
from sklearn.metrics import r2_score

# Define the regression models and their results
regression_models = ["Linear Regression", "SVR", "DecisionTreeRegressor", "RandomForestRegressor", "GradientBoostingRegressor", "AdaBoostRegressor", "XGBRegressor"]
regression_results = [-0.5396689746717418, -0.2887939459942046, -0.6303549571603424, -0.07790616075071366, -0.06021923946097263, -0.3544676952580543, 0.01844820285381421]

# Create a DataFrame to store the results
regression_df = pd.DataFrame({"Model": regression_models, "R-squared (R2)": regression_results})

# Sort the DataFrame by R-squared (higher R2 means better accuracy)
regression_df_sorted = regression_df.sort_values(by="R-squared (R2)", ascending=False)

# Print the sorted DataFrame
print(regression_df_sorted)
114/1:
import pandas as pd
from IPython.display import display

df_C = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df_C)
114/2:
# Check for missing values

missing_values = df_C.isna().sum()
print(missing_values)
114/3:
from collections import Counter

import numpy as np

def detect_outliers(df_C,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df_C[c],25)
        # 3st quartile
        Q3 = np.percentile(df_C[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df_C[(df_C[c] < Q1 - outlier_step) | (df_C[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df_C.loc[detect_outliers(df_C,df_C.columns[:-1])]))
df_C.loc[detect_outliers(df_C,df_C.columns[:-1])]
114/4: data = df.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
114/5: data_C = df_C.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
114/6: data_C = df_C.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
114/7: data_C = df_C.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
114/8: data_C = df_C.drop(detect_outliers(df_C,df_C.columns[:-1]),axis = 0).reset_index(drop = True)
114/9:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data_C.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
114/10: data_C.describe(include="all")
114/11:
import matplotlib.pyplot as plt
import pandas as pd

# Get the list of feature names, excluding 'density'
features = [col for col in data_C.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data_C[data_C['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
114/12:
import pandas as pd

result = data_C[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
114/13:
bins = (2, 6.5, 8)
labels = [0, 1]
data_C['quality'] = pd.cut(x = data_C['quality'], bins = bins, labels = labels)
data_C['quality'].value_counts()
114/14:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data_C[feature_columns]
y = data_C['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
114/15:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
114/16:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
114/17:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix


knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc_knn = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc_knn)

print("KNeighborsClassifier Acc: ", acc_knn)

from sklearn.metrics import classification_report


report = classification_report(y_test, y_pred)
print(report)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
114/18:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
114/19:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import classification_report

knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc_knn = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc_knn)

print("KNeighborsClassifier Acc: ", acc_knn)
report = classification_report(y_test, y_pred)
print(report)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
114/20:
from sklearn.svm import SVC


svc = SVC()
svc.fit(X_resampled, y_resampled)
pred_svc = svc.predict(X_test)

cm_svc = confusion_matrix(y_test, pred_svc)
acc_svc = accuracy_score(y_test, pred_svc)
score = svc.score(X_test, y_test)
results.append(acc_svc)

print("SVC Acc : ", acc_svc)

report_svc = classification_report(y_test, pred_svc)
print(report_svc)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svc, annot=True, fmt='d', cmap='Greens', xticklabels=svc.classes_, yticklabels=svc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
114/21:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_dt = dt_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_dt = confusion_matrix(y_test, pred_dt)
acc_dt = accuracy_score(y_test, pred_dt)

# Print accuracy
print("Decision Tree Classifier Accuracy: ", acc_dt)

# Generate the classification report
report = classification_report(y_test, pred_dt)
print(report)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
114/22:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
y_pred_rf = rf_classifier.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

acc_rf = accuracy_score(y_test, y_pred_rf)
score = rf_classifier.score(X_test, y_test)
results.append(acc_rf)

print("RandomForestClassifier Acc : ", acc_rf)


# Generate the classification report
report = classification_report(y_test, y_pred_rf)
print(report)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
114/23:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()
114/24:
from sklearn.ensemble import AdaBoostClassifier

# Initialize and train the AdaBoostClassifier
ab_classifier = AdaBoostClassifier()
ab_classifier.fit(X_resampled, y_resampled)
pred_ab = ab_classifier.predict(X_test)
cm_ab = confusion_matrix(y_test, pred_ab)
acc_ab = accuracy_score(y_test, pred_ab)
results.append(acc_ab)
print("AdaBoost Classifier Accuracy: ", acc_ab)

# Generate the classification report
report_ab = classification_report(y_test, pred_ab)
print(report_ab)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_ab, annot=True, fmt='d', cmap='YlOrBr', xticklabels=ab_classifier.classes_, yticklabels=ab_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (AdaBoost)')
plt.show()
114/25:
from xgboost import XGBClassifier

# Initialize and train the XGBClassifier
xgb_classifier = XGBClassifier()
xgb_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_xgb = xgb_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_xgb = confusion_matrix(y_test, pred_xgb)
acc_xgb = accuracy_score(y_test, pred_xgb)

# Print accuracy
print("XGBoost Classifier Accuracy: ", acc_xgb)

# Generate the classification report
report_xgb = classification_report(y_test, pred_xgb)
print(report_xgb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='pink', xticklabels=xgb_classifier.classes_, yticklabels=xgb_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (XGBoost)')
plt.show()
114/26:
import pandas as pd

# Define the list of accuracy scores and model names
results = [0.8445945945945946, 0.793918918918919, 0.8175675675675675, 0.8344594594594594, 0.8513513513513513, 0.7837837837837838, 0.847972972972973]
model_names = ["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier", "XGBoost Classifier"]

# Create the DataFrame
df_result = pd.DataFrame({"Score": results, "ML Models": model_names})

# Sort the DataFrame by 'Score' column in descending order
df_result_sorted = df_result.sort_values(by='Score', ascending=False)

# Print the sorted DataFrame
print(df_result_sorted)
114/27:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Train a new model with the new data
gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)

# Make predictions on the new data
y_pred_new = gbc.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
114/28:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

new_df = pd.DataFrame(new_data)

# Make predictions using the pre-trained model
predicted_quality = gbc.predict(new_df)

print("Predicted quality for new data:")
print(predicted_quality)
114/29:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

new_df = pd.DataFrame(new_data)

# Make predictions using the pre-trained model
predicted_quality = gbc.predict(new_df)

print("Predicted quality for new data:")
print(predicted_quality)
114/30:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth=6, random_state=2)
gbc.fit(X_resampled, y_resampled, feature_names=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates'])

# Make predictions using the pre-trained model
predicted_quality = gbc.predict(new_df)

print("Predicted quality for new data:")
print(predicted_quality)
114/31:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth=6, random_state=2)
gbc.fit(X_resampled, y_resampled)

# Make predictions using the pre-trained model
predicted_quality = gbc.predict(new_df)

print("Predicted quality for new data:")
print(predicted_quality)
114/32:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth=6, random_state=2, alpha=0.1)
gbc.fit(X_resampled, y_resampled)
predicted_quality = gbc.predict(new_df)

print("Predicted quality for new data:")
print(predicted_quality)
114/33:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
predicted_quality = gbc.predict(new_df)

print("Predicted quality for new data:")
print(predicted_quality)
114/34:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)
predicted_quality = gbc.predict(new_df)

print("Predicted quality for new data:")
print(predicted_quality)
114/35:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

from sklearn.linear_model import LogisticRegression

# Train multiple gradient boosting classifiers on different subsets of the training data
gbcs = []
for i in range(5):
    gbc = GradientBoostingClassifier(max_depth=6, random_state=2, alpha=0.1)
    gbc.fit(X_resampled[i::5], y_resampled[i::5])
    gbcs.append(gbc)

# Train a logistic regression model to combine the predictions of the individual gradient boosting classifiers
lr = LogisticRegression()
lr.fit(np.concatenate([gbc.predict_proba(X_resampled) for gbc in gbcs], axis=1), y_resampled)

predicted_quality = lr.predict(np.concatenate([gbc.predict_proba(new_df) for gbc in gbcs], axis=1))
114/36:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)
predicted_quality = gbc.predict(new_df)

from sklearn.linear_model import LogisticRegression

# Train multiple gradient boosting classifiers on different subsets of the training data
gbcs = []
for i in range(5):
    gbc = GradientBoostingClassifier(max_depth=6, random_state=2, alpha=0.1)
    gbc.fit(X_resampled[i::5], y_resampled[i::5])
    gbcs.append(gbc)

# Train a logistic regression model to combine the predictions of the individual gradient boosting classifiers
lr = LogisticRegression()
lr.fit(np.concatenate([gbc.predict_proba(X_resampled) for gbc in gbcs], axis=1), y_resampled)

predicted_quality = lr.predict(np.concatenate([gbc.predict_proba(new_df) for gbc in gbcs], axis=1))


print("Predicted quality for new data:")
print(predicted_quality)
114/37:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)
predicted_quality = gbc.predict(new_df)

from sklearn.linear_model import LogisticRegression

# Train multiple gradient boosting classifiers on different subsets of the training data
gbcs = []
for i in range(5):
    gbc = GradientBoostingClassifier(max_depth=6, random_state=2)
    gbc.fit(X_resampled[i::5], y_resampled[i::5])
    gbcs.append(gbc)

# Train a logistic regression model to combine the predictions of the individual gradient boosting classifiers
lr = LogisticRegression()
lr.fit(np.concatenate([gbc.predict_proba(X_resampled) for gbc in gbcs], axis=1), y_resampled)

predicted_quality = lr.predict(np.concatenate([gbc.predict_proba(new_df) for gbc in gbcs], axis=1))


print("Predicted quality for new data:")
print(predicted_quality)
114/38:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)
predicted_quality = gbc.predict(new_df)
print(predicted_quality)
114/39:
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
predicted_quality = gbc.predict(new_df)
print(predicted_quality)
114/40:
import pandas as pd
from IPython.display import display

df_C = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df_C)
114/41:
# Check for missing values

missing_values = df_C.isna().sum()
print(missing_values)
114/42:
from collections import Counter

import numpy as np

def detect_outliers(df_C,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df_C[c],25)
        # 3st quartile
        Q3 = np.percentile(df_C[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df_C[(df_C[c] < Q1 - outlier_step) | (df_C[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df_C.loc[detect_outliers(df_C,df_C.columns[:-1])]))
df_C.loc[detect_outliers(df_C,df_C.columns[:-1])]
114/43: data_C = df_C.drop(detect_outliers(df_C,df_C.columns[:-1]),axis = 0).reset_index(drop = True)
114/44: data_C.describe(include="all")
114/45:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data_C.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
114/46:
import matplotlib.pyplot as plt
import pandas as pd

# Get the list of feature names, excluding 'density'
features = [col for col in data_C.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data_C[data_C['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
114/47:
import pandas as pd

result = data_C[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
114/48:
bins = (2, 6, 8)
labels = [0, 1]
data_C['quality'] = pd.cut(x = data_C['quality'], bins = bins, labels = labels)
data_C['quality'].value_counts()
114/49:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data_C[feature_columns]
y = data_C['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
114/50:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
114/51:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
114/52:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
114/53:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import classification_report

knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc_knn = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc_knn)

print("KNeighborsClassifier Acc: ", acc_knn)
report = classification_report(y_test, y_pred)
print(report)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
114/54:
from sklearn.svm import SVC


svc = SVC()
svc.fit(X_resampled, y_resampled)
pred_svc = svc.predict(X_test)

cm_svc = confusion_matrix(y_test, pred_svc)
acc_svc = accuracy_score(y_test, pred_svc)
score = svc.score(X_test, y_test)
results.append(acc_svc)

print("SVC Acc : ", acc_svc)

report_svc = classification_report(y_test, pred_svc)
print(report_svc)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svc, annot=True, fmt='d', cmap='Greens', xticklabels=svc.classes_, yticklabels=svc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
114/55:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_dt = dt_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_dt = confusion_matrix(y_test, pred_dt)
acc_dt = accuracy_score(y_test, pred_dt)

# Print accuracy
print("Decision Tree Classifier Accuracy: ", acc_dt)

# Generate the classification report
report = classification_report(y_test, pred_dt)
print(report)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
114/56:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
y_pred_rf = rf_classifier.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

acc_rf = accuracy_score(y_test, y_pred_rf)
score = rf_classifier.score(X_test, y_test)
results.append(acc_rf)

print("RandomForestClassifier Acc : ", acc_rf)


# Generate the classification report
report = classification_report(y_test, y_pred_rf)
print(report)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
114/57:
from numpy import argsort
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
# Assuming you have a preprocessing function defined
new_data_preprocessed = preprocess_data(new_data, *argsort, **kwargs)

# Make predictions using the model
predictions = gbc.predict(new_data_preprocessed)

print(predicted_quality)
113/1:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
print(df)
113/2:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
113/3:
from collections import Counter

import numpy as np

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
113/4: data = df.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
113/5: data.describe(include="all")
113/6:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
113/7:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
113/8:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
113/9:
bins = (2, 6.5, 8)
labels = [0, 1]
data['quality'] = pd.cut(x = data['quality'], bins = bins, labels = labels)
data['quality'].value_counts()
113/10:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
113/11:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
113/12:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
113/13:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
113/14:
from sklearn.linear_model import LinearRegression

# Assuming X_resampled, y_resampled are your training data
regressor = LinearRegression()
regressor.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = regressor.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'LinearRegression Mean Absolute Error: {mae}')
print(f'LinearRegression Mean Squared Error: {mse}')
print(f'LinearRegression R-squared (R2): {r2}')
113/15:
from sklearn.svm import SVR

# Assuming X_resampled, y_resampled are your training data
svr = SVR(kernel='rbf')  # You can experiment with different kernels (e.g., 'linear', 'poly')
svr.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = svr.predict(X_test)

# Now you have continuous predictions. You can evaluate the performance using regression metrics.
# For example, you can use Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2), etc.
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'SVR Mean Absolute Error: {mae}')
print(f'SVR Mean Squared Error: {mse}')
print(f'SVR R-squared (R2): {r2}')
113/16:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeRegressor
dt_regressor = DecisionTreeRegressor()
dt_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = dt_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'DecisionTreeRegressor Mean Absolute Error: {mae}')
print(f'DecisionTreeRegressor Mean Squared Error: {mse}')
print(f'DecisionTreeRegressor R-squared (R2): {r2}')
113/17:
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestRegressor
rf_regressor = RandomForestRegressor()
rf_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = rf_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'RandomForestRegressor Mean Absolute Error: {mae}')
print(f'RandomForestRegressor Mean Squared Error: {mse}')
print(f'RandomForestRegressor R-squared (R2): {r2}')
113/18:
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the GradientBoostingRegressor
gbr = GradientBoostingRegressor(max_depth=6, random_state=2)
gbr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = gbr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'GradientBoostingRegressor Mean Absolute Error: {mae}')
print(f'GradientBoostingRegressor Mean Squared Error: {mse}')
print(f'GradientBoostingRegressor R-squared (R2): {r2}')
113/19:
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the AdaBoostRegressor
abr = AdaBoostRegressor()
abr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = abr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'AdaBoostRegressor Mean Absolute Error: {mae}')
print(f'AdaBoostRegressor Mean Squared Error: {mse}')
print(f'AdaBoostRegressor R-squared (R2): {r2}')
113/20:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Train a new model with the new data
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X, y)

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
113/21:
from sklearn.metrics import r2_score

# Define the regression models and their results
regression_models = ["Linear Regression", "SVR", "DecisionTreeRegressor", "RandomForestRegressor", "GradientBoostingRegressor", "AdaBoostRegressor", "XGBRegressor"]
regression_results = [-0.5396689746717418, -0.2887939459942046, -0.6303549571603424, -0.07790616075071366, -0.06021923946097263, -0.3544676952580543, 0.01844820285381421]

# Create a DataFrame to store the results
regression_df = pd.DataFrame({"Model": regression_models, "R-squared (R2)": regression_results})

# Sort the DataFrame by R-squared (higher R2 means better accuracy)
regression_df_sorted = regression_df.sort_values(by="R-squared (R2)", ascending=False)

# Print the sorted DataFrame
print(regression_df_sorted)
114/58:
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}


from collections import Counter

import numpy as np

def detect_outliers(new_data,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(new_data[c],25)
        # 3st quartile
        Q3 = np.percentile(new_data[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = new_data[(new_data[c] < Q1 - outlier_step) | (new_data[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(new_data.loc[detect_outliers(new_data,new_data.columns[:-1])]))
new_data.loc[detect_outliers(new_data,new_data.columns[:-1])]
114/59:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Convert new_data into a DataFrame
new_df = pd.DataFrame(new_data)

def detect_outliers(new_df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(new_df[c],25)
        # 3st quartile
        Q3 = np.percentile(new_df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = new_df[(new_df[c] < Q1 - outlier_step) | (new_df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(new_df.loc[detect_outliers(new_df,new_df.columns[:-1])]))
new_df.loc[detect_outliers(new_df,new_df.columns[:-1])]
114/60:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Convert new_data into a DataFrame
new_df = pd.DataFrame(new_data)

# Check for missing values

missing_values = new_df.isna().sum()
print(missing_values)
114/61:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data)

# 'new_data_scaled' now contains the scaled version of the new data
114/62:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Assuming 'X_train' is your original training data
# Initialize the StandardScaler with mean and standard deviation from the training data
scaler = StandardScaler()
scaler.fit(X_train)

# Convert new_data into a DataFrame
new_df = pd.DataFrame(new_data)

# Extract the numerical values from the DataFrame
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

# 'new_data_scaled' now contains the scaled version of the new data
114/63:
from numpy import argsort
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}
114/64:
from numpy import argsort
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()
114/65:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

new_df = pd.DataFrame(new_data)
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

predictions = gb_model.predict(new_data_scaled)
114/66:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

new_df = pd.DataFrame(new_data)
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

new = pd.DataFrame(new_data_scaled)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)

# Make predictions on the new data
y_pred_new = gbc.predict(new)

print("Predicted quality for new data:")
print(y_pred_new)
114/67:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

new_df = pd.DataFrame(new_data)
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

new = pd.DataFrame(new_data_scaled)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)

# Make predictions on the new data
y_pred_new = gbc.predict(new)

y_pred_discrete = np.round(y_pred_new)
y_pred_discrete = np.clip(y_pred_discrete, 0, 10)  # Clip values to be within 0 and 10

print("Discrete Predicted quality for new data:")
print(y_pred_discrete)

print("Predicted quality for new data:")
print(y_pred_new)
114/68:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

new_df = pd.DataFrame(new_data)
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

new = pd.DataFrame(new_data_scaled)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)

# Make predictions on the new data
y_pred_new = gbc.predict(new)

y_pred_discrete = np.round(y_pred_new)
y_pred_discrete = np.clip(y_pred_discrete, 0, 10)  # Clip values to be within 0 and 10

print("Discrete Predicted quality for new data:")
print(y_pred_discrete)
114/69:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

new_df = pd.DataFrame(new_data)
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

new = pd.DataFrame(new_data_scaled)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)

# Make predictions on the new data
y_pred_new = gbc.predict(new)
y_pred_discrete = np.round(y_pred_new * 5) + 5
y_pred_discrete = np.clip(y_pred_discrete, 0, 10)  # Clip values to be within 0 and 10

print("Predicted quality for new data:")
print(y_pred_discrete)

print("Discrete Predicted quality for new data:")
print(y_pred_discrete)
114/70:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

new_df = pd.DataFrame(new_data)
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

new = pd.DataFrame(new_data_scaled)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)

# Make predictions on the new data
y_pred_new = gbc.predict(new)
y_pred_discrete = np.round(y_pred_new * 5) + 5
y_pred_discrete = np.clip(y_pred_discrete, 0, 10)  # Clip values to be within 0 and 10

print("Predicted quality for new data:")
print(y_pred_discrete)
114/71:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}


# Create a DataFrame with the data
columns = ['Fixed acidity', 'Volatile acidity', 'Citric acid', 'Residual sugar', 'Chlorides',
           'Free sulfur dioxide', 'Total sulfur dioxide', 'Density', 'pH', 'Sulphates']
df = pd.DataFrame(new_data, columns=columns)

# Load the pre-trained GBC model (assuming you have already trained it)
# If not, you will need to train the model on your original data first.
# For training, you'll also need labels (the 'quality' values).
# For simplicity, let's assume you have a trained model file 'gbc_model.pkl'.
# You will need to replace this with the actual file path.
import joblib
gbc_model = joblib.load('gbc_model.pkl')

# Preprocess the data (assuming you used StandardScaler during training)
scaler = StandardScaler()
df_scaled = scaler.transform(df)

# Predict wine quality
predicted_quality = gbc_model.predict(df_scaled)

# Print the predicted quality
print("Predicted quality:", predicted_quality)
114/72:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

new_df = pd.DataFrame(new_data)
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

new = pd.DataFrame(new_data_scaled)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)

# Make predictions on the new data
y_pred_new = gbc.predict(new)

print("Predicted quality for new data:")
print(y_pred_discrete)
114/73:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

new_df = pd.DataFrame(new_data)
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

new = pd.DataFrame(new_data_scaled)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)

# Make predictions on the new data
y_pred_new = gbc.predict(new)

print("Predicted quality for new data:")
print(y_pred_new)
114/74:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

new_df = pd.DataFrame(new_data)
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

new = pd.DataFrame(new_data_scaled)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)

# Make predictions on the new data
y_pred_new = gbc.predict(new)

print("Predicted quality for new data:")
print(y_pred_new)
114/75:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

new_df = pd.DataFrame(new_data)
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

new = pd.DataFrame(new_data_scaled)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)

# Make predictions on the new data
y_pred_new = gbc.predict(new)

print("Predicted quality for new data:")
print(y_pred_new)
115/1:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
115/2:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
115/3:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
115/4:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
115/5:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
115/6:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\customer_churn_large_dataset.csv')
display(df)
115/7:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\Internshala work\New folder\customer_churn_large_dataset.csv')
display(df)
115/8:
import pandas as pd
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
115/9:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\Internshala work\New folder\customer_churn_large_dataset.csv')
display(df)
115/10:
missing_values = df.isnull()
print(missing_values)
missing_counts = missing_values.sum()
print(missing_counts)
115/11:
Name_df = df['Name']
Age_df = df['Age']
Churn_df = df['Churn']
# display(Name_df)
# display(Age_df)
display(Churn_df)
115/12:
duplicates = df.duplicated()
duplicate_rows = df[duplicates]
print(duplicate_rows)
df_cleaned = df.drop_duplicates()
display(df_cleaned)
115/13: df.describe(include="all")
115/14: df.drop(['CustomerID', 'Total_Usage_GB'], axis=1)
115/15:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(6,3)  , dpi=100)
sns.countplot(x='Gender', hue="Churn", data=df)
ax.set_xlabel('Gender')
ax.set_ylabel('Value count')
plt.show()
115/16:
#Countplot of Sub Grade vs Loan Status
fig, ax = plt.subplots(figsize=(12,6), dpi=100)

sns.countplot(x='Location', hue="Churn", data=df)

ax.set_xlabel('Location')
ax.set_ylabel('Value Counts')

plt.show()
115/17:
ax = df['Subscription_Length_Months'].value_counts().plot(kind = 'bar',rot = 0, width = 0.3)
ax.set_ylabel('# of Customers')
ax.set_title('# of Customers by Subscription_Length_Months')
115/18:
import matplotlib.ticker as mtick

colors = ['#4D3425','#E4512B']
ax = (df['Churn'].value_counts()*100.0 /len(df)).plot(kind='bar',
                                                   stacked = True,
                                                   rot = 0,
                                                   color = colors,
                                                   figsize = (8,6))
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
ax.set_ylabel('% Customers', size = 14)
ax.set_xlabel('Churn', size = 14)
ax.set_title('Churn Rate', size = 14)

# create a list to collect the plt.patches data
totals = []

# find the values and append to list
for i in ax.patches:
    totals.append(i.get_width())

# set individual bar labels using above list
total = sum(totals)
for i in ax.patches:
    # get_width pulls left or right; get_y pushes up or down
    ax.text(i.get_x() + 0.15, i.get_height() - 4.0, \
            str(round((i.get_height() / total), 1)) + '%',
            fontsize=12,
            color='white',
            weight='bold')

plt.show()
115/19:
import pandas as pd

# Assuming 'df' is your DataFrame and 'Age' is a column in it
bins = [0, 18, 30, 50, 100]
labels = ['young', 'adult', 'middle-aged', 'senior']

df['age_category'] = pd.cut(df['Age'], bins=bins, labels=labels)
print(df['age_category'])
115/20:
from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder() 
df['Name']= label_encoder.fit_transform(df['Name']) 
df['Name'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['age_category'] = label_encoder.fit_transform(df['age_category'])
df['age_category'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Gender']= label_encoder.fit_transform(df['Gender']) 
df['Gender'].unique() 

label_encoder = preprocessing.LabelEncoder() 
df['Location']= label_encoder.fit_transform(df['Location']) 
df['Location'].unique()
115/21:
# Splitting train and test data

X = df[['Name', 'age_category', 'Gender', 'Location', 'Subscription_Length_Months', 'Monthly_Bill']]
y = df['Churn']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
115/22:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
115/23:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
115/24:
# Running logistic regression model
from asyncio import log
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error


lr = LogisticRegression()
result = lr.fit(X_train, y_train)
from sklearn import metrics
prediction_test = lr.predict(X_test)
# Print the prediction accuracy
print (metrics.accuracy_score(y_test, prediction_test))

# Initialize KFold with k=5
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform k-fold cross-validation
cv_scores = cross_val_score(lr, X, y, cv=kf, scoring='accuracy')

# Print the cross-validation scores
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean()}')

# Predict on the training set
y_train_pred = lr.predict(X_train)
train_mse = mean_squared_error(y_train, y_train_pred)

# Predict on the test set
y_test_pred = lr.predict(X_test)
test_mse = mean_squared_error(y_test, y_test_pred)


print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")
115/25:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Create and train the model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate on training set
train_preds = model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_preds)

# Evaluate on testing set
test_preds = model.predict(X_test)
test_accuracy = accuracy_score(y_test, test_preds)

print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")
115/26:
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)
)

train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)

plt.plot(train_sizes, train_scores_mean, label='Training Accuracy')
plt.plot(train_sizes, test_scores_mean, label='Testing Accuracy')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
115/27:
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Assuming you have X_train, y_train defined from previous code

# Define the hyperparameters and their ranges to search
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the Grid Search object
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,  # Number of cross-validation folds
    scoring='accuracy',  # Evaluation metric
    verbose=1  # Output progress
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_train = best_model.predict(X_train)
train_accuracy = accuracy_score(y_train, y_pred_train)
print(f"Training Accuracy with Best Model: {train_accuracy}")
y_pred_test = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Testing Accuracy with Best Model: {test_accuracy}")
115/28:
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Define the hyperparameters and their possible values
param_dist = {
    'max_depth': np.arange(1, 10),
    'min_samples_split': np.arange(2, 10),
    'min_samples_leaf': np.arange(1, 10)
}

# Create a base model
base_model = DecisionTreeClassifier()

# Create a RandomizedSearchCV object
random_search = RandomizedSearchCV(base_model, param_distributions=param_dist, n_iter=100, cv=5, verbose=1, n_jobs=-1)

# Fit the RandomizedSearchCV object to the data
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the training accuracy with the best model
best_model = random_search.best_estimator_
y_pred_train = best_model.predict(X_train)
accuracy_train = np.mean(y_pred_train == y_train)
print("Training Accuracy with Best Model:", accuracy_train)

# Get the testing accuracy with the best model
y_pred_test = best_model.predict(X_test)
accuracy_test = np.mean(y_pred_test == y_test)
print("Testing Accuracy with Best Model:", accuracy_test)
115/29:
from sklearn.ensemble import RandomForestClassifier

# Create and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=5, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate the model
y_pred_train_rf = rf_model.predict(X_train)
y_pred_test_rf = rf_model.predict(X_test)

train_accuracy_rf = accuracy_score(y_train, y_pred_train_rf)
test_accuracy_rf = accuracy_score(y_test, y_pred_test_rf)

print(f"Random Forest Training Accuracy: {train_accuracy_rf}")
print(f"Random Forest Testing Accuracy: {test_accuracy_rf}")
116/1:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
115/30:
from sklearn.ensemble import RandomForestClassifier

selected_features = ['Name', 'Subscription_Length_Months', 'Name', 'Location', 'Gender', 'Age']

# Assuming X_train and X_test are your training and testing feature matrices
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# Create and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=5, random_state=42)
rf_model.fit(X_train[selected_features], y_train)

# Evaluate the model
y_pred_train_rf = rf_model.predict(X_train[selected_features])
y_pred_test_rf = rf_model.predict(X_test[selected_features])

train_accuracy_rf = accuracy_score(y_train, y_pred_train_rf)
test_accuracy_rf = accuracy_score(y_test, y_pred_test_rf)

print(f"Random Forest Training Accuracy: {train_accuracy_rf}")
print(f"Random Forest Testing Accuracy: {test_accuracy_rf}")
116/2:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
115/31:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
115/32:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import classification_report

knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc_knn = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc_knn)

print("KNeighborsClassifier Acc: ", acc_knn)
report = classification_report(y_test, y_pred)
print(report)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
115/33:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_dt = dt_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_dt = confusion_matrix(y_test, pred_dt)
acc_dt = accuracy_score(y_test, pred_dt)

# Print accuracy
print("Decision Tree Classifier Accuracy: ", acc_dt)

# Generate the classification report
report = classification_report(y_test, pred_dt)
print(report)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
116/3:
import pandas as pd
from IPython.display import display

df_C = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df_C)
116/4:
# Check for missing values

missing_values = df_C.isna().sum()
print(missing_values)
116/5:
from collections import Counter

import numpy as np

def detect_outliers(df_C,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df_C[c],25)
        # 3st quartile
        Q3 = np.percentile(df_C[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df_C[(df_C[c] < Q1 - outlier_step) | (df_C[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df_C.loc[detect_outliers(df_C,df_C.columns[:-1])]))
df_C.loc[detect_outliers(df_C,df_C.columns[:-1])]
116/6: data_C = df_C.drop(detect_outliers(df_C,df_C.columns[:-1]),axis = 0).reset_index(drop = True)
116/7: data_C.describe(include="all")
116/8:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data_C.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
116/9:
import matplotlib.pyplot as plt
import pandas as pd

# Get the list of feature names, excluding 'density'
features = [col for col in data_C.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data_C[data_C['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
116/10:
import pandas as pd

result = data_C[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
116/11:
bins = (2, 6, 8)
labels = [0, 1]
data_C['quality'] = pd.cut(x = data_C['quality'], bins = bins, labels = labels)
data_C['quality'].value_counts()
116/12:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data_C[feature_columns]
y = data_C['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
116/13:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
116/14:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
116/15:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
116/16:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import classification_report

knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc_knn = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc_knn)

print("KNeighborsClassifier Acc: ", acc_knn)
report = classification_report(y_test, y_pred)
print(report)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
116/17:
from sklearn.svm import SVC


svc = SVC()
svc.fit(X_resampled, y_resampled)
pred_svc = svc.predict(X_test)

cm_svc = confusion_matrix(y_test, pred_svc)
acc_svc = accuracy_score(y_test, pred_svc)
score = svc.score(X_test, y_test)
results.append(acc_svc)

print("SVC Acc : ", acc_svc)

report_svc = classification_report(y_test, pred_svc)
print(report_svc)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svc, annot=True, fmt='d', cmap='Greens', xticklabels=svc.classes_, yticklabels=svc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
116/18:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_dt = dt_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_dt = confusion_matrix(y_test, pred_dt)
acc_dt = accuracy_score(y_test, pred_dt)

# Print accuracy
print("Decision Tree Classifier Accuracy: ", acc_dt)

# Generate the classification report
report = classification_report(y_test, pred_dt)
print(report)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
116/19:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
y_pred_rf = rf_classifier.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

acc_rf = accuracy_score(y_test, y_pred_rf)
score = rf_classifier.score(X_test, y_test)
results.append(acc_rf)

print("RandomForestClassifier Acc : ", acc_rf)


# Generate the classification report
report = classification_report(y_test, y_pred_rf)
print(report)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
116/20:
from numpy import argsort
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
# Assuming you have a preprocessing function defined
new_data_preprocessed = preprocess_data(new_data, *argsort, **kwargs)

# Make predictions using the model
predictions = gbc.predict(new_data_preprocessed)

print(predicted_quality)
116/21:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data_C[feature_columns]
y = data_C['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
117/1:
import pandas as pd
from IPython.display import display

df_C = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df_C)
117/2:
# Check for missing values

missing_values = df_C.isna().sum()
print(missing_values)
117/3:
from collections import Counter

import numpy as np

def detect_outliers(df_C,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df_C[c],25)
        # 3st quartile
        Q3 = np.percentile(df_C[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df_C[(df_C[c] < Q1 - outlier_step) | (df_C[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df_C.loc[detect_outliers(df_C,df_C.columns[:-1])]))
df_C.loc[detect_outliers(df_C,df_C.columns[:-1])]
117/4: data_C = df_C.drop(detect_outliers(df_C,df_C.columns[:-1]),axis = 0).reset_index(drop = True)
117/5: data_C.describe(include="all")
117/6:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data_C.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
117/7:
import matplotlib.pyplot as plt
import pandas as pd

# Get the list of feature names, excluding 'density'
features = [col for col in data_C.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data_C[data_C['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
117/8:
import pandas as pd

result = data_C[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
117/9:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data_C[feature_columns]
y = data_C['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
117/10:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
117/11:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
117/12:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
117/13:
from sklearn.metrics import confusion_matrix, pair_confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import classification_report

knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_resampled, y_resampled)
y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

acc_knn = accuracy_score(y_test, y_pred)
score = knn.score(X_test, y_test)
results.append(acc_knn)

print("KNeighborsClassifier Acc: ", acc_knn)
report = classification_report(y_test, y_pred)
print(report)

# Create a custom confusion matrix plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
117/14:
from sklearn.svm import SVC


svc = SVC()
svc.fit(X_resampled, y_resampled)
pred_svc = svc.predict(X_test)

cm_svc = confusion_matrix(y_test, pred_svc)
acc_svc = accuracy_score(y_test, pred_svc)
score = svc.score(X_test, y_test)
results.append(acc_svc)

print("SVC Acc : ", acc_svc)

report_svc = classification_report(y_test, pred_svc)
print(report_svc)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svc, annot=True, fmt='d', cmap='Greens', xticklabels=svc.classes_, yticklabels=svc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
117/15:
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_dt = dt_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_dt = confusion_matrix(y_test, pred_dt)
acc_dt = accuracy_score(y_test, pred_dt)

# Print accuracy
print("Decision Tree Classifier Accuracy: ", acc_dt)

# Generate the classification report
report = classification_report(y_test, pred_dt)
print(report)

# Create a custom confusion matrix plot with green colormap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
117/16:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
y_pred_rf = rf_classifier.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

acc_rf = accuracy_score(y_test, y_pred_rf)
score = rf_classifier.score(X_test, y_test)
results.append(acc_rf)

print("RandomForestClassifier Acc : ", acc_rf)


# Generate the classification report
report = classification_report(y_test, y_pred_rf)
print(report)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='coolwarm', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
117/17:
from numpy import argsort
from sklearn.ensemble import GradientBoostingClassifier


gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
y_pred_gbc = gbc.predict(X_test)
cm_aaa = confusion_matrix(y_test, y_pred_gbc)
acc_gb = accuracy_score(y_test, y_pred_gbc)
score_gb = gbc.score(X_test, y_test)
results.append(acc_gb)

print("GradientBoostingClassifier Acc : ", acc_gb)

# Generate the classification report
report_gb = classification_report(y_test, y_pred_gbc)
print(report_gb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_aaa, annot=True, fmt='d', cmap='Purples', xticklabels=gbc.classes_, yticklabels=gbc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Gradient Boosting)')
plt.show()

new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd

new_df = pd.DataFrame(new_data)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X_resampled, y_resampled)
# Assuming you have a preprocessing function defined
new_data_preprocessed = preprocess_data(new_data, *argsort, **kwargs)

# Make predictions using the model
predictions = gbc.predict(new_data_preprocessed)

print(predicted_quality)
113/22:
import pandas as pd
from IPython.display import display

df = pd.read_csv(r'C:\Users\sound\OneDrive\Desktop\main project\python project main\supervised classification type dataset\winequality-red.csv', sep=';')
display(df)
print(df)
113/23:
# Check for missing values

missing_values = df.isna().sum()
print(missing_values)
113/24:
from collections import Counter

import numpy as np

def detect_outliers(df,features):
    outlier_indices = []
    
    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],25)
        # 3st quartile
        Q3 = np.percentile(df[c],75)
        # IQR
        IQR = Q3 - Q1
        # Outlier Step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces 
        outlier_indices.extend(outlier_list_col)
        
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5) 
    
    return multiple_outliers

print("number of outliers detected --> ",len(df.loc[detect_outliers(df,df.columns[:-1])]))
df.loc[detect_outliers(df,df.columns[:-1])]
113/25: data = df.drop(detect_outliers(df,df.columns[:-1]),axis = 0).reset_index(drop = True)
113/26: data.describe(include="all")
113/27:
import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()
113/28:
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'data' is your DataFrame containing the features and 'quality' column
# Replace this with your actual DataFrame

# Get the list of feature names, excluding 'density'
features = [col for col in data.columns if col != 'density' and col != 'quality']

# Define colors for each quality level
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']

# Create subplots
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns of subplots

for i, feature in enumerate(features):
    row = i // 5
    col = i % 5

    # Create histograms with different colors for each quality level
    for q, color in zip(range(3, 9), colors):
        axes[row, col].hist(data[data['quality'] == q][feature], alpha=0.5, color=color, label=f'Quality {q}', bins=15)

    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
113/29:
import pandas as pd

# Assuming 'data' is your DataFrame
result = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'quality']].groupby(["quality"], as_index=False).mean().sort_values(by="quality")


# Apply background gradient
styled_result = result.style.background_gradient("Reds")

# Display the styled DataFrame
styled_result
113/30:
import pandas as pd
from sklearn.model_selection import train_test_split

feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates']

X = data[feature_columns]
y = data['quality']

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the data
display('X_train: ')
display(X_train.head())
display('X_test: ')
display(X_test.head())
display('y_train: ')
display(y_train.head())
display('y_test: ')
display(y_test.head())
113/31:
print('Shape of X-train:', X_train.shape)
print('Shape of X-test:', X_test.shape)
print('Shape of y-test:', y_train.shape)
print('Shape of y-test:', y_test.shape)
113/32:
import collections
from imblearn.over_sampling import SMOTE

# Initialize the SMOTE object
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print("Before smote --> ", collections.Counter(y_train))
print("After smote --> ", collections.Counter(y_resampled))
113/33:
from sklearn.discriminant_analysis import StandardScaler


scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled) 
X_test = scaler.transform(X_test) 
results = []
113/34:
from sklearn.linear_model import LinearRegression

# Assuming X_resampled, y_resampled are your training data
regressor = LinearRegression()
regressor.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = regressor.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'LinearRegression Mean Absolute Error: {mae}')
print(f'LinearRegression Mean Squared Error: {mse}')
print(f'LinearRegression R-squared (R2): {r2}')
113/35:
from sklearn.svm import SVR

# Assuming X_resampled, y_resampled are your training data
svr = SVR(kernel='rbf')  # You can experiment with different kernels (e.g., 'linear', 'poly')
svr.fit(X_resampled, y_resampled)

# Assuming X_test is your test data
y_pred_regression = svr.predict(X_test)

# Now you have continuous predictions. You can evaluate the performance using regression metrics.
# For example, you can use Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2), etc.
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'SVR Mean Absolute Error: {mae}')
print(f'SVR Mean Squared Error: {mse}')
print(f'SVR R-squared (R2): {r2}')
113/36:
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the DecisionTreeRegressor
dt_regressor = DecisionTreeRegressor()
dt_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = dt_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'DecisionTreeRegressor Mean Absolute Error: {mae}')
print(f'DecisionTreeRegressor Mean Squared Error: {mse}')
print(f'DecisionTreeRegressor R-squared (R2): {r2}')
113/37:
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the RandomForestRegressor
rf_regressor = RandomForestRegressor()
rf_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = rf_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'RandomForestRegressor Mean Absolute Error: {mae}')
print(f'RandomForestRegressor Mean Squared Error: {mse}')
print(f'RandomForestRegressor R-squared (R2): {r2}')
113/38:
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the GradientBoostingRegressor
gbr = GradientBoostingRegressor(max_depth=6, random_state=2)
gbr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = gbr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'GradientBoostingRegressor Mean Absolute Error: {mae}')
print(f'GradientBoostingRegressor Mean Squared Error: {mse}')
print(f'GradientBoostingRegressor R-squared (R2): {r2}')
113/39:
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the AdaBoostRegressor
abr = AdaBoostRegressor()
abr.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_regression = abr.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_regression)
mse = mean_squared_error(y_test, y_pred_regression)
r2 = r2_score(y_test, y_pred_regression)

print(f'AdaBoostRegressor Mean Absolute Error: {mae}')
print(f'AdaBoostRegressor Mean Squared Error: {mse}')
print(f'AdaBoostRegressor R-squared (R2): {r2}')
113/40:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Train a new model with the new data
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X, y)

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
113/41:
from sklearn.metrics import r2_score

# Define the regression models and their results
regression_models = ["Linear Regression", "SVR", "DecisionTreeRegressor", "RandomForestRegressor", "GradientBoostingRegressor", "AdaBoostRegressor", "XGBRegressor"]
regression_results = [-0.5396689746717418, -0.2887939459942046, -0.6303549571603424, -0.07790616075071366, -0.06021923946097263, -0.3544676952580543, 0.01844820285381421]

# Create a DataFrame to store the results
regression_df = pd.DataFrame({"Model": regression_models, "R-squared (R2)": regression_results})

# Sort the DataFrame by R-squared (higher R2 means better accuracy)
regression_df_sorted = regression_df.sort_values(by="R-squared (R2)", ascending=False)

# Print the sorted DataFrame
print(regression_df_sorted)
117/18:
from collections import Counter

import numpy as np


new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

new_df = pd.DataFrame(new_data)
new_data_values = new_df.values

# Apply the same scaling parameters to the new data
new_data_scaled = scaler.transform(new_data_values)

new = pd.DataFrame(new_data_scaled)

gbc = GradientBoostingClassifier(max_depth= 6, random_state=2)
gbc.fit(X, y)

# Make predictions on the new data
y_pred_new = gbc.predict(new)

print("Predicted quality for new data:")
print(y_pred_new)
117/19:
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the GradientBoostingClassifier
gbc_classifier = GradientBoostingClassifier(max_depth=6, random_state=2)
gbc_classifier.fit(X_resampled, y_resampled)
y_pred_gbc = gbc_classifier.predict(X_test)
cm_gbc = confusion_matrix(y_test, y_pred_gbc)

acc_gbc = accuracy_score(y_test, y_pred_gbc)
results.append(acc_gbc)

print("GradientBoostingClassifier Acc : ", acc_gbc)

# Generate the classification report
report = classification_report(y_test, y_pred_gbc)
print(report)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_gbc, annot=True, fmt='d', cmap='coolwarm', xticklabels=gbc_classifier.classes_, yticklabels=gbc_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
113/42:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Train a new model with the new data
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X, y)

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)

import numpy as np

# Assuming pred_scores is the list of predicted quality scores
pred_labels = np.round(pred_scores).astype(int)

# Calculate the percentage of each label
percentage_labels = (np.bincount(pred_labels) / len(pred_labels)) * 100

# Print the percentage of each label
for label, percentage in enumerate(percentage_labels):
    print(f"Percentage of label {label}: {percentage:.2f}%")
113/43:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Train a new model with the new data
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X, y)

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)

import numpy as np

# Assuming pred_scores is the list of predicted quality scores
pred_labels = np.round(y_pred_new).astype(int)

# Calculate the percentage of each label
percentage_labels = (np.bincount(pred_labels) / len(pred_labels)) * 100

# Print the percentage of each label
for label, percentage in enumerate(percentage_labels):
    print(f"Percentage of label {label}: {percentage:.2f}%")
113/44:
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the XGBRegressor
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X_resampled, y_resampled)

# Predict using the trained model
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the performance using regression metrics
mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
r2 = r2_score(y_test, y_pred_xgb)

print(f'xgb_regressor Mean Absolute Error: {mae}')
print(f'xgb_regressor Mean Squared Error: {mse}')
print(f'xgb_regressor R-squared (R2): {r2}')

# Load the new data
new_data = {
    'fixed acidity': [5, 5.7, 5.9, 8, 8.4, 7, 9, 7.2, 8, 8, 9.5, 7, 7.4, 7.8],
    'volatile acidity': [0.32, 0.38, 0.44, 0.5, 0.56, 0.28, 0.34, 0.4, 0.46, 0.52, 0.58, 0.36, 0.42, 0.48],
    'citric acid': [0.35, 0.4, 0.45, 0.5, 0.55, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.4, 0.45, 0.5],
    'residual sugar': [2.2, 2.4, 2.6, 2.8, 3, 2, 2.2, 2.4, 2.6, 2.8, 3, 2.2, 2.4, 2.6],
    'chlorides': [0.021, 0.025, 0.029, 0.033, 0.037, 0.019, 0.023, 0.027, 0.031, 0.035, 0.039, 0.021, 0.025, 0.029],
    'free sulfur dioxide': [33, 35, 37, 39, 41, 31, 33, 35, 37, 39, 41, 34, 36, 38],
    'total sulfur dioxide': [160, 175, 190, 205, 220, 155, 165, 180, 195, 210, 225, 170, 185, 200],
    'density': [0.997, 0.998, 0.999, 1, 1.001, 0.996, 0.997, 0.998, 0.999, 1, 1.001, 0.997, 0.998, 0.999],
    'pH': [3.3, 3.2, 3.1, 3, 2.9, 3.4, 3.3, 3.2, 3.1, 3, 2.9, 3.3, 3.2, 3.1],
    'sulphates': [0.65, 0.7, 0.75, 0.8, 0.85, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8]
}

# Create a DataFrame
new_df = pd.DataFrame(new_data)

# Train a new model with the new data
xgb_regressor = XGBRegressor()
xgb_regressor.fit(X, y)

# Make predictions on the new data
y_pred_new = xgb_regressor.predict(new_df)

print("Predicted quality for new data:")
print(y_pred_new)
117/20:
from sklearn.ensemble import AdaBoostClassifier

# Initialize and train the AdaBoostClassifier
ab_classifier = AdaBoostClassifier()
ab_classifier.fit(X_resampled, y_resampled)
pred_ab = ab_classifier.predict(X_test)
cm_ab = confusion_matrix(y_test, pred_ab)
acc_ab = accuracy_score(y_test, pred_ab)
results.append(acc_ab)
print("AdaBoost Classifier Accuracy: ", acc_ab)

# Generate the classification report
report_ab = classification_report(y_test, pred_ab)
print(report_ab)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_ab, annot=True, fmt='d', cmap='YlOrBr', xticklabels=ab_classifier.classes_, yticklabels=ab_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (AdaBoost)')
plt.show()
117/21:
from xgboost import XGBClassifier

# Initialize and train the XGBClassifier
xgb_classifier = XGBClassifier()
xgb_classifier.fit(X_resampled, y_resampled)

# Predict using the trained model
pred_xgb = xgb_classifier.predict(X_test)

# Calculate confusion matrix and accuracy
cm_xgb = confusion_matrix(y_test, pred_xgb)
acc_xgb = accuracy_score(y_test, pred_xgb)

# Print accuracy
print("XGBoost Classifier Accuracy: ", acc_xgb)

# Generate the classification report
report_xgb = classification_report(y_test, pred_xgb)
print(report_xgb)

# Create a custom confusion matrix plot with random colormap (cmap='coolwarm' for example)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='pink', xticklabels=xgb_classifier.classes_, yticklabels=xgb_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (XGBoost)')
plt.show()
117/22:
import pandas as pd

# Define the list of accuracy scores and model names
results = [0.8445945945945946, 0.793918918918919, 0.8175675675675675, 0.8344594594594594, 0.8513513513513513, 0.7837837837837838, 0.847972972972973]
model_names = ["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier", "XGBoost Classifier"]

# Create the DataFrame
df_result = pd.DataFrame({"Score": results, "ML Models": model_names})

# Sort the DataFrame by 'Score' column in descending order
df_result_sorted = df_result.sort_values(by='Score', ascending=False)

# Print the sorted DataFrame
print(df_result_sorted)
117/23:
import pandas as pd

# Define the list of accuracy scores and model names
results = [0.8445945945945946, 0.793918918918919, 0.8175675675675675, 0.8344594594594594, 0.8513513513513513, 0.7837837837837838, 0.847972972972973]
model_names = ["KNN", "SVC", "DecisionTreeClassifier", "RandomForestClassifier", "GradientBoostingClassifier", "AdaBoostClassifier", "XGBoost Classifier"]

# Create the DataFrame
df_result = pd.DataFrame({"Score": results, "ML Models": model_names})

# Sort the DataFrame by 'Score' column in descending order
df_result_sorted = df_result.sort_values(by='Score', ascending=False)

# Print the sorted DataFrame
print(df_result_sorted)
113/45:
import pandas as pd

df["quality_bins"] = pd.cut(df["quality"], bins=5, labels=False, include_lowest=True, right=False)
print(df)
113/46:
import pandas as pd

data["quality_bins"] = pd.cut(data["quality"], bins=5, labels=False, include_lowest=True, right=False)
print(data)
113/47:
import pandas as pd

data["quality"] = pd.cut(data["quality"], bins=5, labels=False, include_lowest=True, right=False)
print(data)
113/48:
import pandas as pd
import pandas as pd


data["quality"] = pd.cut(data["quality"], bins=[0, 2, 4, 6, 8, 10], labels=False, include_lowest=True, right=False)

# Print the DataFrame to see the changes
print(data)
113/49:
import pandas as pd
import pandas as pd


data["quality"] = np.where(data["quality"] > 5, 1, 0)
data["quality"]
   1: %history
   2: %history -g -f filename
